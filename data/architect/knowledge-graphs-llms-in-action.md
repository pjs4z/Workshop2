Knowledge Graphs and LLMs in Action

1. welcome 2. Part_I:_Foundations_of_hybrid_intelligent_systems 3. 1_Knowledge_graphs_and_LLMs:_a_killer_combination 4. 2_Intelligent_systems:_a_hybrid_approach 5. Part_II:_Building_knowledge_graphs_from_structured_data_sources 6. 3_Create_your_first_Knowledge_Graph_from_ontologies 7. 4_From_simple_networks_to_multi-source_integration 8. Part_III:_Building_knowledge_graphs_from_text 9. 5_Domain-specific_knowledge_extraction_from_unstructured_data 10. 6_Building_knowledge_graphs_with_large_language_models 11. 7_Named_entity_disambiguation 12. 8_NED_with_open_LLMs_and_domain_ontologies 13. Part_IV:_Machine_learning_on_knowledge_graphs 14. 9_Machine_learning_on_knowledge_graphs:_a_primer_approach 15. 10_Graph_feature_engineering:_manual_and_semi-

automated_approaches 16. 11_Graph_representation_learning_and_graph_neural_network 17. 12_Node_classification_and_link_prediction_with_GNNs 18. Part_V:_Information_retrieval_with_knowledge_graphs_and_LLMs 19. 13_Knowledge_graph-powered_retrieval_augmented_generation 20. 14_Ask_a_KG_with_natural_language 21. 15_Building_a_QA_agent_with_LangGraph 22. Appendix_A._Introduction_to_graphs 23. Appendix_B._Neo4j 24. Appendix_C._Build_knowledge_graphs_from_structured_sources

## welcome

Dear Reader,

Thanks for purchasing the MEAP of Knowledge Graphs and LLMs in Action. The integration of Knowledge Graphs with Large Language Models represents a powerful convergence that is transforming how we build intelligent systems. This synergy creates unprecedented opportunities for developing advanced applications that can reason with contextual information while maintaining accuracy and explainability.

Knowledge Graphs and LLMs in Action is a practical guide to effectively leverage the combined power of Knowledge Graphs and LLMs to develop sophisticated applications across different domains. This book guides you through all the stages necessary for building complete solutions where these complementary technologies work together to create truly intelligent systems. It focuses on methods, best practices, algorithms, machine learning techniques, and design patterns based on our direct experience in building complex applications.

This book offers many recipes where Knowledge Graphs and LLMs serve as the main ingredients to prepare a tasty product for your customers or your organization. Across the lifecycle of a complex knowledge-driven project, these approaches prove to be useful in multiple aspects: managing and integrating information from various data sources - including unstructured ones - more efficiently, implementing better algorithms, storing extracted insights for faster access, visualizing results effectively to enable further analysis, accessing the knowledge using natural language.

It is divided into five parts:

Part I: Foundations of Hybrid Intelligent Systems

This part establishes the theoretical and practical foundation for understanding how Knowledge Graphs and LLMs can work together. It

introduces their complementary nature and illustrates how they enhance each other's capabilities through concrete examples and use cases. You'll learn about different types of knowledge representation and reasoning strategies, examining their roles in knowledge acquisition, representation, and various forms of reasoning.

Part II: Building Knowledge Graphs from Structured Data Sources

This part addresses the complex process of constructing knowledge graphs from structured data sources. Through practical healthcare examples, it demonstrates how to transform diverse data into a unified knowledge representation. You'll learn about semantic integration through ontologies, compare different knowledge graph technologies, and gain hands-on implementation guidance. It also explores biomedical knowledge graphs across multiple application areas and provides a comprehensive case study on microRNA-disease associations.

Part III: Building Knowledge Graphs from Unstructured Text

This part explores how Knowledge Graphs and LLMs can extract, structure, and represent knowledge from unstructured text. It guides you through the complete lifecycle of text-to-knowledge-graph conversion, from entity extraction to relationship mapping. Through real-world case studies, you'll learn how to transform unstructured documents into queryable knowledge representations, combine traditional NLP with modern LLM capabilities, and leverage external knowledge bases to enrich extracted information.

Part IV: Machine Learning on Knowledge Graphs

This part demonstrates how representation learning and graph neural networks (GNNs) can transform static knowledge into dynamic, learnable features that power sophisticated downstream tasks. You'll explore how the synergy between knowledge graphs and machine learning enables neural network-based representations, flexible feature engineering, and automated knowledge extraction. Through practical examples of fraud detection, drug repurposing, anti-money laundering, and recommendation systems, you'll see how these techniques tackle complex business challenges.

Part V: Information Retrieval with Knowledge Graphs and LLMs

The final part focuses on leveraging the combined power of Knowledge Graphs and LLMs for accurate and reliable information retrieval. You'll learn how to implement systems that use knowledge graphs as ground truth to enhance LLM capabilities while preventing hallucinations. Through case studies in archival research and law enforcement, you'll see how to build sophisticated question-answering systems that emulate domain expert reasoning and provide transparent, explainable results.

This book condenses the best of both worlds—Knowledge Graphs and LLMs —to create useful intelligent advisor systems that support humans in critical decision-making processes across healthcare, law enforcement, and other crucial domains. It provides detailed, concrete examples of how to make the best use of their combined capabilities, offering practical, production-focused guidance for the modern era where these technologies work in concert.

We hope that what you'll get access to will be of interest to your current knowledge-driven project and to your learning path as a knowledge worker, data scientist, data engineer, or data architect and that it will occupy an important place in your digital or, even better, physical bookshelf.

Please be sure to stop by liveBook Discussion forum with any feedback you have. With your help, we are sure the final book will be great!

Yours,

—Alessandro Negro, Vlastimil Kus, Giuseppe Futia, Fabio Montagna

In this book

welcome Part I: Foundations of hybrid intelligent systems 1 Knowledge graphs and LLMs: a killer combination 2 Intelligent systems: a hybrid approach Part II: Building knowledge graphs from structured data sources 3 Create your first Knowledge Graph from ontologies 4 From simple networks to multi-source integration Part III: Building knowledge graphs from text 5 Domain-specific knowledge extraction from unstructured data 6 Building knowledge graphs with large language models 7 Named entity

disambiguation 8 NED with open LLMs and domain ontologies Part IV: Machine learning on knowledge graphs 9 Machine learning on knowledge graphs: a primer approach 10 Graph feature engineering: manual and semi- automated approaches 11 Graph representation learning and graph neural network 12 Node classification and link prediction with GNNs Part V: Information retrieval with knowledge graphs and LLMs 13 Knowledge graph-powered retrieval augmented generation 14 Ask a KG with natural language 15 Building a QA agent with LangGraph Appendix A. Introduction to graphs Appendix B. Neo4j Appendix C. Build knowledge graphs from structured sources

## Part_I:_Foundations_of_hybrid_intelligent_systems

The convergence of Knowledge Graphs (KGs) and Large Language Models (LLMs) marks a pivotal moment in the development of intelligent systems. This part lays the theoretical and practical foundation for understanding how these complementary technologies can work together to create more powerful and effective solutions. While KGs have long provided a robust framework for representing structured knowledge and supporting explicit reasoning, and LLMs have recently revolutionized natural language understanding and generation, their combination opens new possibilities that were unattainable just a year ago.

The integration of these technologies addresses the key limitations of each approach while amplifying their strengths. KGs provide the explicit, verifiable, and updatable knowledge representation that LLMs often lack, while LLMs offer the natural language understanding and generation capabilities that make complex knowledge structures more accessible. This synergy enables the development of intelligent systems that can:

Handle both structured and unstructured data effectively Combine multiple types of reasoning strategies Provide explainable and verifiable results Continuously update their knowledge base Interact naturally with users while maintaining accuracy and reliability

Throughout this book, we explore this hybrid approach across various domains including healthcare, law enforcement, financial services, and retail, demonstrating how these combined technologies can tackle complex real- world challenges that neither could address effectively alone.

Chapter 1 introduces the powerful combination of KGs and LLMs, establishing their complementary nature and illustrating how they can enhance each other's capabilities through concrete examples and use cases. It

sets the stage for understanding the transformative potential of this hybrid approach.

Chapter 2 explores the fundamental concepts of intelligent systems, diving deep into different types of knowledge representation and reasoning strategies. It illustrates how KGs and LLMs can work together in practice, examining their roles in knowledge acquisition, representation, and various forms of reasoning from deductive to probabilistic.

The frameworks and concepts introduced in these chapters provide the foundation for the practical implementations and advanced applications discussed throughout the remainder of the book.

## 1_Knowledge_graphs_and_LLMs:_a_killer_combination

This chapter covers

Introduction to Knowledge Graphs (KGs) Introduction to Large Language Models (LLMs) How to combine KGs and LLMs to get the best of two How to build data-driven applications using KGs and LLMs

Artificial intelligence (AI) has transformed how we work and live, with Generative AI technologies revolutionizing business after business. For example, ChatGPT, launched by OpenAI on November 30, 2022, reached 1 million users in days and over 100 million in months, marking the beginning of a new era where simple chat interfaces allow users to interact using natural language without programming knowledge.

Despite this success, these technologies fall short in domains like biomedicine, law enforcement and others where specific domain knowledge, high accuracy, and explainability are mandatory. Limitations include black- box models with limited explainability, infrequent updates due to lengthy training cycles, generic training unsuitable for specialized domains, hallucination, and lack of context and relations. This is where knowledge graphs come into play, providing the contextual information, like experiences, environmental characteristics, cultural aspects, and social norms, needed to build the "third wave of AI" [1] for mission-critical applications.

Knowledge graphs are the perfect companion for Generative AI, complementing and supporting each other during development of complex applications. This book explores how a 40-year-old idea—knowledge graphs —can be a core engine for new disruptive technology, and vice versa, how generative AI empowers knowledge graphs to overcome new challenges.

This book teaches the powerful combination of Knowledge Graphs (KGs)

and Large Language Models (LLMs) as complementary technologies that overcome each other's individual limitations. We define knowledge graphs as sophisticated graph structures where nodes represent real-world entities (people, places, diseases, proteins), relationships define meaningful connections between them, and properties provide additional context. LLMs are massive neural networks trained on enormous text corpora that excel at natural language understanding and generation. While KGs provide structured, explainable knowledge representation but are challenging to build and query, LLMs offer natural language processing capabilities but suffer from hallucination, stale information, and lack of domain-specific grounding. Together, they create what we call a "killer combination" where LLMs can extract entities and relationships from unstructured text to build KGs more efficiently and provide more autonomous and powerful graph querying and analysis while KGs provide reliable, up-to-date domain knowledge to ground LLM responses and prevent hallucinations.

We focus on addressing critical needs in mission-critical domains like healthcare, law enforcement, finance and many more where accuracy, explainability, and domain expertise are mandatory—areas where generic AI often falls short. Our intended readers are knowledge graph experts, data scientists, and LLMs practitioners who need to build sophisticated data- driven applications that can harmonize disparate data sources, provide natural language interfaces, and deliver contextually grounded intelligent responses. Through this book, you'll learn to adopt business-focused approaches, model KG schemas, utilize LLMs for entity extraction, validate information integrity, and create conversational AI systems that can answer complex domain-specific questions using both structured and unstructured organizational data. We teach through practical, concrete examples drawn from our direct experience in over ten years of consultancy in this exciting field, focusing on real-world use cases rather than abstract theory.

1.1 Knowledge graphs

Knowledge graphs (KGs) have emerged as a core abstraction to incorporate a structured representation of human knowledge into machines, allowing them to behave intelligently. This process is realized by creating a sophisticated graph structure in which:

Nodes represent real-world entities (people, places, organizations, diseases, proteins, genes); Relationships define meaningful connections between these nodes (a person born in a place, a disease causing symptoms, a gene encoding a protein); and properties provide additional context (birth dates, geographic coordinates, organizational history, disease descriptions in multiple languages).

Figure 1.1 illustrates this concept through a healthcare KG where entities (diseases, drugs, compounds, anatomical structures) are explicitly connected through meaningful relationships. This explicit representation enables machines to perform reasoning and inference on the structured knowledge, supporting complex intelligent systems for decision-making.

Figure 1.1 Example of a KG in the healthcare domain. The nodes (the circles) represent the entities, like people, diseases, anatomic parts, etc. The edges represent meaningful connections among entities. Both nodes and edges have properties describing relevant details.

Despite their effectiveness and advantages in supporting the development of intelligent systems, knowledge graphs haven’t been widely adopted for many years. The primary factors that make KG adoption challenging for intelligent systems development include:

The costs in terms of time, effort, and money to build and maintain them.

The intricate access patterns required to navigate across multiple hops. The structure of the results with information scattered across multiple nodes and relationships.

Building a knowledge graph requires recognizing and extracting relevant entities and connections from multiple and heterogeneous data sources, both structured and unstructured. Dealing with structured data sources is several orders of magnitude simpler – in terms of complexity – compared with unstructured. In structured, or semi-structured, data – coming from relational databases, or files in CSV, XML, or JSON format – the items are already isolated, identified, and, often, typed. Nevertheless, the process is complex since it requires a mapping from multiple original schemas to the common graph schema and the presence of typos and mistakes didn’t facilitate, but, still, it is controllable and fully predictable.

Unstructured data is another beast to tame. Even though any human language has a lot of structures driven by grammatical rules, syntactic dependency, and other conventions, extracting information from text has always been a complex task, mainly for the following reasons:

Multiple languages: Each language has its own set of grammatical rules, vocabulary, idioms, and nuances. Processing text in multiple languages requires a system to understand and manage these differences. Additionally, languages can have unique writing systems (e.g., Latin, Cyrillic, Chinese characters), making it necessary to handle various scripts and character encodings. Typos: Human-written text often contains typographical errors, misspellings, and other mistakes. These errors can lead to significant challenges in understanding the intended meaning of the text. For instance, a simple typo might change a word entirely, confusing the context and requiring sophisticated algorithms to recognize and correct such errors accurately. Pronouns: Pronouns refer to previously mentioned or understood nouns and can be ambiguous without proper context. Resolving what a pronoun refers to (a process known as coreference resolution) is crucial for accurate text comprehension but can be highly complex. For example, in the sentence "John saw Bob and he waved," it is unclear

whether "he" refers to John or Bob without additional context. Different styles in writing: People have diverse ways of expressing themselves, from formal to informal, technical to colloquial. These variations can affect how information is conveyed and interpreted. Additionally, different authors might use synonyms, varied sentence structures, and unique expressions, making it challenging for a system to maintain consistency and understand the text accurately across different writing styles. Domain-specific terminology and concepts: Each specialized field employs unique vocabularies, technical jargon, and conceptual frameworks that require domain expertise to understand and extract accurately. For example, medical notes contain clinical terminology, drug names, and diagnostic codes that differ significantly from legal documents with their statutory references, case law citations, and juridical language.

Overall, these factors contribute to the complexity of processing human languages, necessitating advanced techniques and robust systems to accurately extract and interpret information from unstructured text.

Once the knowledge graph has been built, the journey is just beginning. At this stage, it contains a vast set of interconnected information, referred to as knowledge, which needs to be accessed correctly to obtain the right answers to our questions and support intelligent systems. The flexibility in defining the schema for a knowledge graph is advantageous for dealing with heterogeneous data sources and handling the complex connections of the domain being modeled. However, it also complicates access to this knowledge for those who do not know how to query it properly. It is possible to prepare upfront queries and analyses to build specific intelligent systems, but this severely limits the types of users or the support these systems can provide. Last but not least, once the results have been obtained often, they are difficult to interpret by non-experts and in different user interfaces.

Let’s consider some examples to clarify our point here. Perhaps the most well-known early adopter of knowledge graphs has been Google, which used them to enhance the user search experience by providing "relevant" connected information when users search for something. This approach

emphasizes searching for things, not just strings, showcasing an impressive use of knowledge graphs that has heightened interest in this technology. While this usage enables all users to access knowledge graphs for finding relevant information, the potential of this valuable knowledge source is limited to search applications. In contrast, it could be utilized for various purposes.

On the flip side, many customers use knowledge graphs for law enforcement and policing tasks. The knowledge graphs they create contain a wealth of useful information accessed in multiple ways by analysts to answer complex investigative questions. In this scenario, the power of knowledge graphs is fully harnessed. However, due to the complexity of the graph querying process and the need for specific interfaces, this potential is confined to a smaller user base.

Imagine a scenario where individuals – like doctors or police officers – can pose questions to the knowledge graph using natural language, and the intelligent system could find the correct answers by querying the graph effectively, potentially transforming the results into quick, simple summaries. We hope you grasp the direction we are heading in. This future vision is our dream, which we aim to show and construct throughout this book.

1.2 Large Language Models

For the reasons we just mentioned, generative AI can be pivotal in developing intelligent applications. In particular, Large Language Models (LLMs), which specialize in handling natural language, offer the necessary support to eliminate all barriers to the complete evolution of intelligent systems that leverage knowledge graphs as the core technology. These systems can aid various types of users in accomplishing tasks in complex domains like finance, biomedicine, law enforcement, retail, and much more.

The foundation of LLMs lies in transfer learning—the ability to reuse patterns learned in generic tasks (like predicting masked tokens) for specific tasks (like relation extraction) [2]. This breakthrough shifted the paradigm from training many small, task-specific models to training few large models reusable across multiple tasks, significantly reducing required training data

and computational resources. Figure 1.2 describes, at a high level, how transfer learning works.

Figure 1.2 Transfer Learning high-level principles. In transfer learning a model (or part of it) trained on a specific task (e.g. predicting randomly masked tokens) is then copied to be part of the training and the prediction for another task (e.g. relation extraction).

This significantly reduces the required training dataset size (and the overall training time and resources) for supervised learning and poses a significant milestone in shifting the mindset from having many small models, one for each specific task, to training a few big models and reusing them for multiple tasks. The introduction of pre-trained language models (PLMs) trained with transformer architectures [3] on large-scale corpora has demonstrated powerful capacities in addressing diverse natural language processing (NLP) tasks with a single big model. Researchers discovered that enhancements in model scaling could lead to heightened model capacities. As a result, further investigations into scaling effects were conducted by increasing the parameter scale to even greater sizes. When these parameters exceeded a particular scale, these expanded language models exhibited not only substantial performance improvements but also specific capabilities (like context learning) absent in smaller models such as BERT [4]. To distinguish between models of different parameter scales, the term large language models (LLMs) has been used to refer to PLMs of significant scale, typically containing tens or hundreds of billions of parameters.

The emergence of LLMs such as GPT-2 [5] marked a transformative moment in the field of artificial intelligence. These models comprise hundreds of billions of parameters and are trained on enormous volumes of textual data. Their modern counterparts, including GPT-4 [5], Gopher [6], and PaLM [7], breathe new life into the phrase “unreasonable effectiveness of data” [8]. To summarize, their performance is deemed "unreasonable" for three interconnected reasons:

1. Model complexity, a.k.a. number of parameters 2. Size – and in the case of GPT also quality! - of the training corpus 3. Many tasks requiring human intelligence can be reduced to next token

prediction with an adequately powerful model.

As shown in the paper “Scaling Laws for Neural Language Models” [9], larger neural models (i.e., more trainable parameters) require fewer data samples to reach the same performance in terms of the test-set loss. For example, OpenAI’s GPT family of models started with about 120 million parameters (GPT-1), continued to 1.5 billion (GPT-2), and 175 billion (GPT- 3.5), and it is rumored that the latest one, GPT-4, which became available in

the 2nd half of 2023, has round 1.7 trillion of parameters. If only humanity’s wisdom grew that fast!

The same paper also proves that the size of the training corpus is of paramount importance. Not surprisingly, the more data the model has to learn from, the better it gets. But it's not just about data quantity—data quality is equally crucial for training high-quality machine learning models, particularly LLMs. This highlights a fundamental shift in approach: traditional Machine Learning typically followed a model-centric paradigm, focusing on identifying the best model architecture and fine-tuning hyperparameters. This approach, in its extreme form, often overlooked potential data quality issues—it should not be surprising that faulty data leads to faulty predictions. What has gained significant traction over time is the data-centric paradigm, which prioritizes data engineering to improve both the quality and quantity of data used for training high-complexity ML models. The recognition that data size and quality are the most critical factors in training high-performance models has been one of the key driving forces behind the success of LLMs. Due to these data-focused improvements, LLMs became so powerful that we can simply formulate tasks in natural language and the model generates accurate answers, with minimal model engineering required. Figure 1.3 summarizes the key elements and characteristics of LLMs, differentiating them from the past approaches.

Figure 1.3 LLMs building blocks and differentiating characteristics.

Once the model architecture becomes sufficiently complex, what matters the most to accomplish a wide range of inference objectives is the training data quality, and at the inference stage the so-called prompt engineering, i.e. the formulation of a task (more about that in the next chapters).

1.3 KGs and LLMs: stronger together

How has the advent of LLMs impacted our day-to-day work as knowledge graph experts or data scientists? Knowledge graphs and LLMs work perfectly in tandem because each can support the other in delivering better service, and together they can boost the implementation of powerful – in terms of capabilities – intelligent systems. LLMs can assist knowledge graph-based solutions in various ways. In this book, we are particularly interested in:

Knowledge Graph Building: Specifically, focusing on how to extract relevant concepts and connections from unstructured data. This task has traditionally been complex, necessitating the training of custom NLP models to tailor the extraction to specific domain needs. For instance, it is essential to develop models that can recognize diseases, treatments, and other specifics in medical domains. This typically requires considerable effort from domain experts. LLMs have greatly simplified this process by providing an extensive model that can serve multiple purposes with minimal configuration (such as the prompt, which we will discuss in concrete examples later in the book). Knowledge graphs building with and without the LLMs support will be covered in the second part of the book. Figure 1.4 summarizes how LLMs participate in the knowledge graph-building process. Knowledge Graph Querying: Knowledge graphs often store vast amounts of information and create connections among distant concepts. Extracting knowledge from these intricate connections sometimes requires traversing multiple hops from the starting concept to the destination. Such traversals often necessitate a thorough understanding of a significant portion of the schema and a detailed knowledge of the query language to access it. LLMs can assist in extracting relevant and precise information, thereby supporting querying and information search. This aspect will be dealt with in the third part of the book.

Summarization: In numerous scenarios, it may be more practical to return information extracted from the knowledge graph in text form rather than in tabular, graph, chart, or other formats. This feature also allows for various utilization scenarios where users might not be able to interact with a screen directly. For instance, think about police officers driving a car and querying the system about where a specific car plate has been seen in the past three hours.

Figure 1.4 Knowledge graph building with and without LLMs and LLMs support for querying and retrieval.

Conversely, LLMs can greatly benefit from the support of knowledge graphs (KGs). LLMs have various limitations that can reduce their utility in specific

scenarios where domain-specific, accuracy, transparency, and up-to-dateness are critical. Here, we present a non-exhaustive list of major drawbacks of LLMs that can be resolved or mitigated through different levels of knowledge graph integration [10]:

Hallucination: The reliability of LLMs is significantly affected by the hallucination problem, where they generate inaccurate information. Despite ongoing efforts to address it, the issue of hallucination is likely to persist in the realm of LLMs for the foreseeable future. To aid in detecting or mitigating hallucinations, KGs offer reliable information that can serve as a foundation for the answers provided by leveraging LLMs. By combining LLMs and KGs, researchers can develop comprehensive solutions, discussed later in part 3 of the book, that can identify or prevent hallucinations in various domains. Stale Information: LLMs possess the ability to store extensive real- world knowledge but struggle to update their internal knowledge to reflect changes or new information. KG-based prompts offer a potential solution for modifying knowledge in LLMs, but they are restricted to handling basic tuple-based knowledge in KGs. Even if the entire LLM undergoes re-training, the knowledge presented through prompts would likely be assimilated within the vast network structure of the LLM. Explainability: While pre-training and knowledge editing methods can bring LLMs up to date with the latest information, they require access to the internal structures and parameters of LLMs. However, many cutting- edge LLMs only offer APIs that allow users and developers to interact with them, keeping their internal workings hidden from the public. Consequently, traditional KG injection techniques that involve modifying LLM structures with additional knowledge fusion modules cannot be employed. One potential solution is to convert various types of knowledge into different text prompts. However, it remains an area of ongoing research to determine if these prompts can effectively adapt to new LLMs. Additionally, the approach of using KG-based prompts is constrained by the length of input tokens accepted by LLMs. Therefore, the question of how to enable effective knowledge injection for black- box LLMs remains unanswered.

Figure 1.5 summarizes how these two technologies support and complement

each other.

Figure 1.5 Summary of how LLMs and KGs can complement each other. Inspired by [11].

As illustrated in figure 1.5, this collaboration generates a flywheel effect: the continuous integration builds momentum, enhancing the delivery of advanced functionalities.

1.4 The paradigm shift in data-driven applications

Traditional paradigms build systems for specific purposes with structured, homogeneous databases. This approach works for tailored needs but becomes impractical for complex domains requiring adaptation to user characteristics and heterogeneous data integration.

Knowledge graphs offer fundamental advantages over traditional RDBMS by treating relationships as first-class citizens rather than secondary constructs. This design naturally captures connections, enabling relationship discovery through graph pattern matching and traversal. Both RDF and LPG provide machine-readable formats remaining interpretable by humans.

LLMs provide frameworks for understanding human language and retrieval but don't aid knowledge management. KGs assign central roles to data with rich, meaningful representation exploitable by humans and machines, realizing the paradigm shift where intelligent behavior is encoded once in a unique source of truth.

According to McKinsey & Company [12], addressing data fragmentation can cut annual data spending by 5-15% in the short term. KGs overcome siloed data issues, creating unique knowledge sources for scenarios requiring holistic overviews while lowering data access barriers and enhancing governance.

1.4.1 The four pillars of knowledge graphs (KGs)

To encode this paradigm shift in a concrete implementation, we propose a new definition of a KG, synthesizing all features that impact both the technical and business sides:

Definition

A knowledge graph is an ever-evolving graph data structure composed of a set of typed entities, their attributes, and meaningful named relationships. Built for a specific domain, it integrates both structured and unstructured[1] data to craft knowledge for humans and machines alike.

This definition provides the groundwork for the four pillars of KGs:

Evolution; Semantics; Integration; Learning.

Evolution: Providing an ever-evolving graph data structure

Knowledge Graphs (KGs) allow us to continuously ingest, integrate, and unify various types of information into a single source to support this requirement. Additionally, compared to the rigid schema of relational databases, the graph structure of a KG is more flexible. Primarily based on nodes, edges, and properties, it can be easily extended, evolving according to the new needs of the analysis and the purposes we intend to achieve.

For example, consider a social media platform or an online retailer that tracks user interactions and preferences. As new features or categories are introduced and user behaviors change, the platform’s data model must adapt. A KG can seamlessly incorporate new types of interactions or content without needing a complete overhaul of the existing structure. This flexibility ensures that the platform can continuously adapt to new requirements and provide up-to-date insights based on the latest data.

Semantics: Maintaining a set of typed entities, their attributes, and meaningful relationships

Usually, we model information in traditional databases’ schema to maximize efficiency in reading and writing data for a specific purpose. Therefore, we often neglect the meaning of the data (or semantics) in favor of performance. The goal of a KG is to make explicit the semantics of the data, modeling information within a knowledge infrastructure characterized by typed entities

and meaningful relationships.

Compared with data lakes, KGs incorporate contextual data and metadata into a coherent structure. New data is combined with the existing data, and it becomes immediately available for performing analysis: the KG becomes a recipient to integrate and keep track of added information. Contextual knowledge emerges from this KG infrastructure, which drives our business activities and decisions. Such knowledge explicitly connects typed entities describing broad or narrow categorizations and supports, for instance, identity, transitive, or inverse relationships. This expressiveness in representing data opens the doors to explainability [13]. KGs provide a backbone for several reasoning mechanisms, ranging from consistency checking to causal inference.

Integration: Building for a specific domain from both structured and unstructured data

The Knowledge Graph (KG) serves as the central reference for all structured data and unstructured content related to a specific domain. In many use cases, such as law enforcement, policing, financial services, and healthcare, data is gathered from multiple sources—both internal and external to the organization—and integrated into a single source of truth. The flexibility of organizing knowledge using a KG makes this technology suitable for various scenarios. Since the KG represents information by focusing on the meaning of data, it allows practitioners to overcome challenges related to different data types, formats, and provenance, thereby coherently connecting information from multiple data sources.

For example, in the healthcare domain, patient information can be scattered across various systems including hospital databases, lab results, and insurance records. A KG can integrate all this information into a unified view. For instance, a doctor could use the KG to quickly access a patient’s comprehensive medical history, including previous treatments, lab results, and prescriptions, even if this data is stored in different systems. This holistic view helps in making informed decisions, improves patient care, and ensures that all relevant data is considered regardless of its original format or source. The same idea can be transposed to the law enforcement or policing use case.

Learning: Crafting knowledge for humans and machines alike

The KG represents a specific domain's core information and big picture and conveys value to humans and machines. Humans can analyze, visualize, and query graph data to extract clear insights. In this context, we include graph technology experts and domain experts who can benefit from KGs to address the challenges related to their activities.

Inference rules and machine learning algorithms are performed on top of KG to infer new information not explicitly encoded within the KG. The graph- based nature of KGs allows analysts to apply specific methods including centrality and connectivity analysis to identify the most influential nodes, network analysis to detect the shortest path between two nodes, and community analysis to recognize groups of similar nodes.

Figure 1.6 shows the four pillars that characterize a KG. For each pillar, we list the most important keywords.

Figure 1.6 The four pillars of KGs: evolution, semantics, integration, and learning

1.5 Building data-driven applications using KGs and LLMs

The KG paradigm shift enables us to create a new generation of data-driven systems to address complex problems. LLMs remove any obstacle related to the processing of unstructured information and make access to the

information more natural (using language, spoken or written). This section proposes a few examples of the potential use of KGs and LLMs in critical areas such as the healthcare industry, law enforcement, retail, and finance. Healthcare, in particular, is a frequent domain in the book because it has broad characteristics, issues, and requirements that can be easily applied to different domains with an equivalent set or a subset of them. Furthermore, the abundance of publicly available data makes healthcare an ideal domain, as it allows for the demonstration of real examples using data sets that are comparable with real-world use cases.

1.5.1 Example use case: Drug discovery and development

Drug development is a complex process that requires integrating knowledge from numerous domains, from biology to chemistry. For instance, in the context of target discovery, researchers need to understand the body’s mechanisms involved in a disease to develop a drug for treating it. Due to the complexity of this process, bringing a new drug to the market is a costly operation and has a high chance of failure. It becomes essential to develop fast and practical approaches to inform and drive the research activity dedicated to designing and creating a new drug.

Challenge: Medical and pharmaceutical data is vast, produced at unprecedented rates by diverse institutions using different formats. Integration must ensure data integrity, accuracy, and consistency while correctly contextualizing data points.

KG-based solution: Model diverse interactions between biological entities at different scales, connecting genes, diseases, and drugs using different relationship types. Typed relationships with multiple rules better represent domain meaning, enabling transitive bonds and reasoning mechanisms for inferred associations.

Role of LLMs: Process unstructured data from scientific publications, clinical reports, and databases, ensuring consistency in integrated knowledge bases. LLMs contribute to expanding KGs by analyzing literature corpora and inferring potential relationships, performing sophisticated text mining for chemical structures and experimental results.

In summary, the integration of LLMs into the drug discovery and development pipeline enhances data integration, augments KGs, facilitates hypothesis generation, improves information retrieval, and enables advanced text mining. These capabilities collectively drive more efficient and informed drug discovery processes, ultimately accelerating the development of new therapeutics.

1.5.2 Example Use Case: Conversational AI for customers support

The development of personalized assistant systems has gained significant importance for various applications, providing users with relevant information and recommendations. These systems must not only answer user queries but also possess the ability to ask follow-up questions, creating intricate conversational flows.

Traditional rules-based engines struggle to achieve this level of sophistication. They are inflexible and cannot adapt to unforeseen demands due to their lack of access to contextual and specific knowledge. Consequently, an effective tool needs to combine general expertise with specific user requests, managing vast amounts of information efficiently.

Challenge: Most decision trees powering old-fashioned conversational systems are optimized for specific use cases, making it impractical to reuse this knowledge across multiple scenarios. Even when the right content is available, unstructured data often contains redundant, duplicated, and irrelevant details. Despite advancements in natural language generation (NLG) techniques, answers from language models can become, as recognized by Zhang et al. [14], repetitive and uninformative. For a conversational system to provide useful suggestions and information, it needs to extract relevant entities and relationships from the text while being supported by external and other internal structured knowledge to ground the conversation.

KG-based solution: Zhang et al. [14] claim that “conversations often develop around knowledge.” In particular, natural conversations evolve around concepts that form this knowledge. Unlike traditional rule-based systems, KGs offer a significant advantage by connecting such concepts and

establishing meaningful relationships. KGs can ground conversations, integrate additional information, and support response generation. In this setup, response generation is influenced by the content of the conversation – with key factors (entities and relationships) extracted using NLP technologies – and the graph-based background context of the dialogue – which drives the conversational flow, helping users find the answers they need.

Role of LLMs: With ChatGPT and sequent versions, Large Language Models (LLMs) have revolutionized the field of conversational AI. These models, trained on vast datasets, can generate human-like text and understand complex language patterns. LLMs can handle a wide range of topics and provide coherent responses, making them valuable for building sophisticated conversational systems. However, without additional contextual grounding, the responses from LLMs can become generic, lack depth, and cannot be related to the organization’s data.

By integrating LLMs with specific knowledge sources, such as Knowledge Graphs (KGs), the conversational system can enhance its responses. LLMs can leverage the structured information within KGs to provide more accurate and contextually relevant answers. This integration allows the system to navigate complex queries, offer precise information, and maintain a natural conversational flow. Part three of this book will present an extensive guide on how LLMs can be used to answer very specific questions in the organization domain and using internal and external data of any type.

1.5.3 What should I ask myself?

The previous examples show typical scenarios in which the role of a KG becomes crucial, and despite their diversity, these scenarios are characterized by common challenges. We therefore have raised the following questions, which can be helpful to guide the reader in understanding whether a KG is the right solution to address their business and technical challenges.

Consider KGs if you answer yes to:

Do I need to harmonize disparate data silos into consistent overviews? Do I need to connect data meaningfully across structured and

unstructured sources? Do I need flexible data representation where structure evolves? Do I need to track pipeline provenance and consistency? Do I need to equip advanced search and recommendation services? Do I need to visualize network structures showing communities and interdependencies? Do I need to apply ML models that benefit from relational data nature?

Consider LLMs if you answer yes to:

Do I need to extract entities and relationships from unstructured data? Do I need to interpret complex user queries for accurate answers? Do I need to provide conversational interfaces? Do I need to summarize comprehensive results into text?

If you answered yes even to just one of them, you need LLMs to empower your KG-based solution.

1.6 Knowledge graph technologies

This book adopts a technologically agnostic approach and provides code examples, interchanging wherever possible two of the most common paradigms in creating and querying KGs:

The Resource Description Framework (RDF) and the SPARQL query language, both defined by the World Wide Web Consortium (W3C). The labeled property graph (LPG) approach and query languages such as openCypher[2] and Gremlin[3], were adopted by graph technology vendors such as Neo4j, Amazon Neptune, and many others.

RDF is a data model that focuses on knowledge representation, where the graph is encoded as a collection of statements or triplets. It aims to standardize data publication and sharing on the web, ensuring semantic interoperability. In the RDF, the core of intelligent systems is based on reasoning performed on the semantic layer of the available statements (the triplets).

The LPG representation, on the other hand, focuses on the structure (properties and relationships) of the graph. This model allows nodes and edges to have properties, emphasizing the features of the graph data. This enables new opportunities in terms of data analysis, visualization, and the development of graph-powered machine learning systems to infer further information.

Both RDF and LPG aim to facilitate semantic representation, each with distinct strengths. RDF, with its SPARQL query language, excels at data interoperability and consistency across systems through standardized statements, offering powerful hyper-graph and federation features that enable linking different RDF graphs together with rich contextual information. LPG implementations, using query languages like openCypher and Gremlin, provide advantages in pathfinding queries and graph traversal operations. A key architectural difference lies in how relationships are handled: in LPG, each edge is unique with its own identity and properties, while in RDF, relationships are global predicates that can be reused across different statements throughout the knowledge base.

While RDF and LPG are distinct paradigms, they can be complementary depending on the use case. RDF excels in scenarios requiring semantic consistency, web-scale interoperability, and the ability to leverage ontologies for knowledge inference, while LPG advantages lie in rich property-based representations and efficient graph traversals. Despite these differences, there are ongoing developments exploring how these paradigms might converge or integrate to leverage the strengths of both approaches.

1.6.1 Taxonomies and ontologies

The modern implementation of KGs must use the traditional features of graph data and, as defined by [15], the organizing principle enabled by the semantics, turning the latent knowledge of a graph into a KG. As we already saw, graph models can be instantiated from a collection of statements (RDF) or instantiated through the LPG model, which focuses on a graph-based structure that includes attributes to nodes and edges. However, merely incorporating this structural information does not fully capture the richness of relationships within the data. For instance, without additional semantic

information, we cannot determine if one category is broader than another, if a relationship is the inverse of another, or if it is transitive. We can inject these semantic features into KGs by employing taxonomies and ontologies:

Taxonomies enable us to represent the hierarchical dimension of the data, organizing categories in broader-narrower relationships. For example, in a taxonomy, a "Vehicle" category might be broader than a "Car" category, which in turn is broader than a "Sedan" category. In complex KGs, multiple taxonomies can be integrated allowing the graph to be used more explicitly for computing, for instance, semantic similarities and enabling novel research directions. Ontologies allow us to introduce more complex relationships beyond simple hierarchies. By utilizing ontologies, we can make explicit the relationships of identity, difference, and more intricate interconnections between entities. For instance, an ontology might specify that "Car" and "Automobile" are identical (synonyms), while "Car" and "Bicycle" are disjoint (cannot be the same). Ontologies support expressive class definitions including union, complement, disjointness, and cardinality restrictions. They shape the core of the knowledge representation, capturing the conceptual structure of the domain. Without an ontology, a vocabulary remains vague as it does not explicitly encode the intrinsic relationships between concepts.

Traditional approaches to defining taxonomies and ontologies are often abstract and demand strict completeness (covering the entire domain) and exclusivity (each concept belongs to only one group). These constraints aimed to create comprehensive and unambiguous representations, but they also introduced rigidity and complexity. Issues such as the need for exhaustive domain coverage and rigid categorization often made these systems less adaptable to evolving knowledge and diverse data sources.

Modern KGs adopt a more pragmatic approach characterized by "just enough semantics." This involves selecting a subset of ontology features that address current issues without being overly prescriptive. For example, in a healthcare KG, instead of requiring a complete classification of all possible diseases, symptoms, and treatments, practitioners might focus only on the specific medical domains relevant to their application—such as oncology or

cardiology—while leaving room to expand into other specialties as needed. Similarly, a financial services KG might initially model only the relationships between customers, accounts, and transactions that are essential for fraud detection, rather than attempting to capture every possible banking relationship from the outset.

This selective approach allows KGs to evolve and expand their expressive capacity incrementally. Rather than enforcing rigid, complete taxonomies, modern KGs integrate partial ontologies that can be extended organically. A retail recommendation system, for instance, might begin with basic product categories and customer preferences, then gradually incorporate more sophisticated concepts like seasonal trends or demographic patterns as the business requirements mature. This flexibility enables more dynamic and scalable knowledge representation that adapts to real-world constraints and evolving business needs.

1.7 How do we teach knowledge graphs and LLMs?

This book equips readers with essential tools for creating and utilizing KGs while demonstrating how to leverage LLMs for advanced intelligent applications. You will learn to:

Adopt business-need mindsets focusing on goals, then data, then algorithms Model KG schemas considering future extensions, taxonomies, and ontologies Import data from structured sources and map entities/relationships to schemas Utilize LLMs for extracting domain-relevant entities and relationships from text Validate ingested information ensuring integrity and accuracy Perform analysis using latest ML technologies like Graph Neural Networks Query and visualize graph portions, leveraging LLMs for natural language questions

All these concepts will be explained through specific, concrete, and practical

examples drawn from our direct experience in over ten years of consultancy in this exciting field.

1.8 Summary

LLMs and KGs empower each other by overcoming individual limitations when used in isolation. LLMs support KG creation from structured data and simplify the querying phase. KGs provide ground knowledge for LLMs to answer domain-specific questions using up-to-date and private data. Data-driven systems with contextualized knowledge are strategic for high-impact applications like recruitment tools and medical predictions. KGs represent a core abstraction for incorporating human knowledge into machines, while LLMs provide natural language understanding capabilities. KG and LLM adoption represents a paradigm shift where intelligent behavior is encoded once in a unique source of trust. This empowers data representation for different applications and diverse tasks. KGs are ever-evolving graph data structures containing typed entities, their attributes, and meaningful relationships. They are built for specific domains from structured and unstructured data to craft knowledge for both humans and machines. KGs have four pillars: evolution, semantics, integration, and learning. KGs and LLMs support critical domains with data-driven decisions across multiple applications. These include Customer 360 for banking, drug discovery, retail recommendations, and conversational systems. Two key technologies represent KGs: Resource Description Framework (RDF) and labeled property graphs (LPG). Taxonomies and ontologies play fundamental roles by incorporating semantic metadata that makes traditional graphs smarter.

1.9 References

1. J. Launchbury. “A DARPA Perspective on Artificial Intelligence”

(2020). Accessed: May 5, 2022. [Online].

2. J. Yosinski, et al., "How transferable are features in deep neural

networks?," Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2 (p./pp. 3320--3328), Cambridge, MA, USA: MIT Press, Accessed: July 26, 2024. [Online]. Available: https://arxiv.org/abs/1411.1792 3. A. Vaswani et al., "Attention is all you need," In Proceedings of the 31st

International Conference on Neural Information Processing Systems (NIPS'17). Curran Associates Inc., Red Hook, NY, USA, 6000–6010, 2017. Accessed: July 26, 2024. [Online]. Available: https://arxiv.org/abs/1706.03762 4. J. Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers

for Language Understanding," 2018, Accessed: July 26, 2024. [Online]. Available: https://arxiv.org/abs/1810.04805 5. A. Radford et al., "Language models are unsupervised multitask

learners," 2019. Accessed: July 26, 2024. [Online]. Available: https://api.semanticscholar.org/CorpusID:160025533 6. OpenAI, GPT-4 Technical Report. 2023, Accessed: July 26, 2024.

[Online]. Available: https://arxiv.org/abs/2303.08774 7. J. W. Rae et al., "Scaling language models: Methods, analysis & insights

from training Gopher," arXiv preprint arXiv:2112.11446, 2021, Accessed: July 26, 2024. [Online]. Available: https://arxiv.org/abs/2112.11446 8. A. Chowdhery et al., "PaLM: Scaling language modeling with

pathways," The Journal of Machine Learning Research, Volume 24, Issue 1 Article No.: 240, Pages 11324 - 11436, 2023, Accessed: July 26, 2024. [Online]. Available: https://dl.acm.org/doi/10.5555/3648699.3648939 9. J. Kaplan et al., “Scaling Laws for Neural Language Models,” 2020,

arXiv:2001.08361 [cs.LG], Accessed: July 26, 2024. [Online]. Available: https://arxiv.org/abs/2001.08361 10. J. Z. Pan et al., "Large Language Models and Knowledge Graphs:

Opportunities and Challenges," Transactions on Graph Data and Knowledge, 1, 2:1--2:38. doi: 10.4230/TGDK.1.1.2, 2023, Accessed: July 26, 2024. [Online]. Available: https://arxiv.org/abs/2308.06374 11. S. Pan et al., "Unifying Large Language Models and Knowledge

Graphs: A Roadmap," IEEE Transaction on Knowledge Data Engineering, 36, 3580-3599. 2024, Accessed: July 26, 2024. [Online].

Available: https://ieeexplore.ieee.org/document/10387715 12. D. Grande et al., “Reducing data costs without jeopardizing growth.”

McKinsey Digital, 2020. Accessed: May 5, 2022. [Online]. Available: https://www.mckinsey.com/business-functions/mckinsey-digital/our- insights/reducing-data-costs-without-jeopardizing-growth 13. F. Lecue, “On the role of knowledge graphs in explainable AI,”

Semantic Web, vol. 11 no. 1, pp. 41-51, 2020. Accessed: May 5, 2022. [Online]. Available: http://semantic-web- journal.org/system/files/swj2259.pdf 14. H. Zhang et al., “Grounded conversation generation as guided traverses

in commonsense knowledge graphs,” arXiv Labs, arXiv preprint arXiv:1911.02707, May 5, 2020. Accessed: May 5, 2022. [Online]. Available: https://arxiv.org/abs/1911.02707 15. J. Barrasa, A. E. Hodler, and J. Webber, Knowledge Graphs. Sebastopol,

CA, USA: O'Reilly Media, Inc., 2021. Accessed: May 5, 2022. [Online]. Available: https://www.oreilly.com/library/view/knowledge- graphs/9781098104863/

[1] Structured data is highly organized and easily searchable data, typically stored in databases or spreadsheets, and adheres to a predefined format (e.g., rows and columns). Examples include customer information in CRM systems or financial records. Unstructured data lacks a predefined format or structure, making it more challenging to organize and analyze. It includes text, images, audio, and video content, such as emails, social media posts, and multimedia files.

[2] https://opencypher.org/

[3] https://tinkerpop.apache.org/gremlin.html

## 2_Intelligent_systems:_a_hybrid_approach

This chapter covers

Key design concepts and architecture for intelligent advisor systems How hybrid systems leverage the complementary strengths of KGs and LLMs Integration strategies for combining KGs and LLMs in intelligent advisor systems

In this chapter, we embark on a journey to explore the foundational concepts behind intelligence and intelligent behavior, which are essential for designing effective systems for our purposes. At the core of our discussion lies the application of knowledge graphs (KGs) and large language models (LLMs) to solve highly complex problems, like helping physicians heal patients, aiding law enforcement analysts in fighting crime, assisting credit card providers in combating fraud and many others. These types of problems require the combination of existing knowledge and context with reasoning capabilities, to build proper intelligent systems.

To make informed decisions about the trustworthiness and safety of intelligent systems for critical applications – that are the type of systems we would like to design and implement in this book –, we must know how these systems operate internally, avoiding the attribution of capacities they lack while leveraging their genuine capabilities. Understanding the inner workings of intelligent systems is crucial for AI practitioners. By dissecting the functioning of these systems, we can better understand their limitations and leverage their potential.

2.1 What is intelligence?

To begin, we must address a fundamental question: what is intelligence? As

many dictionaries define it, intelligence is the ability to acquire and apply knowledge. This definition highlights a crucial aspect of intelligence: learning from experience. Acquiring, retaining, and utilizing knowledge to solve problems, interact with the environment, and abstract for specific instances is critical for any agent — be it human, animal, electronic, or mechanical — to exhibit intelligent behavior. Human beings, for example, make decisions based on their accumulated knowledge about the world, a process that often occurs effortlessly and unconsciously. Humans rarely contemplate the mechanisms behind knowledge acquisition and representation in our brains or how this knowledge transforms into intelligence. This natural process, honed by evolution, provided us a competitive edge over other species, a phenomenon further refined by natural selection.[1] In this book, instead, we need to dig into these processes by disassembling the various tasks and components, to design first a high-level architecture of an intelligent system and then a detailed structure of each element.

The simplest definition of intelligence focuses on two major aspects related to acquiring and applying knowledge to accomplish tasks and actions. The first goal of an intelligent agent is to get examples, evidence, rules, etc. from the world, memorize them, to use them later to act or make decisions. For example, kids learn simple mathematical rules to apply them to perform sums, multiplications, subtractions, and divisions. A classification algorithm learns from examples with labeled data how to classify unseen data points.

From the outside, an intelligent system is a black box accomplishing complex tasks – like driving a car, identifying fraudsters, and repurposing drugs. The first step to understanding it is dissecting it into simpler and smaller components based on their main functions. For this purpose, we decompose an intelligent agent into two major components accomplishing the most relevant activities:

Knowledge representation: It is the component dedicated to representing information about the world in a form that a computer system or human can utilize to solve complex tasks. It's about how we structure and encode information. As input to reasoning engines, it provides the foundational information that the system will use to reason or make

predictions. It can include facts, rules, relationships, ontologies, semantic networks, frames, or properly organized structured data. The quality and structure of this input greatly influence the effectiveness of reasoning. As output of reasoning engines, it represents the system's conclusions, inferences, or forecasts. May include new facts, probabilities, decision trees, causal networks, or other structured representations of derived knowledge. In essence, knowledge representation serves as the language through which we communicate with AI systems, both in providing them with the information they need to reason or predict, and in interpreting the results they produce. The choice of how to represent knowledge can significantly impact the capabilities and limitations of the AI system. Reasoning: Reasoning is the cognitive process of analyzing information, applying rules, and drawing conclusions based on evidence or premises. It encompasses various types such as deductive, inductive, and others, each serving different purposes in problem-solving and decision- making. The reasoning goes beyond simple pattern recognition, involving the understanding of relationships and the ability to deduce new information. It's crucial in fields ranging from logic and mathematics to everyday decision-making and complex query comprehension. While humans naturally employ reasoning, artificial intelligence systems, particularly those combining Knowledge Graphs and Large Language Models, are increasingly adept at mimicking and even enhancing this fundamental cognitive skill.

Starting from the definitions and the considerations above, Figure 2.1 depicts a high-level architecture of a generic intelligent system, revealing how the two components interact with each other and with the outside world. The resulting schema is the mental model for the entire chapter.

Figure 2.1 A high-level schema of an intelligent system. It is made of 2 component types: knowledge bases and reasoning engines. Each type can have multiple instances based on the tasks that the Intelligent system has to serve.

Figure 2.1 highlights how multiple instances of the two components can coexist in a single intelligent agent to serve the purposes it has been designed. They can interact in several ways, in sequence and parallel and the same knowledge representation can be used for multiple engines or as a communication pattern among different models. The output of one process can become the input for another, creating a cycle of knowledge refinement.

There's often a trade-off between how expressive a knowledge representation is and how efficiently it can be processed. Moreover, the most effective knowledge representation often depends on the specific domain and task. Knowledge representation and reasoning are interconnected, influencing each other.

One of the central arguments of this book is that KGs serve as a fundamental data structure for efficiently representing knowledge, deriving fresh insights, and laying the groundwork for precise and effective reasoning. When used in combination with LLMs it is possible to improve how data sources can be converted into knowledge – knowledge acquisition discussed later –, or how it is possible to interpret tasks or questions and provide answers to the users. Internally, LLMs use a different type of knowledge representation that we will discuss here to understand how different models coexist in the realization of complex agents and how these models can be adjusted (fine-tuned) to serve specific purposes.

In this chapter, we will design a basic intelligent system that is capable of

Gathering and effectively representing knowledge; Autonomously reasoning that knowledge; Answering simple and complex questions and supporting informed decisions.

We highlight the role of KGs and LLMs in the development of intelligent systems, describing how they concretely simplify and improve the entire process. While many concepts discussed here apply broadly to generative AI, including those generating videos, images, and songs, our emphasis will be on LLMs, the primary technology utilized in this book.

2.2 Designing an intelligent system

To define what is an intelligent system, we’ll implement a high-level architecture and identify the most critical aspects of its development. To do that, let’s consider a concrete task. Suppose that you have been tasked to create an autonomous medical diagnostic system. When a patient with an illness visits the doctor, the autonomous medical diagnostic system should support the doctor in choosing a sequence of actions (e.g., queries, medical tests, attempted therapies) to diagnose the disease and heal the patient.

This is an extraordinarily complex task because it involves large general and specific knowledge bases as well as sophisticated reasoning, but no worry, we are not going to solve the problem so early in the book. We use this scenario to help you understand which kind of system we are going to build, how it can be structured, and the different types of approaches.

To illustrate the limitations of using generic LLMs alone, consider asking ChatGPT to diagnose a patient reporting headaches, memory issues, and speech difficulties. While ChatGPT provides reasonable general advice— recommending immediate medical attention and symptom tracking—it cannot leverage specific patient history, current medications, or contextual factors that would enable more targeted recommendations. This demonstrates why publicly available LLMs cannot be used out-of-the-box for our autonomous medical advisor system.

Such a system is one of the most common examples of the type of intelligent systems we would like to design. A system that combines global generic knowledge with contextualized information and provides more accurate and specific answers. Such a system would work also for other types of tasks like supporting intelligence analysts in solving crimes faster, helping banks to support their customers or fighting frauds, and many more.

We will cover many different algorithms and approaches to distill knowledge from different sources and use it to accomplish complex tasks. Still, none of them can provide sufficient intelligent behavior when used in isolation. To unlock the power of machine learning (ML) in the scenarios considered, we need to build a system of algorithms and tools where, in most cases, the

output of one algorithm is used as the input of another, or the output of two algorithms is combined to improve the reasoning performance. The book presents how to combine KGs and machine learning techniques, including LLMs, to implement such intelligent systems. Compared with those without, we will see how approaches including KGs allow practitioners to implement more efficient, explainable, and effective systems.

2.2.1 What is an intelligent system?

Let’s begin by clearly defining what an intelligent system is and identifying its key characteristics. Among the various existing definitions, we prefer the one offered by Geoff Hulten [1]:

Definition

Intelligent systems connect users to artificial intelligence (AI) and machine learning (ML) to achieve meaningful objectives. An intelligent system is one in which intelligence evolves and improves over time, particularly when it improves by watching how users interact with the system.

This definition emphasizes several crucial aspects that align with our vision of intelligent systems and guides the approaches we take in this book. First and foremost is the pivotal role of the user. The primary objective of the intelligent system is to support users in accomplishing complex tasks – not by replacing them, but by enhancing their decision-making capabilities.

For example, an autonomous medical diagnosis system is designed to assist physicians in making informed decisions rather than replacing their expertise. Similarly, a financial recommendation system might help investors by analyzing market trends and suggesting potential investment opportunities, but the final decision remains with the user. In customer service, an intelligent chatbot might support agents by providing relevant information or suggesting responses, enabling faster and more accurate service while leaving the human agent in control.

This contrasts with other types of intelligent systems, such as self-driving cars, where the aim is to make the machine function independently of the

user (the driver, who becomes merely a passenger). Another example is an autonomous drone for delivery services, which is designed to operate with minimal human intervention, focusing on efficiency and automation rather than user support.

Secondly, an intelligent system must have the ability to learn from user interactions and explicit feedback and leverage contextual information (such as a patient’s medical history and condition in the case of a medical system). The system should continuously develop, utilize, and maintain a knowledge base that evolves. This evolution is driven not only by new or updated data sources but also by ongoing interactions with users. For instance, a personalized learning platform could adapt its content based on how students engage with the material, thus supporting more effective learning outcomes.

Figure 2.2 provides a high-level overview of an intelligent system, depicted as a black box for now, illustrating the primary source of knowledge and how it interacts with users and other systems.

Figure 2.2 A high-level overview of an intelligent system and its interactions with other elements. An intelligent system acquires knowledge by observing the environment or by ingesting data from existing sources. The internal processes elaborate the knowledge and provide results to the end user. These results can be either suggestions or concrete actions.

2.2.2 Categories of intelligent systems

The distinction between supporting users and acting on behalf of users defines two main categories of intelligent systems. Martin Molina presents a comprehensive taxonomy that divides these systems into intelligent autonomous systems and intelligent advisor systems (IASs) [2].

In an intelligent autonomous system, the machine performs tasks independently, effectively replacing the user in decision-making and execution. A self-driving car is a prime example of this category, where the vehicle operates without human intervention, handling all aspects of driving. Key features of autonomous systems include:

Full automation: The system operates without human input (or minimal input, like the destination), making all decisions based on its programming and sensory data. Real-time decision-making: The system must analyze data and make decisions in real-time, such as navigating traffic or avoiding obstacles. Adaptability: Autonomous systems often need to adapt to changing environments and unexpected situations, like weather conditions or road closures.

On the other hand, an intelligent advisor system assists the user in making informed decisions and acting. In this case, the user retains control and makes the final decisions, while the system’s role is to provide valuable information and recommendations. This information may be generated using descriptive or predictive analytical methods, such as pattern recognition, diagnostic tools, or temporal projections. Additionally, the system can employ prescriptive methods, like resource assignment, scheduling, or planning, to suggest the best course of action. Key features of advisor systems include:

Decision support: The system provides insights, suggestions, and recommendations to help users make informed choices, but it does not execute actions on its own. Context awareness: The system leverages contextual information, such as user preferences or specific scenarios, to tailor its advice to the

situation at hand. User interaction: These systems are designed for easy interaction with users, allowing them to explore options, ask questions, and receive detailed explanations to aid in their decision-making process.

Figure 2.3 illustrates the high-level differences between these two types of intelligent systems.

Figure 2.3 Differences between a) an intelligent autonomous system and b) an intelligent advisor system. The first acts on behalf of the users, while the second only makes suggestions. In both cases, the intelligent system supports or helps the end user, it is not aimed to replace it.

In this book, we focus primarily on intelligent advisor systems, as these are the systems that can most effectively leverage the power of a KG. The three key features of advisor systems – decision support, context awareness, and user interaction – align closely with the strengths of a KG:

Enhanced Decision Support: A KG can integrate and organize vast

amounts of structured and unstructured data, providing a rich source of information that advisor systems can tap into to generate accurate and contextually relevant insights. Deep Context Awareness: By connecting diverse data points and establishing relationships between them, a KG enables advisor systems to better understand the specific context in which decisions are made. This contextual intelligence is crucial for providing personalized and situation-specific recommendations (think about law enforcement or the physician example). Interactive User Experience: KGs facilitate the dynamic exploration of data, allowing users to navigate through information intuitively. This enhances the interactive experience of advisor systems, enabling users to delve deeper into the data, ask more nuanced questions, and receive more detailed and meaningful responses.

Given these synergies, our focus on intelligent advisor systems allows us to fully exploit the capabilities of KGs, ultimately providing users with more powerful and effective decision-support tools. Moreover, the intelligent advisor systems are those types of systems that we helped our customers build in the last decade, so we are more familiar with them, and we can bring our direct experience to these pages. Examples of intelligent advisor systems can be found in various fields, including law enforcement and fraud detection:

In law enforcement, intelligent advisor systems are used to help officers and investigators make more informed decisions. For example, predictive policing systems analyze crime data to identify potential hotspots or forecast where crimes are likely to occur. These systems provide recommendations on where to allocate resources or patrols, but the final decision on how to act is left to the officers. In financial services, intelligent advisor systems assist in detecting and preventing fraud. These systems analyze transaction patterns, flagging suspicious activities that could indicate fraud. For example, a system might alert a bank when it detects unusual spending patterns on a customer's account. The system may suggest actions like freezing the account or contacting the customer, but the decision to take these actions is made by human analysts.

In biomedical scenarios, intelligent advisor systems have numerous applications. For example, as mentioned several times in this chapter, they can assist physicians in making informed decisions when diagnosing and treating diseases. These systems can provide a list of potential diagnoses and recommend specific treatment options based on available data. However, it's important to note that while the system offers valuable insights, the physician ultimately determines and is responsible for the final course of treatment.

And many more that we will be mentioned in the book.

2.2.3 Characteristics of an intelligent system

An intelligent system must include four key characteristics that should drive both design and implementation:

A meaningful objective—The system should exist for a specific, achievable purpose that is meaningful to end users. This objective must drive the entire development process—guiding data model definition, data source selection, and algorithm identification—rather than starting from available data and hoping to discover meaning. The intelligent experience—The system must present intelligent outputs to users in ways that achieve desired outcomes. This requires interfaces that adapt based on predictions, maximize the value when intelligence is correct, and minimize costs when mistakes occur. The interface must also facilitate both implicit and explicit user feedback to improve the system over time. Knowledge creation and update—Intelligent behavior requires the capability to build, maintain, and reason with knowledge continuously. This represents a crucial differentiation between LLMs and KGs: while LLMs require enormous effort to build initial models and costly update processes, KGs can be constantly updated with contextual and specific information. Combining them enables proper handling of evolving knowledge and user feedback. Orchestration—Intelligent systems involve multiple algorithms and tools working together, where the output of one becomes input for another. This includes managing how the system acquires knowledge

from sources, controls risk, and maintains quality throughout its lifecycle.

These characteristics emphasize that a meaningful objective should drive the process, rather than letting available data or algorithmic preferences determine the direction.

Continuing our design process, we need to consider key aspects that drive our architectural decisions:

Focus on autonomous advisor systems—Our intelligent systems should suggest actions rather than accomplish actions on behalf of users. Use an established knowledge base—This includes research papers, existing ontologies, and structured data sources rather than generic knowledge. The system shouldn't provide generic answers but should leverage domain-specific understanding refined by experts. Learn from experience—Intelligent systems should extend their knowledge base using feedback from the results of suggested actions.

A key element emerges from this list—the concept of knowledge. This represents how we define what the system knows about the target domain, and it's a major differentiator between KGs and LLMs. The two technologies use different approaches to consume knowledge, build their internal models, and reason with that knowledge.

We now have all the elements to design our intelligent system with a clear understanding of the key components and processes, as summarized in Figure 2.4.

Figure 2.4 Extended model of an intelligent system. The focus is on the core components (the knowledge base and the reasoning engine) and the core processes (knowledge acquisition and reasoning). This is how they will be presented in the following sections.

The next two sections focus on the key processes of an intelligent system:

Knowledge acquisition—Collecting information from data sources, environment, or domain experts Reasoning—Converting acquired knowledge into actionable expertise

Through these processes, we'll explain how intelligent system elements interact and compare how KGs and LLMs store, process, and utilize data differently.

2.3 Knowledge acquisition and representation

Knowledge acquisition and representation are critical processes for the intelligent system we are targeting. Knowledge acquisition converts the data we have (or, better, the relevant part of it) into knowledge that an intelligent agent can use for reasoning. The available data sources represent the starting point of the construction of the knowledge base. These can be internal, such as multiple data silos in the organization, or external, such as different open data and, of course, a mix of them. In our mental model, knowledge acquisition is the process that enables the Intelligent Advisor System to "learn" from existing data available within the domain where the system will be deployed, from user feedback, and the environment. This process results in converting raw data into structured knowledge representations, tailored to the specific system's requirements, and ultimately used during the reasoning phase. The result of the process is stored in one or more knowledge bases. How knowledge is acquired, represented and stored depends heavily on the underlying reasoning mechanisms, which is where LLMs and KGs diverge significantly. Figure 2.5 shows which components of our mental model are covered in this section.

Figure 2.5 We are focusing in this section on the knowledge acquisition part of our mental model and how it is represented in the knowledge bases. It allows the intelligent system to acquire data and convert it in knowledge.

For KGs, the knowledge acquisition process typically involves transforming

raw, structured, or semi-structured data into a graph-based format, where entities are represented as nodes, and their relationships are captured as edges. This explicit structuring of information plays a crucial role in how the data is organized and queried. The structure and semantics of the domain— defined by ontologies or schemas—are essential in guiding this transformation. One key challenge in KG development is the significant effort required for data preparation and mapping. This process often requires also domain expert to be involved, to better understand the semantics of the data and the intrinsic relationships among entities. Converting heterogeneous data sources into a unified, explicit schema demands meticulous work, ensuring that the resulting KG accurately reflects the domain it represents. However, this rigorous preparation pays off in terms of flexibility. KGs can ingest a wide variety of data types thanks to advanced technologies like machine learning tools, which can process unstructured or semi-structured data. For example, image processing algorithms can extract meaningful information from visual content, such as identifying people or objects in a video, while tools that extract fingerprints from facial recognition can be used to enrich data within the graph. Furthermore, the integration of LLMs, even with customized models, adds another layer of power. LLMs can help convert unstructured text into structured knowledge, automatically extracting named entities and relationships, and mapping them into the graph.

This versatility in handling different data types makes KGs particularly robust for building rich, interconnected knowledge representations. We will explore this process of KG construction in detail in the second and third part of the book.

In contrast, the knowledge acquisition process for LLMs is fundamentally different. LLMs acquire knowledge by ingesting vast amounts of unstructured text data, which is then encoded into dense, high-dimensional vector spaces during the training process. These models are capable of learning complex language patterns, associations, and relationships between words and phrases, offering a highly flexible understanding of language. Crucially, unlike KGs, LLMs do not require extensive data preparation—raw text alone suffices for training, and they don’t require extensive use of domain experts since the process is mainly unsupervised. The main effort in this case consists in properly selecting the data sources and cleaning them,

avoiding duplicates for example.

A fundamental difference with KGs is that LLMs represent knowledge through implicit knowledge representation—encoding information as statistical patterns across billions of parameters rather than explicit relationships defined by schemas. This implicit approach allows LLMs to capture nuanced language understanding and contextual associations that emerge from statistical correlations in the training data but makes the knowledge opaque and difficult to inspect or modify directly.

Figures 2.6 and 2.7 compare how the acquisition process works for the KGs and for the LLMs, respectively. These figures help us understand the clear differences between the two regarding the complexity of the resulting model and the role of the domain experts.

Figure 2.6 Knowledge acquisition for KGs: converting available data into explicit knowledge representation through structured entities, relationships, and properties. This process involves domain expert to identify relevant data sources, supporting the data model and evaluate the results.

Figure 2.7 Knowledge acquisition for LLMs: converting text data into implicit knowledge representation through statistical parameters. The domain experts are involved during the data source selection and the results evaluation.

Given these distinct approaches to knowledge acquisition and representation, a few key differences emerge that must be considered when designing systems that leverage both technologies:

Access: In LLMs, knowledge is stored through implicit representation using billions of parameters in continuous vector spaces, making it opaque and inaccessible to humans. Conversely, KGs use explicit knowledge representation through nodes, relationships, and properties that are directly interpretable by both humans and machines. However, KGs can also be converted into vector representations through manual graph feature extraction or Graph Neural Networks (GNNs), enabling hybrid approaches that combine the interpretability of explicit representation with the computational advantages of vector-based methods—techniques we'll explore in detail in Part 4 of this book. Updates: Updating a KG involves adding, removing, or modifying nodes and relationships, and can be done incrementally and continuously, even for minimal changes. In contrast, updating an LLM is far more complex, requiring retraining or fine-tuning the model, which can be time-consuming, computationally expensive, and often demands a significant amount of new data. Capabilities: LLMs are inherently adept at understanding and generating human language, often requiring no human input to interpret nuanced or ambiguous language constructs. KGs, on the other hand, depend heavily on how developers design their access patterns and domain schemas. This makes KGs more reliant on the quality of their initial setup, but they excel in areas requiring precision and formal reasoning.

While there are clear advantages and limitations to each method of knowledge acquisition and representation, these differences are also complementary. As discussed in the first chapter, the strengths of KGs and LLMs can offset each other's weaknesses. This is why, in this book, we aim to show how these two approaches can be effectively combined to create more powerful intelligent systems capable of leveraging both structured, explicit knowledge (KGs) and flexible, implicit knowledge (LLMs) to improve reasoning and decision-making capabilities.

A hybrid approach to the development of intelligent systems can overcome the limitations of both and empower each other by offering solutions for a wider set of tasks. This new paradigm embraces a broader spectrum of computational tasks and leverages diverse forms of knowledge representations, from structured data models to numerical parameters. In this era, reasoning is no longer confined to formal inference mechanisms but also includes probabilistic, contextual, and pattern-based computations that LLMs excel at. This shift allows AI systems to reason with explicit, structured knowledge (as in traditional expert systems) and, at the same time, unstructured, ambiguous, and contextual knowledge derived from language and experience.

In summary, a hybrid intelligent system provides flexible and comprehensive reasoning capabilities:

Explicit Knowledge Representation (KGs) provides structured reasoning based on symbolic logic, graph traversal, and inference, while also enabling conversion to vector representations through manual feature extraction or automated GNN techniques. Implicit Knowledge Representation (LLMs) captures language patterns and relationships in continuous parameter spaces, providing language understanding and generation.

In a KG, elements from the domain of interest are explicitly represented for computational and exploration purposes. These elements include:

Entities – Represent the key concepts around which knowledge is built. Entities can be physical (such as a compound, disease, patient, or location) or abstract (such as ideas or sentiments). Entities have attributes such as a person's name or a city's geographical location. Relationships – Define how entities are connected. For instance, a city is in a region, a gene encodes a protein, a disease has symptoms. Relationships can have attributes such as date, description, or probability. Ontologies – Define the types of entities and relationships that exist in a specific domain and their attributes. For example, you can define hierarchical representations, establish attribute cardinality (an organization can have multiple founders but one ethics policy), or define

relationship properties like transitivity.

These elements are stored naturally in KGs, with entities and relationships as key components and ontologies often represented as graphs describing concept connections and constraints.

In LLMs, knowledge is implicitly represented through millions of parameters —numerical weights that adjust how different inputs influence the model's predictions. During training, these parameters learn to capture word associations and contextual relationships across multiple neural network layers, each refining understanding of syntax, semantics, and context. This allows LLMs to generate contextually rich responses and handle tasks like translation without explicitly coded rules, relying instead on learned statistical patterns.

The bridge between these approaches lies in representation learning techniques that can extract vector embeddings from graph structures, enabling KGs to leverage the computational advantages of continuous representations while maintaining their interpretability. Vice versa, LLMs embedding capabilities applied to nodes and relationships textual properties can be used as input of GNNs algorithms that require such representation. Representation learning of graphs and how it leverages LLMs will be covered in the Part 4.

2.4 About reasoning

In an Intelligent Advisor System, the reasoning engine delivers insights and suggestions. The user provides input by formulating the request containing the desired goals and the further information required, like details about the patient, some filter conditions to reduce the solution space search, etc. Figure 2.8 shows the reasoning component in the mental model.

Figure 2.8 Reasoning uses the knowledge base(s) to accomplish the tasks the intelligent system is supposed to deliver to the end users.

Earlier, in the knowledge acquisition and representation section, we discussed how to gather, collect, and organize existing information distributed across multiple data sources and how to refine it using domain experts. At this stage, there are still some open questions that are important to consider:

How do I deal with uncertainty? Not all the information I have is true, accurate, or unequivocal. Some are valid with a certain probability or in a specific context, and some we simply don’t know. Reasoning accuracy directly depends on the certainty of the initial statements. In the autonomous doctor example, this is particularly true because we don’t know all the possible outcomes of treatment as each patient reacts differently. How can I infer some of the knowledge I need? Under some circumstances, even when we don’t have a complete knowledge base, it is possible to derive new information from the available. Social networks provide a good example: if two people have many friends in common, there is a high probability that they will be friends, if not already. Drug repurposing is another example and is one that’s related to our use case. It aims at using existing compounds to treat new or existing diseases. (This would have been useful during the COVID-19 pandemic, for example.) How can we abstract from what we have seen to a broader understanding of the domain? In the doctor scenario, this means, for instance, using a limited amount of evidence to infer generic guidelines. This allows us to treat similar symptoms.

As in the case of reasoning, we’ll describe the learning process with an example taken from Alessandro Negro’s book, Graph-Powered Machine Learning [3]. Consider the implementation of a spam filter for emails. A pure programming solution is to write a program to memorize all the emails labeled as spam by a human user and store the result in a simple knowledge base. When a new email arrives, the pseudo-agent searches for a similar match in the previous spam emails in our knowledge base, and, if it finds a match, the new email is rerouted to the spam folder. Otherwise, the email passes through the filter untouched.

This approach could work and, in some scenarios, can be useful. Yet, it is not a proper learning process because it lacks an important aspect of learning: the ability to generalize, transforming the individual examples into a broader model. In this specific use case, this means the ability to label unseen emails even though they are not the same as the previously labeled emails. This process is also referred to as inductive reasoning or inductive inference.[2]

For example, the algorithm in our example could scan the training data consisting of emails with some marked as spam and then extract a set of words whose appearance in an email message is indicative of spam (because they are more frequent in spam than in legitimate emails). Then, for a new email, the agent would check whether one or more suspicious words appear and predict its label (spam or legitimate) accordingly.

Now that we introduced a new type of reasoning, it’s worth spending more time comparing logical reasoning (also defined as deductive inference) and inductive reasoning (or inductive inference, which is typical of ML). Let’s look at deductive reasoning first.

Deductive reasoning is a basic form of reasoning. It starts with a general statement, or hypothesis, and examines the possibilities to reach a specific, logical conclusion. For example, consider the reasoning: "All men are mortal. Alessandro is a man. Therefore, Alessandro is mortal." With deductive reasoning, the hypothesis must be correct. It is assumed that the premises, "All men are mortal" and "Alessandro is a man" are true. Therefore, the conclusion is logical and true. To summarize, deductive reasoning generally moves from generic statements to specific statements.

Inductive reasoning is the opposite of deductive reasoning. Inductive reasoning makes broad generalizations from specific observations. Starting from the data in which there are samples of the reality (considering the previous example, a few emails where some of which are classified as spam), then conclusions are drawn from the data. Another example of inductive logic is this statement: "The coin I pulled from the bag is a penny. The second and the third coins from the bag are pennies. Therefore, all the coins in the bag are pennies." Even if all the premises are true in the statement, inductive reasoning can lead to some false conclusions. Here's an example: "Harold is a grandfather. Harold is bald. Therefore, all grandfathers are bald."

This is a straightforward example of overfitting.[3] The conclusion does not follow logically from the statements. To summarize, inductive reasoning moves from specific observations to generic statements.

In this book we will use many algorithms, tools, and techniques that will allow us to build, query, and analyze KGs with the specific scope of building different types of intelligent advisor system capable of supporting users in decision-making and analysis processes. To use a tool properly, you need to know what it has been designed for by knowing its strengths and weaknesses. For example, since ChatGPT and similar tools can mimic humans in conversation, people often think that they possess extensive reasoning capabilities that go beyond the probabilistic reasoning space. Let’s do a quick example. We used claude.ai[4], which offers one of the best “reasoning” tools powered by LLMs, to verify our assumptions. We used the following prompt:

Listing 2.1 Prompt for checking reasoning capabilities

A farmer stands at the side of the river with a sheep. There is a

This is a simpler variation of the problem of the farmer where apart from the farmer and the sheep there are also lettuce and a wolf. We selected this specific example because it is a very famous problem, and we are sure that the training data used to train the LLM contained many of these examples. So, we are expecting that the probabilistic reasoning engine will tend to go in the direction of the full problem instead of “understanding” what we asked for. The answer confirmed our assumption. Figure 2.9 is a snapshot of the results from Claude.ai.

Figure 2.9 Result from Claude.ai prompting it with the content of the Listing 2.1.

The answer from Claude.ai in Figure 2.9 contains some reasoning issues: there is no sense in bringing back and forth the farmer. The problem was solved completely after step 1, but since the problem formulation is very similar (but not the same) to another easily available in the data sources used during the training, “probabilistically” the proposed solution is the closest.

The example above shows how LLMs have some limitations for specific types of reasoning. Wu et al. in their article [4] tested a suite of 11 different tasks spanning from coding to drawing, from Logic to spatial, and from chess to arithmetic. They observe nontrivial performance on the counterfactual variants – like the one about the farmer and the sheep – but find that performance substantially and consistently degrades compared to the default conditions. This suggests that while current LLMs may possess abstract task- solving skills to an extent, they often also rely on narrow, non-transferable procedures for task-solving. These results motivate a more careful interpretation of language model performance that teases apart these aspects of behavior. In any case this is changing every day. We have seen significant progresses in the last year, so some of these limitations can be already solved while you are reading this book.

This discussion leads to the conclusion that KGs and LLMs could be the foundation for different types of reasoning, complementing each other in intelligent systems. KGs are particularly effective for deductive reasoning and other type of structured reasoning due to their explicit representation of entities and relationships. They excel in tasks requiring precise logical inference, such as determining hierarchical relationships or following explicit rules. In contrast, LLMs shine in inductive leveraging their ability to recognize patterns and draw connections across vast amounts of unstructured data. They excel in generating plausible explanations for observations by drawing on their broad knowledge base. By combining these approaches, intelligent systems can leverage the strengths of both KGs and LLMs: using KGs for tasks requiring precise, rule-based reasoning and explicit knowledge representation, and LLMs for tasks involving pattern recognition, context understanding, and handling ambiguity or incomplete information. Moreover, LLMs can effectively reason on graph structures and their derived metrics, enhancing their analytical capabilities. When provided with graph-based

information, such as community structures, node statistics (e.g., degree, in/out degree), or inferred metrics like PageRank or Betweenness centrality, LLMs can interpret and contextualize this data within their broader knowledge base. This ability to reason on graph-derived information allows LLMs to provide more nuanced and insightful analyses, combining structural understanding with contextual knowledge to generate comprehensive interpretations and recommendations.

On the other way around, both KGs and LLMs face distinct limitations in their reasoning capabilities. Some of which we have seen in the examples before. KGs, while excellent for explicit and rule-based reasoning, struggle with handling ambiguity and contextual nuances. They are limited by the completeness and accuracy of their underlying data and ontologies, potentially leading to incorrect conclusions when faced with incomplete or inconsistent information. KGs also face challenges in scaling to very large and complex domains, as maintaining consistency and updating the graph structure can become computationally- and human-intensive. On the other hand, LLMs, despite their flexibility and broad knowledge base, often lack transparency in their reasoning process, making it difficult to trace or explain their conclusions. They can produce confident-sounding but incorrect or "hallucinated" information, especially when dealing with specific facts or numerical data outside their training scope. LLMs also struggle with tasks requiring precise logical reasoning or mathematical calculations, as their statistical nature can lead to inconsistencies in such structured problem- solving.

Both KGs and LLMs can perpetuate biases present in their training data or knowledge base, potentially leading to skewed or unfair reasoning outcomes. Additionally, neither approach inherently possesses common sense reasoning capabilities comparable to humans, often failing to make intuitive leaps or understand the implicit context that would be obvious to a person. These limitations underscore the importance of carefully considering the strengths and weaknesses of each approach when designing intelligent systems and potentially combining them to mitigate their shortcomings.

The considerations in this chapter reinforce, once more, the concept that developing a powerful intelligent (advisor) system combining KGs and

LLMs in a hybrid system provides the grounding architecture to provide a bigger variety of task capabilities with higher accuracy.

2.5 Reasoning engines

Having defined knowledge base and different types of reasoning, let’s now extend our framework to explore how these elements interact in the development of an intelligent system. Figure 2.10 illustrates where we are in the mental model.

Figure 2.10 Multiple reasoning engines, each with different types of reasoning strategy, can contribute to delivering the tasks required to the intelligent system.

The reasoning engine, as depicted in Figure 2.10, is generic. It can employ a single type of reasoning—deductive, inductive, or otherwise—or a combination of reasoning strategies. Importantly, this engine doesn’t just read from the knowledge base; it also writes back to it. The actions (or suggestions) generated by the engine influence the environment, which in turn produces new observations. These observations are processed by the reasoning engine to build new knowledge, driving subsequent actions or suggestions. This feedback loop creates an iterative process, where the system continuously improves its ability to respond to environmental changes.

2.5.1 Limitation of a pure deductive reasoning engine

Let’s focus on the deductive reasoning process and how it can be applied in the context of automated medical diagnosis, as an example. Imagine a patient visiting a virtual doctor—our intelligent system—seeking a diagnosis. The system must propose a series of actions (e.g., medical tests, treatments, queries) to diagnose the illness and suggest a course of therapy.

This sequence of actions is computed using the knowledge base, which contains information about the costs and outcomes of potential actions, probabilistic relationships between diseases and symptoms, and the patient’s preferences (e.g., quality of life vs. longevity). The deductive reasoner can logically infer optimal actions when the knowledge base encodes all necessary data. In this idealized scenario, the deductive reasoner can outperform other reasoning methods.

However, a major limitation of deductive reasoning is that it requires a highly complete and accurate knowledge base, which is rarely available. Figure 2.11 visualizes this deductive reasoning process, where the knowledge base is constructed by transforming data sources into logical statements that guide decision-making.

Figure 2.11 The deductive reasoner. The knowledge base is created by transforming the sources. The deductive reasoner uses logical statements applied to segments of the knowledge base to act or to provide suggestions.

2.5.2 Leveraging inductive reasoning and Machine Learning

Inductive reasoning, powered by machine learning (ML), addresses some of the limitations of purely deductive reasoning. Inductive reasoning can enhance the system in two key ways:

By learning and building relevant ontologies and relationships, ML helps expand the knowledge base, enabling it to handle a broader range of cases. By providing inference under uncertainty, ML allows the system to generalize from incomplete data and make predictions even when not all information is available.

In Figure 2.12, we see how an inductive reasoner works. The first step involves transforming raw data into a structured format, often with the help of ML algorithms. For example, Natural Language Processing (NLP) converts unstructured text into structured data that can be incorporated into the knowledge base. This step can contribute to the creation or extension of a KG. The second step uses this knowledge to make predictions or generate actions through inductive reasoning, which abstracts patterns from available observations.

Figure 2.12 An inductive reasoner. The knowledge base construction requires more effort; it is not a simple transformation. This reasoning engine is capable of abstracting from the knowledge base and, hence, works under some level of uncertainty.

In traditional ML approaches, this process often requires manually selecting

features from the knowledge base to train prediction models. This can be a tedious and sometimes infeasible task, especially in complex domains like image or audio processing, where feature extraction is non-trivial.

2.5.3 The role of LLMs in the reasoning engine

LLMs offer a powerful complement to traditional reasoning engines, particularly when dealing with incomplete or uncertain information that characterizes real-world scenarios. Unlike purely deductive systems that require complete knowledge bases and explicit rules, LLMs can leverage their probabilistic reasoning capabilities to generate contextually relevant suggestions even when critical information is missing.

Consider a medical scenario where a patient presents with non-specific symptoms like persistent fatigue, occasional headaches, and difficulty concentrating — symptoms that could indicate anything from stress to serious neurological conditions. Traditional deductive reasoning would struggle with such ambiguous presentations, especially when patient records are incomplete. An LLM, however, can draw on patterns learned from vast medical literature to evaluate multiple diagnostic possibilities simultaneously.

The LLM's strength lies in its ability to reason probabilistically with uncertainty. It can weigh the likelihood of different conditions based on available data, suggesting that while stress-related disorders might be statistically most probable, the symptom combination warrants ruling out conditions like sleep disorders, thyroid dysfunction, or early-stage neurological issues. Rather than providing a single definitive answer, the LLM can recommend a prioritized diagnostic approach—starting with less invasive tests while flagging when more serious investigations might be needed.

This probabilistic reasoning capability allows LLMs to bridge gaps in knowledge that would otherwise halt a purely deductive reasoning process. When integrated with KG-based reasoning engines, LLMs can provide the contextual understanding and uncertainty management that complement the structured, rule-based inference capabilities of graph-based systems. The LLM essentially acts as a reasoning layer that can interpret ambiguous inputs,

fill knowledge gaps, and provide nuanced recommendations that account for the inherent uncertainty in complex decision-making scenarios.

This hybrid approach—combining the explicit, traceable reasoning of KGs with the probabilistic, context-aware capabilities of LLMs—enables intelligent advisor systems to function effectively in real-world environments where information is often incomplete, ambiguous, or uncertain.

2.6 A knowledge graphs approach to IASs

Where do KGs fit in the development of Intelligent Autonomous Systems (IASs)? The short answer is everywhere. In recent years, academia and industry have used KGs extensively as a form of structured human knowledge [5][6][7][8]. On top of this graph-based representation, several reasoning and analysis algorithms were devised to derive insights from KGs. In the first chapter, we defined a KG extensively. Here is the final definition:

Definition

A knowledge graph is an ever-evolving graph data structure, composed of a set of typed entities, their attributes, and meaningful named relationships. They are built for a specific domain from both structured and/or unstructured data to craft knowledge for humans and machines alike.

This definition clarifies that a KG is a perfect fit for knowledge representation and, as we will discover later in the book, for reasoning, several ML tasks, and supporting the development of intelligent systems with LLMs. The idea of using a graph to support decision-making processes is not new. Stokman and de Vries [9] anticipated that with knowledge-based systems, it is possible to construct computer programs that advise professional users with a limited domain of expertise. In this context, they speculated that

“The structuring of knowledge in a graph can be seen as the construction of a knowledge-based system integrating knowledge from different sources [9].”

Hence, they coined the concept of a KG without mentioning it. It took more than 25 years to have this simple concept broadly accepted and embraced by industrial applications. In more recent years, KGs have been globally recognized as a standard approach to merge different distributed data sources (often just a separate data silo in the same organization) into a single connected source of truth. Such applications span from efficiently supporting data governance to metadata management and from data enrichment to data integration [10]. Moreover, with the advent of Generative AI, and, more specifically, LLMs, KGs are the perfect companion for overcoming many of the limitations of such platforms: KGs support other types of reasoning, help to solve hallucinations, and provide up-to-date data to mention the most relevant [11].

Although we appreciate this recognition of the central role of a KG as an aggregator of knowledge, we think that this point of view misses the focus of why KGs exist. This perspective loses touch with the final goal for which they have been introduced—building an intelligent system. We consider this as a bottom-up approach to KG construction. It starts from the data available in the different data sources, tries to ensemble it in a single source of truth, and then starts the discovery. It expects the system to tell a story without any idea of what the end user is looking for. Figure 2.13 summarizes this approach.

Figure 2.13 A bottom-up approach to KG creation. It starts by importing all the data instead of thinking first about the functional tasks we want to achieve. Not all data should be converted in a KG if not needed.

Based on our experience, this approach can easily drive KG adoption to failure. We have seen this in many different cases. The problems with this approach are that there are too many data sources, each with different structures and identifiers, requiring huge effort to normalize data into a single homogeneous structure. Much of the content is task-specific and therefore not relevant to global thinking.

The goal is to develop intelligent agents. Such a goal requires us to represent the knowledge (in our case, as a KG) in a way that is effective and capable of capturing and handling the intrinsic complexity of the domain in which the agent must operate. This approach should be driven by the business objectives rather than by available data.

Building on established machine learning project methodologies, CRISP-DM [12], we can use a purpose-driven approach for KG construction. Figure 2.14 illustrates how a KG serves as the central knowledge representation in this process.

Figure 2.14 CRISP-DM revisited to apply to KG platforms. The KG is used as a model of the knowledge base for the intelligent system. It is the center of the revisited CRISP-DM process.

This approach emphasizes that everything starts from the business

understanding, which allows us to set the initial goal and all subsequent ones. These goals drive the data understanding, allowing us to focus on the specific portion of the data we need rather than blindly exporting all available data sources. This determines requirements for the definition of the content and the structure of the KG. The results of the initial phases produce the identification of the portion of the data sources to be included in the analysis, the schema of the KG to be used, and a clear definition of the pipelines necessary to import the data from the data sources to the KG.

We consider the KG as a self-sufficient, domain-specific, and customizable source of truth that copies and transforms the data that we need. During the acquisition, LLMs play a crucial role in the extraction of relevant entities and relationships from unstructured data as well as more generic understanding like sentiment analysis or topic identification.

In the modeling phase, we use and test one or more algorithms to reach specific goals, and the results are evaluated in the next phase. In this phase, the LLMs can be involved for reasoning on top of KGs, for understanding users’ questions and providing answers in natural languages. The output of these two phases is composed by:

A set of selected algorithms A set of trained or pre-trained models A report about the tests and the overall quality of the trained models

If everything goes well, we incorporate the graph schema and model, the pipelines for ingestion and post-processing, the algorithms, and the predictive models[5] into a product and then deploy. At that point, a new round can start. In this case, we do not start from an empty KG.

For the second round, we work by difference and extension, making sure that the previous iteration results are not affected. In this sense, the schemaless[6] approach for the graph helps greatly because it allows for extensions with new node and relationship types, without compromising previous data and functionalities.

In the book, we use schemas quite often to drive the process between the different scenarios and use cases. These schemas are repurposed, and the

different phases are highlighted as an example of how this should work in practice.

2.7 Summary

Intelligence is fundamentally about acquiring and applying knowledge, making knowledge representation and reasoning the core components of intelligent system architecture. Intelligent systems are categorized as autonomous systems that act independently or advisor systems that support human decision-making. Knowledge acquisition differs between KGs and LLMs: KGs use explicit, structured representations requiring domain expertise but offering interpretability, while LLMs use implicit statistical patterns capturing language understanding but lacking transparency. Hybrid systems combining KGs and LLMs leverage complementary strengths—KGs provide structured reasoning and explicit knowledge while LLMs handle ambiguity, context, and natural language understanding. LLMs enhance reasoning engines by bridging knowledge gaps and providing contextual interpretation, making intelligent systems more robust with incomplete or ambiguous information. Purpose-driven KG development starting from business objectives proves more effective than bottom-up data integration strategies that often lead to project failure.

2.8 References

1. G. Hulten, Building Intelligent Systems: A Guide to Machine Learning

Engineering, 1st ed. New York, NY, USA: Apress, 2018 2. M. Molina. “What is an intelligent system?”, arXiv Labs, arXiv preprint

arXiv:2009.09083, Feb. 12, 2020. Accessed: May 5, 2022. [Online]. Available: https://arxiv.org/abs/2009.09083 3. A. Negro, Graph-Powered Machine Learning. Shelter Island, NY, USA:

Manning Publications, 2021 4. Z. Wu et al. “Reasoning or Reciting? Exploring the Capabilities and

Limitations of Language Models Through Counterfactual Tasks”

arXiv:2307.02477, 2023. Accessed: October 13, 2024 [Online]. Available: https://arxiv.org/abs/2307.02477 5. X. Dong et al., “Knowledge vault: A web-scale approach to probabilistic

knowledge fusion,” presented at SIGKDD. ACM, 2014, pp. 601–610 6. M. Nickel et al., “A review of relational machine learning for knowledge

graphs,” Proceedings of the IEEE, vol. 104, no. 1, 2016, pp. 11–33 7. Q. Wang et al., “Knowledge graph embedding: A survey of approaches

and applications,” IEEE TKDE, vol. 29, no. 12, 2017, pp. 2724–2743 8. A. Hogan et al., “Knowledge graphs,” arXiv Labs, arXiv preprint

arXiv:2003.02320, 2020. Accessed: May 5, 2022. [Online]. Available: https://arxiv.org/abs/2003.02320 9. F. N. Stokman and P. H. de Vries, “Structuring knowledge in a graph,”

in Human-Computer Interaction, Springer-Verlag. 1988, pp. 186–206 -> fixed 10. D. Sculley et al., “Hidden technical debt in Machine learning systems,”

Proceedings of the 28th Int. Conf. on Neural Information Processing Systems, vol. 2 (NIPS'15). Cambridge, MA, USA: MIT Press, 2015, pp. 2503–2511. 11. J. Pan et al., Large Language Models and Knowledge Graphs:

Opportunities and Challenges. Transactions on Graph Data and Knowledge, 1 (1): 2:1--2:38 (2023) Accessed: October 13, 2024 [Online]. Available: https://arxiv.org/abs/2308.06374 12. R. Wirth and J. Hipp. “CRISP-DM: Towards a Standard Process Model

for Data Mining.” Proceedings of the Fourth Int. Conf. on the Practical Application of Knowledge Discovery and Data Mining, 2000, pp. 29–39

[1]As you can see, we are a great fan of Charles Darwin and his theory of evolution in his book, On the Origin of Species.

[2]According to the Stanford Encyclopaedia of Philosophy website (https://plato.stanford.edu/entries/logic-inductive/ ), with inductive logic, the premises should provide support for the conclusion to some extent. By contrast, with deductive reasoning, the premises logically entail the conclusion. For this reason, induction is sometimes defined as the process of deriving general principles from a specific observation (although some people disagree with this definition).

[3]Overfitting refers to a model that represents the training data too well. This happens when the training dataset doesn’t contain enough examples to cover the spectrum of possible cases. This negatively impacts the model’s ability to generalize.

[4] https://claude.ai/ The version we tested is 3.5 Sonet. We are pretty sure, considering the speed of improvements that running the same experiment after a few months will generate a different result.

[5]The predictive model is a formula for estimating the unknown value of interest—the target. Such a formula could be mathematical, a query on a data structure (a database or a graph, for instance), a logical statement such as a rule, or any combination of these. It represents in an efficient format the result of the learning process on the training dataset, and we access it to perform the actual prediction.

[6]Schemaless refers to the flexibility of storing the data in a database or in a generic data structure with less constraints or no constraints at all (in terms of how the data items should be formatted and related to each other). Graph databases are generally considered schemaless because their constituent elements (nodes and relationships) and their attributes can store practically everything.

## Part_II:_Building_knowledge_graphs_from_structured_data_sources

The journey from disparate data sources to a unified, meaningful knowledge graph represents one of the most critical challenges organizations face when implementing knowledge graph solutions. This part addresses the complex but essential process of constructing knowledge graphs from structured data sources, a fundamental step before enriching them with unstructured information and combining them with Large Language Models.

Organizations across domains typically maintain vast repositories of data, each with its own schema, structure, and storage format. The challenge lies not just in importing this data, but in harmonizing it into a coherent knowledge graph while preserving its semantic meaning and relationships. This part guides readers through this intricate process, demonstrating how to transform diverse structured data sources into a unified knowledge representation that serves as the foundation for more advanced applications.

A key theme throughout this part is the critical importance of data quality and validation. Through various techniques and mechanisms, readers will learn how to verify data integrity, ensure accurate entity matching, and validate the semantic correctness of their knowledge graphs. This validation process is fundamental to the success of any knowledge graph implementation, as the quality of downstream applications—whether they involve reasoning, analysis, or integration with LLMs—depends entirely on the reliability of the underlying knowledge representation.

Chapter 3 illustrates this journey through a practical healthcare example, demonstrating how to construct a knowledge graph that helps clinicians diagnose rare diseases based on patient symptoms. It introduces fundamental concepts like semantic integration through ontologies, compares different knowledge graph technologies (RDF vs LPG), and provides hands-on implementation guidance using the Human Phenotype Ontology (HPO) as a foundation for clinical decision support.

Chapter 4 expands on these foundations by exploring the progression from simple networks to comprehensive multi-source integration across biomedical applications. It demonstrates advanced analysis methodologies including community detection algorithms, domain-specific metrics like Degree-Weighted Path Count (DWPC), and practical applications spanning multi-omic analysis, pharmaceutical research, and clinical use cases. The chapter also introduces the integration of Large Language Models for result interpretation and clinical decision support, bridging quantitative graph analysis with actionable biological insights.

Throughout this part, readers will encounter practical examples across different domains, with a particular focus on healthcare and biomedical applications (because of the complexity of the domain and the availability of open accessible structured data). These examples not only demonstrate the technical aspects of knowledge graph construction but also illustrate how these principles can be applied to solve real-world problems in any domain. The techniques and approaches presented here—from ontology-based reasoning to multi-source data integration and advanced network analysis— establish the robust foundation necessary for the more advanced applications and integrations discussed in subsequent parts of the book.

## 3_Create_your_first_Knowledge_Graph_from_ontologies

This chapter covers

Building your first knowledge graph starting with business goals Selecting the best technology for the knowledge graph representation based on use cases Constructing a valuable knowledge graph to support clinicians' activity Performing analysis and ontology-based reasoning on top of this knowledge graph

The KG construction is complex because it requires extracting and integrating information from various data sources. We can relate the differences among these sources to the data format (XML, CSV, or JSON), the storage technology (relational or document-oriented databases), and the information syntax (2022-08-09 or 9 August 2022). Another key difference involves the meaning of the data, which becomes critical for information exchange. In the case of healthcare, for instance, the adoption of diverse expressions to identify the same concept (type 2 diabetes or ketosis-resistant diabetes), the use of the same acronym to define distinct concepts (PE for physical examination or pulmonary embolism), and varying levels of information granularity (necrosis or lobular necrosis) present significant obstacles to data integration.

When constructing a KG, we want to achieve a unified, well-grounded, and meaningful representation of data from different sources, where the individual pieces of information are fused into a unique view. This process implies overcoming the differences in terms of format, technology, and syntax, but even more relevant, reconciling multiple expressions with the same meaning, disambiguating the terms characterized by various meanings, and aligning information with diverse granularity.

The issues related to the meaning of data can be addressed using a semantic

integration process. One of the common strategies to apply semantic integration is adopting one or more ontologies as a reference schema and vocabulary for incoming data. An ontology allows you to model your data using a standard vocabulary. This provides ingredients such as formal names, properties, categories, and relationships between entities described within the data.

Considering these features, we can incorporate the components of ontology into a KG: ontology acts as an intermediary between semantically heterogeneous information located in different data sources. Once the components of the ontology are incorporated, we need to create a bridge, which is sometimes defined as mapping, between the local schema of a data source and the reference schema represented by the ontology. One way to realize this mapping annotates each data element in the data source with concepts expressed by ontology. As a main consequence, such annotations allow us to bring together data elements characterized by different origins.

Building on these premises, this chapter seeks to provide a set of guidelines to build a KG using a reference ontology. It specifically addresses a challenge related to clinicians' activities in the healthcare sector and examines how to assist them in identifying rare diseases. This task can be daunting, particularly for clinicians who may have never encountered such conditions in their careers.

To reach this goal, the chapter highlights the importance of data understanding and preparation, focusing on the ingestion and processing of the Human Phenotype Ontology (HPO; see https://hpo.jax.org/app/) along with a dataset annotated using this ontology, which we will incorporate into the KG. The HPO source provides information on the connections between diseases and their associated phenotypic abnormalities. These abnormalities represent observable physical or biochemical characteristics that deviate from typical human traits and may result from genetic mutations, environmental influences, or a combination of both.

Moreover, this chapter explores the key differences among various KG technologies and offers a blueprint for selecting the most suitable option based on the specific problem being addressed. Finally, it outlines a set of analyses, including ontology-based reasoning, to support clinicians in

diagnosing rare diseases. These analyses leverage a multifaceted KG system that enables:

Access to a precise and well-maintained collection of terms. Check the co-occurrence of abnormal traits affecting specific diseases. Explore phenotypic abnormalities at different levels of granularity based on the hierarchical structure described by the ontology.

Figure 3.1 provides a mental model to illustrate the full process described in this chapter, from the business understanding to the KG creation and querying.

Figure 3.1 The mental model describing the KG construction process as a specification of the CRIPS-DM model, from understanding the business goal to defining the KG queries that support the clinicians' activities.

The central part of figure 3.1 shows the steps to create a KG in the specific context related to the clinician's activity. The bottom part shows the blocks of an abstract pipeline for constructing a KG that can be adopted in different scenarios. The components of this pipeline derive from a version of the CRIPS-DM model adapted to KG, which was introduced in chapter 2 (and it is presented again in figure 3.2).

Figure 3.2 The CRIPS-DM model adapted to KGs. A subset of the components of this diagram, including business understanding, data understanding, data preparation, and KG model creation/update, represent key phases described in this chapter.

3.1 Knowledge graph building: Warm-up

Before creating the KG, we’ll deal with business-driven and data understanding aspects. We’ll analyze the problem we want to solve, build an overview of the application domain, and scout the data that can be useful to provide an overview and to address the problem.

3.1.1 Business and domain understanding

The target persona of our KG is the clinician. This is a healthcare professional with direct contact and responsibility for the patient. The main activity of the clinician is to diagnose and treat diseases based on detected symptoms, or more technically, the phenotypic features manifested by the patient. In this context, one of the most complex activities is correctly identifying a disease based on phenotypic traits, particularly in the case of rare syndromes (see figure 3.3).

Figure 3.3 Understanding the business domain for creating a KG that supports the clinicians' activities. This phase is not strictly related to the technical aspects, but it is fundamental for the next steps.

In addition to prescribing specific tests to reach a diagnosis, the clinician can use a structured knowledge base of available information. Two main aspects must characterize this knowledge base:

A contextual description of the phenotype domain. For instance, phenotypic anomalies related to the same organs or systems should be explicitly connected in the knowledge structure. A collection of data describing the relationship between phenotypic

anomalies and diseases. Such information must be well-tracked to allow clinicians to explore the original source of the connections.

The problem we intend to solve requires shaping a KG that incorporates these two features, allowing direct use by the clinicians or an information system. Considering the flexibility of KGs, such available information can be potentially further extended. The patient's history can be fully maintained and integrated into this representation and, appropriately anonymized, such data can be helpful to support the activities of the researchers when identifying, for instance, novel target therapies.

To better understand the application domain, we need to clarify the distinction between phenotype, phenotypic features, and diseases, identifying edge cases that represent a grey zone in which differences are not well- defined. According to Kohler et al.[1], "The phenotype of an individual with a disease can be said to be the sum of all of the phenotypic features manifested by that individual." Moreover, a disease is an entity characterized by four main features:

A set of causes for a specific condition. A time course. A group of phenotypic features. A characteristic response to a particular treatment.

Following such a definition, we can state that a virus causes the common cold. It is characterized by distinct phenotypic features including fever and fatigue. For the common cold, the time course goes from a couple of days to a week with a relatively acute onset. Treatments such as aspirin can support the healing of the disease. Moreover, we should also consider that fatigue could be a phenotypic feature that characterizes other conditions.

Regarding the aforementioned grey zone, we can classify specific cases such as diabetes mellitus as a disease or as a phenotypic characteristic of other rare syndromes. Complex problems related to grey zones and the manifestation of phenotypic features related to different conditions represent an exciting challenge to support the clinician with a KG. It is worth noting here that this overlapping of concepts or grey areas is common in many different domains. The annotated Venn diagram in figure 3.4 illustrates this situation.

Figure 3.4 An annotated Venn diagram shows that Type 1 Diabetes Mellitus can be considered as either a disease or a phenotypic feature. Based on the context, two different IDs can be adopted.

These grey zones, this uncertainty, is where we will focus our attention. This provides valuable support to technicians when addressing the specific use case that we will handle in this chapter.

3.1.2 Data understanding

To support the clinician in this complex task, we identify a first data source called Human Phenotype Ontology (HPO). This source provides two sets of information for our example (figure 3.5).

Figure 3.5 Understanding the data to support the clinicians' activities. This fundamental explorative phase gets the key information needed to construct the KG.

The first set of information, contained in an RDF/XML file called hpo.owl (http://purl.obolibrary.org/obo/hp.owl), includes an ontology that contains standardized information on phenotypic anomalies. Such fundamental standardization creates the condition for interoperability and, therefore, for integrating data from multiple sources. Listing 3.1 shows a subset of the information included in the hpo.owl file related to Type I diabetes mellitus. The data is serialized from RDF/XML to Turtle (Terse RDF Triple Language) for improving readability.

Listing 3.1 Type I diabetes mellitus details reported in hpo.owl

obo:HP_0100651 a owl:Class ; #A

rdfs:label “Type I diabetes mellitus” ^^xsd:string ; obo:IAO_0000115 “A chronic condition in which the pancreas pr little or no insulin…” ^^xsd:string ; #B oboInOwl:created_by “doelkens”^^xsd:string ; #C oboInOwl:creation_date “2010-12-29T06:37:55Z”^^xsd:string ; oboInOwl:hasDbXref “MSH:D003922”^^xsd:string, #D “SNOMEDCT_US:46635009” ^^xsd:string, “UMLS:C0011854” ^^xsd:string ; oboInOwl:hasExactSynonym “Diabetes mellitus Type I”^^xsd:stri “Juvenile diabetes mellitus” ^^xsd:string, “Type 1 diabetes”, “Type I diabetes”; oboInOwl:hasRelatedSynonym “Insulin-dependent diabetes mellitus”^^xsd:string ; oboInOwl:id “HP:0100651”^^xsd:string ; rdfs:comment “The onset of type 1 diabetes is typically durin adolescence…” ^^xsd:string ; rdfs:subClassOf obo:HP_0000819 . #E

Reading an OWL file can be challenging. Therefore, you can use the rdflib Python library to explore this file as a collection of triples, each of which includes a subject, a predicate, and an object. This library enables users to create, parse, query, and serialize RDF data in various formats. The Python script in listing 3.2 illustrates how to easily explore the hpo.owl file using rdflib.

Listing 3.2 Process an OWL file using the rdflib Python library

from rdflib import Graph, URIRef g = Graph()

g.parse("hp.owl", format="xml")

g.bind("obo", "http://purl.obolibrary.org/obo/") g.bind("rdf", "http://www.w3.org/1999/02/22-rdf-syntax-ns#") g.bind("rdfs", "http://www.w3.org/2000/01/rdf-schema#") g.bind("xsd", "http://www.w3.org/2001/XMLSchema#")

subject_uri = URIRef("http://purl.obolibrary.org/obo/HP_0100651") filtered_statements = g.triples((subject_uri, None, None)) for subject, predicate, obj in filtered_statements: print(f"({g.qname(subject)}, {g.qname(predicate)}, {g.qname(obj)

print()

The output from this script is displayed in listing 3.3 (long strings have been truncated for clarity).

Listing 3.3 Sample of OWL file shown as a set of triples

(obo:HP_0410050, rdf:type, owl:Class)

(obo:HP_0410050, owl:equivalentClass, N25507ac984704bd78a0effd951

(obo:HP_0410050, rdfs:subClassOf, obo:HP_0011013)

(obo:HP_0410050, obo:IAO_0000115, A decrease in the level of…)

(obo:HP_0410050, dc:date, 2018-01-27T00:26:24+00:00)

(obo:HP_0410050, dcterms:creator, ns1:0000-0001-5208-3432)

(obo:HP_0410050, oboInOwl:hasExactSynonym, Decreased level of 1,5

(obo:HP_0410050, oboInOwl:hasExactSynonym, Decreased level of 1,5

(obo:HP_0410050, oboInOwl:hasExactSynonym, Decreased level of 1,5

(obo:HP_0410050, rdfs:label, Decreased level of 1,5 anhydroglucit

The second set of information, contained in the TSV file phenotype.hpoa, collects recognized, discovered, and annotated phenotypic features associated with different diseases including rare syndromes. The available data shows that more than 20 features are associated with Atypical Juvenile Parkinsonism (https://hpo.jax.org/app/browse/disease/ORPHA:391411). These annotations also include the modifiers that clarify the age of onset and

the frequency of each feature concerning the illness.

Note that Atypical Juvenile Parkinsonism, Rigidity, and Hypomimic face are frequent, meaning they are present from 80–99% of all cases. At the same time, the frequency for Anarthria is occasional. Anarthria is a defect in the motor ability that enables speech. The following listing provides a sample of this annotation file.

Listing 3.4 Sample of the phenotype.hpoa file

database_id disease_name qualifier hpo_id reference

OMIM:222100 Diabetes mellitus, insulin-dependent-1

HP:0410050 PMID:9357814;PMID:17659063;PMID:16731998 PCS 30/30 P HPO:NicoleVasilevsky[2018-02-23];HPO:NicoleVasilevsky[2018-03

OMIM:222100 Diabetes mellitus, insulin-dependent-1

HP:0000103 OMIM:222100 IEA P HPO:iea[2009-02-17]

The annotation file reported in listing 3.4 includes the following fields:

database id (OMIM:222100): disease identifier from diverse ontologies such as OMIM, Orphanet, DECIPHER; disease name (Diabetes mellitus, insulin-dependent-1): disease name from the related ontology; qualifier (empty string): optional field that can be used to qualify the annotation; hpo_id (HP:0410050): HPO identifier of the related phenotypic abnormality; reference (PMID:9357814;PMID:17659063;PMID:16731998): source of information used for the annotation. This may be taken from an article indicated with the related PubMed ID (PMID); evidence (PCS): level of evidence supporting the annotation. The PCS stands for Published Clinical Study; onset (empty string): optional field indicating age of onset; frequency (30/30): A count of patients affected within a group of people with a common statistical characteristic. 30/30 indicates that 30 of the 30 patients with the specified disease were found to have the phenotypic

abnormality referred to by the HPO term; sex (empty string): optional field that refers to phenotypic sex; modifier (empty string): optional field connected to the sex field; aspect (p): phenotypic aspect. P means Phenotypic abnormality; biocuration (HPO:NicoleVasilevsky[2018-02- 23];HPO:NicoleVasilevsky[2018-03-02]): reserch center or user making the annotation and the date on which the annotation was made.

For further details, the reader can check the following link: https://hpo- annotation-qc.readthedocs.io/en/latest/annotationFormat.html#phenotype- hpoa-format.

3.2 Understanding knowledge graph technologies

After the data understanding, the following phase is related to the ingestion and processing of the data located in the available sources. Before that, however, we will discuss the different KG technologies to make an informed decision according to our specific use case. This is a relevant topic because it affects how we create and store the KG and the use of a specific tool or a set of tools for the construction and persistence. Each model also drives some of the technical decisions that will follow. It is definitely worth spending some time learning about the KG technologies to make the right decision.

Two of the most popular approaches for creating KGs are:

Resource Description Framework (RDF). Labeled Property Graph (LPG).

RDF is a standard framework, defined and regulated by the World Wide Web Consortium (W3C), for data exchange on the web. With RDF, each statement is composed of three elements: subject, predicate, and object (called triples). The subject is a node (vertex) in the graph, the predicate represents a relationship (edge), and the object is another node. This framework models a KG as a collection of statements, which enables representing, storing, and exchanging information by exploiting web technologies. RDF is particularly suitable for creating ontologies describing a specific domain of knowledge by emphasizing the key concepts as well as how these concepts are related to

each other.

LPG, however, moves from a different objective. Its approach provides a fast, query-based traversal mechanism of graph data and path analysis features, highlighting the efficiency of data storage and access. Such efficiency is guaranteed by the structured information in the form of key-value pairs associated with nodes and relationships in the graph.

The definition of an internal structure for the relationships is one of the critical differences between RDF and LPG. In RDF, relationships (triples) are defined globally—meaning that metadata applied to a predicate affects all instances of that relationship throughout the graph. To address this limitation, RDF supports mechanisms such as Named Graphs, which enable groups of triples to be treated as a single entity. This enables the representation of context-specific information, such as data provenance or source attribution. In contrast, LPGs support unique edges between nodes, allowing metadata and properties to be attached directly to individual relationships. This provides a flexible model for representing fine-grained, edge-specific information. However, the RDF-DEV Community Group is currently working on a defined RDF* (as known as RDF-star) specification that allows users to add properties to the edges. This represents a promising direction in reconciling RDF and LPG technologies.

One of the missing parts of LPG is its capacity to express the advanced semantics of RDF, which enables reasoning mechanisms that are essential for deriving new knowledge not explicitly mentioned in the data. To address this specific issue, vendors such as Neo4j developed and provide specific tools that can reduce the gap between RDF and LPG. The Neosemantics plugin enables the use of RDF and its associated vocabularies (OWL, RDFS, SKOS, and others) in Neo4j and allows users to run basic inference. Other vendors, such as Amazon Neptune, employ alternative strategies that enable the execution of Cypher queries (the query language of LPG graphs) on RDF data.

After this high-level comparison, the following section presents limitations and opportunities in adopting RDF and LPG for the specific use case described in this chapter. This provides a straightforward approach to building your first KG.

3.2.1 RDF or LPG? A goal-driven discussion

To select the best technology for building the KG, we need to achieve a better understanding of the available information (in our case, the HPO ontology and the annotations data) and have a clear goal. We already mentioned that RDF is particularly suitable for creating ontologies. This is the reason why the HPO ontology is serialized using RDF. More specifically, as you may have noticed, the file extension of the hpo file is .owl. OWL stands for Web Ontology Language, and its primary goal is to enrich the semantic information available in RDF to support more expressive class definitions including union, complement, and disjointness as well as additional property definitions such as symmetry and transitivity. OWL ontologies are widely used, and many LLMs, including GPT and Claude, have been trained on them—making it easier for these models to interpret and reason over OWL- based data.

For the clinicians in this specific case, the critical information is not related to how the knowledge is represented and modeled. The most significant interest is related to the specific contents of the ontology. As a clinician, we are interested in an unambiguous representation of the phenotypic features, possibly preserving their hierarchical structure. For instance, by exploiting the ontology, a hypothetical system used by the clinician should be able to recognize automatically that abnormality of thyroid physiology includes other abnormal functionalities such as the abnormal thyroid hormone level, hyperthyroidism, and hypothyroidism.

Concerning the annotated data, we remind the reader that the core information lays in the annotation of the cases in which a specific phenotypic feature is identified with a disease. In many cases, this information directly comes from the scientific literature. For example, the entry that shows a connection between Diabetes Mellitus, Insulin-dependent-1 (OMIM:222100) and a Decreased level of 1,5 anhydroglucitol in serum (HP:0410050) was published in a clinical study entitled “A kinetic mass balance model for 1,5- anhydroglucitol: applications to monitoring of glycemic control” [3] (PMID: 9357814), created by Nicole Vasilevsky in February, 2018.

Intuitively, such data describe the nature or, in other words, the properties of

the relationship between a disease and a phenotypic feature. Therefore, the best way to model this information is to incorporate all these details into a relationship between Diabetes Mellitus, Insulin-dependent-1 (OMIM:222100) and Decreased level of 1,5 anhydroglucitol in serum (HP:0410050). Moreover, modeling information in this way allows us to create multiple relationships, each potentially representing a specific annotation characterized by a defined provenance and date.

Figure 3.6 illustrates how we can convert the data from the annotations located in a table structure into an edge in the KG. The disease and the phenotypic feature are represented as nodes, while information related to the author of the annotation, the creation date, and the source are specific properties of the edge identified as HAS_PHENOTYPIC_FEATURE in the figure.

Figure 3.6 Data transformation from a table row to a KG edge. Information included in this table is adapted to define the properties of nodes and the edges of the KG.

Exercise

We propose a conceptual exercise to invite the reader to select the best technology for supporting the clinicians’ activities for their defined use case. This exercise can be beneficial because we can replicate it in other situations and re-adapted it to different domains and applications. The following bullet points remind the reader of the main requirements in this specific context:

The goal of the clinician is to exploit the available data to make informed decisions in diagnosing the correct disease, especially for rare pathologies. In this case, the clinician is not interested in creating a knowledge base that fully represents and expresses the knowledge of the clinical domain. Clinicians are more interested in discovering in which cases anomalous phenotypic features (or a combination of them) can be associated with diseases that are not easy to detect. For this reason, they want access to information that reports these cases, considering the provenance and the date of this information. Using this metadata, clinicians want to make easy comparisons between all the cases in which a specific phenotypic feature is associated with a disease.

The selection of the right technology does not have a unique answer, but selecting the most suitable one helps to reach defined goals in a more straightforward way.

3.2.2 Representing edge properties with RDF and LPG

The requirements of the clinicians’ use case drive the selection of the most suitable technology. From the author’s point of view, LPG looks to be the best solution to represent the data because emphasis is given to the information related to an edge connecting a phenotypic feature and a disease.

To clarify why LPG is the most suitable technology to represent these properties, let’s make a concrete comparison between RDF and LPG. Moreover, we will show the reader how these representations can be queried using SPARQL and Cypher, respectively. The goal is to retrieve all the information, including source, author, and creation date, related to the annotation, which connects, for example, Diabetes Mellitus, Insulin- dependent-1 (OMIM:222100) and Decreased level of 1,5 anhydroglucitol in serum (HP:0410050). As we already mentioned, we can adopt specific workarounds to represent such data using RDF, which are described in detail in the following sections.

RDF: N-ary relations

One of the standard approaches to model the data related to a specific edge is adopting the n-ary relations. This approach consists of creating a new concept to connect such data; in our example, this is defined as an annotation. To better understand this approach, consider the RDF representation in listing 3.5 and the related SPARQL query in listing 3.6.

Listing 3.5 Example of n-ary relations

_:Annotation rdf:type :PhenotypicAnnotation ;

:forDisease OMIM:222100 ; :phenotypicFeature HP:0410050 ; :source PMID:9357814 ; :createdBy "Nicole Vasilevsky" ; :creationDate "2018-02-23"^^xsd:date .

This RDF snippet represents a phenotypic annotation using the Turtle syntax. The annotation is expressed as a blank node (_:Annotation), which is an unnamed resource used to group related information without assigning it a global identifier. A blank node can be considered as a placeholder for “something” that exists but doesn't need a specific name—much like an anonymous object in programming.

The blank node is typed as a :PhenotypicAnnotation and links a disease (identified by an OMIM ID) to a phenotypic feature (from the Human Phenotype Ontology). Additional metadata includes the data source (a PubMed ID), the author of the annotation, and the creation date. This structure supports provenance tracking and semantic interoperability in biomedical datasets.

Listing 3.6 SPARQL query in the context of n-ary relations

SELECT ?source ?author ?date WHERE {

?annotation a :PhenotypicAnnotation ; :forDisease OMIM:222100 ; :phenotypicFeature HP:0410050 ; :source ?source ; :createdBy ?author ; :creationDate ?date . }

This SPARQL query retrieves metadata about a specific phenotypic annotation. It filters annotations by a given disease (OMIM:222100) and phenotypic feature (HP:0410050), then returns the source of the information, the author who created the annotation, and the date it was created.

In many cases, data consumers can easily interpret and adapt to changes in the original schema. However, it is important to recognize that as the ontology evolves, its complexity may increase, potentially introducing challenges related to backward compatibility and long-term maintenance.

RDF: Named graphs

RDF formally introduced named graphs, which consist of a fourth element that we can attach to the formal statement. This fourth element specifies that this statement is part of a specific named (sub)graph and can be considered a node of the RDF graph itself. Therefore, we can create new statements for attaching the data related to the annotation. This approach is represented in listing 3.7 and the related SPARQL query is defined in listing 3.8.

Listing 3.7 Example of named graphs

:Graph1 {

OMIM:222100 :hasPhenotypicFeature HP:0410050 . }

:Graph1

:source PMID:9357814 ; :createdBy "Nicole Vasilevsky" ; :creationDate "2018-02-23"^^xsd:date .

This RDF example uses TriG syntax to define a named graph called :Graph1. In simple terms, TriG lets you group RDF statements together under a label —like putting related facts into a folder with a name. This “folder” is the named graph, and you can describe it (e.g., who created it, where it came from, when it was added).

Inside this graph, the triple asserts that the disease OMIM:222100 has the phenotypic feature HP:0410050. Metadata about this assertion is attached directly to the named graph :Graph1, including the source (PMID:9357814),

the creator (“Nicole Vasilevsky”), and the creation date. Named graphs like this allow contextual metadata to be added to groups of RDF statements, which is essential for tracking provenance and managing versions.

Listing 3.8 SPARQL query in the context of named graphs

SELECT ?source ?author ?date WHERE {

GRAPH :Graph1 { OMIM:222100 :hasPhenotypicFeature HP:0410050 . } :Graph1 :source ?source ; :createdBy ?author ; :creationDate ?date . }

This SPARQL query retrieves metadata about a specific phenotypic annotation stored in a named graph. It first looks inside the graph :Graph1 to find a triple asserting that OMIM:222100 has the phenotypic feature HP:0410050. It then queries metadata about :Graph1 itself—returning the source, the author, and the creation date. This pattern is useful when you need to audit or trace the origin of specific claims in linked data.

While named graphs provide powerful mechanisms for representing contextual metadata and provenance, their use can introduce additional complexity. In particular, managing a large number of named graphs may lead to inefficiencies in data storage and exchange. Furthermore, performing fine-grained updates at the level of individual statements within named graphs can become more challenging.

RDF-star

As previously mentioned, RDF-star (RDF*) is an extension of RDF that introduces a more expressive syntax for representing statements about statements. Its primary goal is to narrow the gap between RDF and property graph models such as LPG. This approach is illustrated in listing 3.9, with the corresponding SPARQL* query shown in listing 3.10.

Listing 3.9 Example of RDF-star

<<OMIM:222100 :hasPhenotypicFeature HP:0410050>>

:source PMID: 9357814 ; :createdBy "Nicole Vasilevsky" ; :creationDate “2018-02-23”^^xsd:date .

Listing 3.10 Example of SPARQL-star query in the context of RDF-star

SELECT ?source ?author ?date {

<<OMIM:222100 :hasPhenotypicFeature HP:0410050>> :source ?source ; :createdBy ?author ; :creationDate ? date . }

RDF-star represents a further step in the RDF world to attach properties to the edges with a compact syntax and a more readable SPARQL query. However, some articles mention that the query performance of RDF-star must be improved and, as noticed by Orlandi et al. [2], “The use of a new syntax extension requires a specific implementation of RDF engines and, therefore, limits the adoption of this approach.”

In addition to the approaches discussed, other methods exist for annotating RDF statements, such as reification and singleton properties. Reification involves creating a separate resource that represents a statement, allowing metadata to be attached directly to that statement. Singleton properties, on the other hand, generate unique property instances—typically by appending an identifier to the original predicate—to enable the association of additional information. These methods are generally less exploited in real-world applications, where more scalable and maintainable alternatives like named graphs or n-ary relations are preferred.

LPG

The Labeled Property Graph (LPG) approach represents annotation details directly within the relationship, using key-value pairs. This enables a compact and intuitive method for enriching the edges of the graph with metadata. An example of this modeling approach and the corresponding Cypher query are shown in listings 3.11 and 3.12, respectively.

Listing 3.11 Example of LPG representation

(d { id: “OMIM:222100” }) -[:HAS_PHENOTYPIC_FEATURE {

source: “PMID:9357814” createdBy: “Nicole Vasilevsky”; creationDate: “2018-02-23}]-> (p { id: “HP:0410050” })

This listing shows how an LPG encodes an annotated relationship between a disease and a phenotypic feature. The two nodes represent entities: a disease (OMIM:222100) and a phenotype (HP:0410050). The relationship :HAS_PHENOTYPIC_FEATURE connects them and includes key-value pairs that describe the source of the annotation (“PMID:9357814”), the creator (“Nicole Vasilevsky”), and the date it was created (“2018-02-23”).

Listing 3.12 Example of the Cypher query

MATCH (d)-[r:HAS_PHENOTYPIC_FEATURE]->(p) WHERE d.id = “OMIM:222100” and p.id = “HP:0410050” RETURN r.source, r.createdBy, r.creationDate

This Cypher query retrieves the metadata attached to the :HAS_PHENOTYPIC_FEATURE relationship between the same disease and phenotype nodes shown in listing 3.11. It matches the pattern in the graph, filters based on the node IDs, and returns the annotation details stored in the relationship.

As these examples demonstrate, the LPG model is particularly well-suited for modeling metadata-rich relationships in a way that is both expressive and accessible. For these reasons, and as explored throughout this chapter and much of the book, we adopt LPG and Cypher as core tools for building our KG system.

3.3 Knowledge graph building

After discussing the main features of RDF and LPG, let’s get in to the details of how to materialize our first KG. The first step consists of loading the ontology, and the second consists of ingesting a data source using the

ontology as a reference. To build the KG, you can run the code in the GitHub repository (https://github.com/alenegro81/knowledge-graphs-and-llms-in- action/tree/main/chapters/ch03) or directly test the Cypher queries available in this section using the Neo4j browser UI.

The code discussed in this chapter has been tested using a working instance of Neo4j (version 5.20.0 enterprise edition installed with the Neo4j Desktop 1.6.1 application), along with the APOC library (version 5.20.0) and the Neosemantics plugin (version 5.20.0). Details for installing Neo4j and its plugins are provided in the related appendix. Each query outlined in this chapter will be explained. However, this chapter assumes a basic understanding of the Cypher query language. The results of the analysis are derived from the version of the HPO data source available in February 2025.

3.3.1 Ontology ingestion and processing with neosemantics

Figure 3.7 illustrates the ontology ingestion and processing phase. The initial step involves creating and initializing the HPO database. You can create the database using the command in listing 3.13.

Figure 3.7 Ontology ingestion and processing.

Listing 3.13 Create the hpo database in Neo4j

CREATE DATABASE hpo IF NOT EXISTS

Next, in listing 3.14, we establish constraints that ensure the uniqueness of the uri and id properties of the nodes labeled as Resource. We also create indexes for the id properties of HpoPhenotype and HpoDisease nodes to enhance access to this information during the KG building phase and the

information retrieval. HpoPhenotype and HpoDisease are the key labels that define our phenotypic abnormality and disease nodes.

Listing 3.14 Create constraints and indexes

CREATE CONSTRAINT n10s_unique_uri IF NOT EXISTS FOR (r:Resource) CREATE CONSTRAINT IF NOT EXISTS FOR (n:Resource) REQUIRE (n.id) I CREATE INDEX disease_id IF NOT EXISTS FOR (n:HpoDisease) ON (n.id CREATE INDEX phenotype_id IF NOT EXISTS FOR (n:HpoPhenotype) ON (

The second step defines an initial configuration for the neosemantics component. This establishes how neosemantics imports the data. Listing 3.15 shows this step.

Listing 3.15 Set the configuration of the Neosemantics plugin

CALL n10s.graphconfig.init(); CALL n10s.graphconfig.set({ handleVocabUris: "IGNORE" }); CALL n10s.graphconfig.set({ applyNeo4jNaming: True });

This configuration defines two main importing rules. The first rule ignores the namespaces in the import phase (namespaces can be helpful in keeping track of distinct ontologies that use similar expressions.) The second rule encodes the relationship types in uppercase, following the standard representation of the LPG relationships. After the configuration, the next step is to load the HPO vocabulary using the command in the listing 3.16.

Listing 3.16 Load the HPO ontology in to Neo4j

CALL n10s.rdf.import.fetch("http://purl.obolibrary.org/obo/hp.owl

At the end of this step, 899,558 statements are loaded into Neo4j (tested with the ontology version available on the HPO website in February 2025). Before processing and loading the annotation data, we can enrich our nodes with the HpoPhenotype label and the id property computed from the resource's original URI. The code in Listing 3.17 performs this node enrichment process.

Listing 3.17 Enrich Resource nodes representing phenotypic features/abnormalities

MATCH (n:Resource) WHERE n.uri STARTS WITH "http://purl.obolibrary.org/obo/HP" SET n:HpoPhenotype,

n.id = coalesce(n.id, replace(apoc.text.replace(n.uri,'(.*)obo/',''),'_', ':')) /

Let's take a moment to review the current state of the KG after the ontology ingestion. Listing 3.18 shows the code used to retrieve a small portion of this graph, illustrated in figure 3.8. The reader can explore this by running the code in the Neo4j browser, as the Python script includes a cleaning phase that removes nodes and relationships from the ontology that are not strictly necessary for our use case.

Warning Message

The query reported in listing 3.18 will only work if executed following the chapter instructions, one step at a time. If you run the entire ingestion process using the repository code, the query will fail due to the final data cleaning phase.

Listing 3.18 Show a small portion of the KG at the current stage

MATCH path1=(n:HpoPhenotype)<-[:SUBCLASSOF]-(m:HpoPhenotype) WHERE n.label = "Diabetes mellitus" WITH path1 MATCH path2=(i:HpoPhenotype)<-[:ANNOTATEDSOURCE]-(j) WHERE i.label in ["Diabetes mellitus", "Type I diabetes mellitus" WITH path1, path2, j MATCH path3=(j)-[:ANNOTATEDPROPERTY|HASSYNONYMTYPE]-() RETURN path1, path2, path3

Figure 3.8 A portion of the HPO ontology loaded in the graph database using LPG as storage model. We can distinguish between two types of information: ontological information (left side) and domain-specific information related to the phenotypic features (right side).

The HPO ontology provides different types of information. The left side of figure 3.8 reports ontological information on the nature of the node. This shows, for instance, nodes that can be categorized as definitions. The right side in the figure includes details on the hierarchical connections related to diabetes mellitus. This second type of information can be more interesting for the clinician in our specific use case.

3.3.2 Annotations ingestion and processing

At this point, we have completed the ontology loading step. To complete the KG construction, we must ingest and process the annotations file. The phenotypic abnormalities included in this file are connected to the associated diseases, whose terms come from other ontologies. Figure 3.9 shows the second phase of the data processing and modeling.

Figure 3.9 Annotation data set ingestion and processing to finalize the construction of the KG.

Unlike the hpo.owl file generated using the RDF data model, our next file is provided using an annotation format called hpoa, simply a tab-separated- values format (TSV). But this type does not occur in every case. For knowledge bases such as DBpedia (https://www.dbpedia.org/) and Wikidata (https://www.wikidata.org/) the vocabulary and the annotations are released using RDF standards. The hpoa file includes valuable information such as:

An explicit association between a disease and multiple phenotypic

features or abnormalities. A piece of evidence supporting this association; for example, inferred from electronic annotation, published clinical study, or traceable author statement. The age of onset that is described as a phenotypic feature. The frequency with which a disease and a phenotypic feature appear together can also be described as a phenotypic feature. Additional metadata that describes the ontology source of the specified terms.

Further details are available at the following link: https://hpo-annotation- qc.readthedocs.io/en/latest/annotationFormat.html.

Working with this TSV file allows us to incorporate a different type of file coherently based on the existing knowledge. The following Cypher queries from listing 3.19 to listing 3.24 let us load, process, and integrate information from the annotations file. We split the whole process for readability and maintain data integrity. Listing 3.19 shows how to create disease nodes.

Listing 3.19 Create HpoDisease nodes

LOAD CSV FROM 'https://github.com/obophenotype/human-phenotype-on FIELDTERMINATOR '\t' WITH row SKIP 5 // #A MERGE (dis:Resource:HpoDisease {id: row[0]}) ON CREATE SET dis.label = row[1];

Listing 3.20 shows how to create the relationships between disease nodes and phenotypic feature nodes.

Listing 3.20 Create the relationships between HpoDisease and HpoPhenotype nodes

LOAD CSV FROM 'https://github.com/obophenotype/human-phenotype-on FIELDTERMINATOR '\t' WITH row SKIP 5 MATCH (dis:HpoDisease) WHERE dis.id = row[0] MATCH (phe:HpoPhenotype) WHERE phe.id = row[3]

MERGE (dis)-[:HAS_PHENOTYPIC_FEATURE]->(phe)

The creation of these relationships represents a key step in integrating information from the hpo.owl and the phenotype.hpoa files. The code in listing 3.21 allows you to query the result of this integration process.

Listing 3.21 Sample of association between HpoDisease and HpoPhenotype nodes

MERGE (dis:HpoDisease)-[:HAS_PHENOTYPIC_FEATURE]->(phe:HpoPhenoty RETURN dis.label, collect(phe.label) LIMIT 3

The result of the query is reported in table 1.

Table 3.1 Sample of associations between HpoDisease and HpoPhenotype nodes.

HpoDisease name HpoPhenotype name Developmental and epileptic encephalopathy 96

Hydrops fetalis, Autosomal dominant inheritance, Death in infancy, Epileptic spasm, Primary microcephaly, EEG with burst suppression, Intellectual disability, profound, Small for gestational age, Epileptic encephalopathy, Neonatal respiratory distress, Tonic seizure Pseudohyperkalemia, familial, 2, due to red cell leak

Generalized muscle weakness, Hyperkalemia, Periodic paralysis, Muscle spasm, Hemolytic anemia, Hand tremor, Autosomal dominant inheritance Immunoglobulin kappa light chain deficiency

Chronic diarrhea, Recurrent infections, Recurrent respiratory infections, Absent circulating immunoglobulin kappa chain, Childhood onset, Diarrhea, Autosomal recessive inheritance

Listing 3.22 reports code to add relationship properties in the form of key- value pairs.

Listing 3.22 Add properties to the HAS_PHENOTYPIC_FEATURE relationships

LOAD CSV FROM 'https://github.com/obophenotype/human-phenotype-on FIELDTERMINATOR '\t' WITH row SKIP 5 MATCH (dis:HpoDisease)-[rel:HAS_PHENOTYPIC_FEATURE]->(phe:HpoPhen WHERE phe.id = row[3] and dis.id = row[0] FOREACH(ignoreMe IN CASE WHEN row[4] is not null THEN [1] ELSE [] SET rel.source = row[4]) FOREACH(ignoreMe IN CASE WHEN row[5] is not null THEN [1] ELSE [] SET rel.evidence = row[5]) FOREACH(ignoreMe IN CASE WHEN row[6] is not null THEN [1] ELSE [] SET rel.onset = row[6]) FOREACH(ignoreMe IN CASE WHEN row[7] is not null THEN [1] ELSE [] SET rel.frequency = row[7]) FOREACH(ignoreMe IN CASE WHEN row[8] is not null THEN [1] ELSE [] SET rel.sex = row[8]) FOREACH(ignoreMe IN CASE WHEN row[9] is not null THEN [1] ELSE [] SET rel.modifier = row[9]) FOREACH(ignoreMe IN CASE WHEN row[10] is not null THEN [1] ELSE [ SET rel.aspect = row[10]) FOREACH(ignoreMe IN CASE WHEN row[11] is not null THEN [1] ELSE [

SET rel.biocuration = row[11])

Listing 3.22 illustrated a flexible approach to adding new properties that enrich relationship information. This script matches existing nodes and relationships in the Neo4j graph, and conditionally sets additional relationship properties based on the presence of values in each row of the input file. Each of the following FOREACH blocks is used to add a new property to the relationship only if the corresponding column in the CSV is not null. This makes the script resilient to missing data and avoids overwriting values with nulls. Then, we recommend incorporating the information from the query presented in listing 3.23 to clarify the meaning of the properties associated with the relationships between diseases and phenotypic features.

Listing 3.23 Enrich HAS_PHENOTYPIC_FEATURE with more properties

CALL apoc.periodic.iterate(

"MATCH (dis:HpoDisease)-[rel:HAS_PHENOTYPIC_FEATURE]->(phe:Hp "SET rel.createdBy = apoc.text.regexGroups(rel.biocuration, ' rel.creationDate = apoc.text.regexGroups(rel.biocuration, '\ rel.aspectName = CASE

WHEN rel.aspect = 'P' THEN 'Phenotypic abnormality' WHEN rel.aspect = 'I' THEN 'Inheritance' END, rel.aspectDescription = CASE WHEN rel.aspect = 'P' THEN 'Terms with the P aspect are WHEN rel.aspect = 'I' THEN 'Terms with the I aspect are END, rel.evidenceName = CASE WHEN rel.evidence = 'IEA' THEN 'Inferred from electronic WHEN rel.evidence = 'PCS' THEN 'Published clinical study WHEN rel.evidence = 'TAS' THEN 'Traceable author stateme END, rel.evidenceDescription = CASE WHEN rel.evidence = 'IEA' THEN 'Annotations extracted by WHEN rel.evidence = 'PCS' THEN 'PCS is used for informat WHEN rel.evidence = 'TAS' THEN 'TAS is used for informat END, rel.url = CASE WHEN rel.source STARTS WITH 'PMID:' THEN 'https://pubmed WHEN rel.source STARTS WITH 'OMIM:' THEN 'https://omim.o END", {batchSize: 1000})

The query in listing 3.23 leverages the apoc.periodic.iterate mechanism to efficiently process and update the HAS_PHENOTYPIC_FEATURE relationships in batches. For example, it creates metadata from the biocuration property by extracting the curator and the creation date using a regex. Moreover, the query adds further properties to relationships to improve their readability during graph exploration.

The annotation file includes only a short version of the information related to aspect (P or I values) and evidence (IEA, PCS, or TAS values). To clarify such information, we added new properties such as aspect_name, which can have Phenotypic abnormality or Inheritance as value, and evidence_name, which can have Inferred from electronic annotation or Published clinical study or Traceable author statement as value. The main goal of the query is to simplify access to information for a human being.

The final step of the building process is to clean the KG by removing nodes and relationships that come from the ontology but are not strictly necessary for our purposes. The query in Listing 3.24 shows how to perform this cleaning process.

Listing 3.24 Clean the KG by removing unnecessary nodes and relationships

CALL apoc.periodic.iterate(

"MATCH (n:Resource) RETURN id(n) as id", "MATCH (n) WHERE id(n) = id AND NOT 'HpoPhenotype' in labels(n) AND NOT 'HpoDisease' in labels(n) DETACH DELETE n", {batchSize:10000}) YIELD batches, total return batches, total

3.4 Querying the data

After constructing the KG, clinicians can exploit the KG as support for diagnosing rare diseases, starting with the detection of specific phenotypic abnormalities in a patient. Entering specific traits, the clinicians or an application at their disposal can perform some queries to the KG to identify the presence of a rare pathology. This querying phase represents the final step of our mental model, and it is depicted in figure 3.10.

Figure 3.10 Querying the generated KG to support the clinician's activities.

Let's imagine a clinician meets a known patient, a young boy affected by Type 1 diabetes. The patient's clinical history is stored in the hospital database as an electronic health record (EHR). Imagine also that the hospital has embraced the KG paradigm change. Therefore, the patient information is stored using the terms included in the HPO and the Online Mendelian Inheritance in Man (OMIM), an online catalog of genetic disorders and rare diseases. Type 1 diabetes is classified as a phenotypic feature and a disease,

and for this reason, the information is stored using two different identification codes:

HP:0100651 (phenotypic feature): https://hpo.jax.org/app/browse/term/HP:0100651. OMIM:222100 (disease): https://www.omim.org/entry/222100.

Considering the patient's disease, the clinician recognizes the typical phenotypic features of Type 1 diabetes, which can also be explored in the KG with the query in the following listing 3.25. Figure 3.11 shows the result of this query.

Listing 3.25 Phenotypic features associated with Type 1 diabetes

MATCH path=(dis:Disease)-[:HAS_PHENOTYPIC_FEATURE]->(phe:Hpo) WHERE dis.id = "OMIM:222100" RETURN path

Figure 3.11 Result of a query that gets all the phenotype features related to Type 1 diabetes.

In this result, the central node defines Type 1 diabetes, and the other nodes define the associated phenotypic features. However, during the medical examination, the clinician recognizes new symptoms classified as phenotypic features. These are not directly connected to Type 1 diabetes, however. Among these features, the clinician catches:

Growth delay: https://hpo.jax.org/app/browse/term/HP:0001510. Large knee: https://hpo.jax.org/app/browse/term/HP:0030866. Sensorineural hearing impairment: https://hpo.jax.org/app/browse/term/HP:0000407. Pruritus: https://hpo.jax.org/app/browse/term/HP:0000989.

The clinician wants to exploit the information within the KG to discover the presence of other pathologies connected to these phenotype features. To perform this task, the clinician runs the query in the following listing. Table 3.2 shows the result of the query in Listing 3.26.

Listing 3.26 Top diseases associated with specific phenotypic features

MATCH (phe:HpoPhenotype) WHERE phe.label in ["Growth delay", "Large knee", "Sensorineural WITH phe MATCH path=(dis:HpoDisease)-[:HAS_PHENOTYPIC_FEATURE]->(phe) UNWIND dis as nodes RETURN dis.id as disease_id, dis.label as disease_name, collect(phe.label) as features, count(nodes) as num_of_features ORDER BY num_of_features DESC, disease_name LIMIT 5

Table 3.2 Top diseases matching clinician-identified phenotypic features

disease_id disease_name features num_of_feature OMIM:619269 Ondontochondrodysplasia

Growth delay, Sensorineural hearing impairment,

5

2 with hearing loss and diabetes

Pruritus, Large knee, Type I diabetes mellitus OMIM:618500 Holoprosencephaly 12

Sensorineural hearing impairment, Growth delay, Type I diabetes mellitus

3

with or without pancreatic agenesis

OMIM:614700 3-methylglutaconic

Growth delay, Sensorineural hearing impairment

2

aciduria, type VIII

OMIM:616192 Alobar

Growth delay, Sensorineural hearing impairment

2

holoprosencephaly

OMIM:602782 Alpha-

Growth delay, Type I diabetes mellitus

2

Thalassemia/mental retardation syndrome, X- linked

These results show that the patient’s symptoms led to the diagnosis of Ondontochondrodysplasia 2 with hearing loss and diabetes. Starting with these results, the clinician can perform further investigations to discover how frequently these phenotypic features are associated with such a disease and, eventually, if there are sources such as publications that can be further explored in this context.

Exercise

Extend the query in listing 3.26, getting the relationship properties such as

evidence_name, evidence_description, source, and url.

3.5 Reasoning over the knowledge graph

In the previous case, we analyzed how to obtain results from the explicit information stored in the KG. However, one of the most powerful tools of a KG is the inference mechanism, which follows deductive reasoning principles (see Chapter 2 for further details) based on logical rules, allowing us to derive results from implicit information. Consider, for instance, the following question: which diseases are characterized by an abnormality of the endocrine system?

Some annotations are explicitly connected to this phenotypic feature. But a clinician would also be interested in more specific phenotypic traits that involve the thyroid. For this particular purpose, we can use the hierarchical representation of HPO. Consider the query in listing 3.27, which allows us to get a subset of the phenotypic features representing subclasses of the abnormality of the endocrine system.

Listing 3.27 Subclasses of abnormality of the endocrine system (id=HP:0000818)

MATCH (p:HpoPhenotype)<-[:SUBCLASSOF*1..3]-(n:HpoPhenotype)// #A WHERE p.id = "HP:0000818" RETURN p,n

Using this hierarchical structure, we can infer annotations implicitly linked to the abnormalities of the endocrine system through a Neosemantics procedure reported in listing 3.28. A subset of the results of this procedure is presented in Table 3.3.

Listing 3.28 Phenotypic features related to the abnormality subclasses

MATCH (cat:HpoPhenotype {label: "Abnormality of the endocrine sys CALL n10s.inference.nodesInCategory(cat, {

inCatRel: "HAS_PHENOTYPIC_FEATURE", subCatRel: "SUBCLASSOF"}) YIELD node as dis MATCH (dis)-[:HAS_PHENOTYPIC_FEATURE]->(phe:HpoPhenotype) RETURN dis.label as disease, collect(DISTINCT phe.label) as featu ORDER BY size(features) ASC, disease

SKIP 200 LIMIT 5

Table 3.3 Subset of the results of the annotations that are implicitly connected to the Abnormality of the endocrine system phenotypic feature

disease features Congenital atransferrinemia

Anemia, Abnormality of the pancreas, Recurrent infections, Arthritis, Abnormality of the cardiovascular system, Hypothyroidism Deafness, autosomal recessive 4, with enlarged vestibular aqueduct

Enlarged vestibular aqueduct, Congenital onset, Goiter, Autosomal recessive inheritance, Incomplete partition of the cochlea type II, Sensorineural hearing impairment Diabetes mellitus, transient neonatal, 1

Transient neonatal diabetes mellitus, Autosomal dominant inheritance, Dehydration, Hyperglycemia, Intrauterine growth retardation, Severe failure to thrive Edema, familial idiopathic, prepubertal

Diabetes mellitus, Abnormality of the genitourinary system, Irritability, Vomiting, Autosomal dominant inheritance, Edema Familial dysalbuminemic hyperthyroxinemia

Abnormal circulating free T4 concentration", " Abnormal thyroid-stimulating hormone level", "Autosomal dominant inheritance", "Autosomal recessive inheritance", " Euthyroid hyperthyroxinemia", " Increased circulating free T4 concentration"

This result set includes diseases directly connected to the abnormality of the endocrine system. Moreover, in Table 3.3 we have highlighted all the phenotypic features that are direct or inferred subclasses of abnormality of the endocrine system.

3.6 Summary

Constructing the knowledge graph (KG) is a complex process because it requires extracting and integrating information from heterogeneous

sources. Incorporating these sources aims to harmonize their data and leads to a coherent perspective on the general information. At the end of the building process, the resulting KG must be a unified, well-grounded, and meaningful representation of data from different sources, where the individual pieces of information are fused into a unique view. A well-driven KG construction requires a clear idea of the problem you want to solve, an understanding of the reference domain, and a phase involving data scouting, exploration, and comprehension. The Resource Description Framework (RDF) and the labeled property graph (LPG) are two of the most prominent technologies for materializing KGs. The RDF data model focuses on knowledge representation and is particularly suitable for constructing ontologies. The LPG approach intends to provide a fast and query-based traversal mechanism of graph data and path analysis features, emphasizing the efficiency of data storage and access. Understanding the differences between RDF and LPG is crucial to selecting the best technologies for your specific purpose. Building a KG from the Human Phenotype Ontology (HPO) dataset can be particularly helpful to clinicians, which enables them to make better diagnoses, starting with the analysis of the phenotypic features detected in a patient.

3.7 References

1. Kohler, S., Gargano, M., Matentzoglu, N., Carmody, L. C., Lewis-

Smith, D., Vasilevsky, N. A., ... & Robinson, P. N. (2021). The human phenotype ontology in 2021. Nucleic acids research, 49(D1), D1207- D1217. https://academic.oup.com/nar/article/49/D1/D1207/6017351? login=true 2. Orlandi, F., Graux, D., & O'Sullivan, D. (2021, January). Benchmarking

RDF Metadata Representations: Reification, Singleton Property, and RDF. In 2021 IEEE 15th International Conference on Semantic Computing (ICSC) (pp. 233-240). IEEE. https://ieeexplore.ieee.org/abstract/document/9364401/

3. Stickle, D., & Turk, J. (1997). A kinetic mass balance model for 1, 5-

anhydroglucitol: applications to monitoring of glycemic control. American Journal of Physiology-Endocrinology and Metabolism, 273(4), E821-E830. https://journals.physiology.org/doi/full/10.1152/ajpendo.1997.273.4.E821

## 4_From_simple_networks_to_multi-source_integration

This chapter covers

Building KGs of increasing complexity from structured data and integrating them Knowledge graph exploration examples Analysis and query techniques LLM-assisted interpretation of knowledge graph analysis results

Our journey on knowledge acquisition continues in this chapter. Chapter 3 shows how to leverage an ontology to build a first simple knowledge graph that can help clinicians to identify diseases and heal the patient starting from their symptoms. That first example of knowledge acquisition showed, in a concrete way, which kind of applications can be “empowered” by knowledge graphs. This chapter move from the previous one and extend our understanding of how to build knowledge graphs of growing complexity and size and how to leverage them to build more powerful intelligent advisor systems.

It's worth noting here that the major difference with the previous chapter is that before we had a single, simple, well-organized knowledge base, in the form of ontology, that we transformed in an LPG-based knowledge graph, from now on we deal with more complex knowledge graphs, created from multiple data sources. To focus on knowledge graph design patterns and analytical techniques, this chapter works with structured data sources that are already available in graph-friendly formats. This approach allows us to emphasize graph modeling decisions, integration strategies, and analysis methods while maintaining clear objectives. Readers who need to start from completely raw data sources will find comprehensive guidance on data ingestion and transformation processes in Appendix C. Through the examples presented in the following sections we cover in particular:

The transformation of many heterogenous schemas and data formats in a homogenous graph. Possible solutions to the problem of names and identifiers reconciliation and matching. The post processing techniques to merge entities and relationships and clean the resulting graph. Analysis of the resulting knowledge graph to distill first set of relevant information before moving to more complex algorithms.

On purpose, at this stage of our journey, the data sources considered are structured and semi-structured, i.e., data stored using a defined and easy to read schema such as CSV, relational database, or a non-fixed schema but still easy to read like JSON, XML and similar. Part III of the book discusses, instead, how to build knowledge graphs from textual data. These two types of data sources are “complementary” and not “alternative”. As we will see along the rest of the book, proper knowledge graphs acquired from the different data available to the company are always a combination of both structured and unstructured data.

For the knowledge graph construction examples in this chapter, we use biomedical data sources. This choice is driven by several practical and technical considerations:

Data complexity and variety: Biomedical domains naturally contain the full spectrum of knowledge graph challenges-from simple networks (protein interactions) to complex multi-domain integrations (genes, diseases, drugs, pathways) Rich relationship types: The field offers diverse relationship patterns that demonstrate various graph modeling approaches and integration techniques High-quality structured datasets: Extensive publicly available databases with well-documented schemas, making examples fully reproducible Real-world complexity: The domain presents authentic challenges like entity disambiguation, multi-source integration, and conflicting data that you'll encounter in any field Comprehensive coverage: Allows us to demonstrate the complete progression from simple networks to complex multi-source systems in a

single coherent domain LLM integration potential: This domain is at the forefront of combining knowledge graphs with LLMs to solve complex problems like drug repurposing, discovering new disease treatments, and advancing precision medicine Social impact: By providing practical examples in this field, we aim to support the amazing work being done to improve human health and save lives through advanced KGs and LLMs applications

The techniques, patterns, and approaches demonstrated here are directly transferable to any domain requiring knowledge graph construction from structured sources. For instance, in law enforcement, intelligence, tax evasion and criminal investigation domains you must load and merge data coming from different structured data sources such as phone call records, cellular towers, vehicle data registry, property records, and so on coming from different companies and entities. The same goes for the bank or retail sectors where the knowledge graph collects data from multiple data silos related to many organizational units. Unfortunately, for all these domains the data is quite sensitive because it’s related to people, so it is hard to find good and sufficient resources and our customers in these domains cannot share their data even in an anonymized form to avoid the chance they could be matched with real people and real events.

While this chapter focuses primarily on building knowledge graphs from structured data sources, Large Language Models (LLMs) play complementary but limited roles in this phase of the knowledge graph lifecycle. In structured data integration, LLMs can assist with entity resolution tasks—helping to identify when two nodes from different data sources represent the same real-world entity, particularly when dealing with variations in naming conventions or identifiers across databases. Additionally, LLMs serve as valuable tools for validating the semantic consistency of integrated data and providing quality assurance during the graph construction process. However, the most significant applications of LLMs with knowledge graphs—such as natural language querying, automated reasoning, and intelligent interpretation of analytical results—are covered in Part V of this book. The structured nature of the data sources in this chapter (CSV files, relational databases, and well-defined APIs) means

that traditional data integration techniques remain the primary approach, with LLMs serving as auxiliary tools rather than core components of the construction pipeline.

4.1 Biomedical Knowledge graphs and applications

Imagine you are working in pharma or bio medical industries or for a knowledge graph company that aims at supporting clinicians in their job. In these cases, you could be tasked to solve or provide support for one or multiple of the following problems:

Starting from known relationships between diseases and proteins as well as interactions among proteins, can we discover new connections between diseases and proteins? Can we discover meaningful unknown relationships between micro- RNAs (see later for a detailed description) and diseases without expensive in-vitro tests? What are the key processes involved in the celiac diseases (or any other disease)? Is it possible to repurpose existing drugs for a new or old disease without a multi-year long (and very expensive) study? How can we support precision medicine, that uses patient specific information like DNA, lifestyle, etc., to heal people faster and better increasing the cure success rate?

All these tasks, and many more can be addressed organizing the existing biomedical knowledge in the form of graphs and analyzing it properly. Starting from the business needs, the construction of the knowledge graphs requires multiple relevant data to be collected from several sources, cleaned, imported, and merged. At that point it is possible to start analyzing the graph to extract relevant insights.

The purpose of the analysis at this stage is to verify that the constructed graph contains the knowledge necessary to answer complex questions. In many cases, to answer those questions, a query will be enough. In other, it requires machine learning algorithms involving embeddings, features extraction, classification and so on. These techniques will be described in the fourth part

of the book.

Another interesting side effect of this chapter is that we present many techniques based on code and queries that allow practitioners to extract relevant insights from knowledge graphs. These are in the form of statistics analysis, qualitative evaluation, and objective checks. They represent preliminary steps to assure that more advanced analytics are based on solid knowledge. In many cases the extracted information can be part of a more complex feature extraction process, useful, later, as input for machine learning tasks.

Let’s start by delimiting the context of this domain. Biomedical science deals with the organs and systems of the human body focusing on medical applications. This area of science focuses mostly on diseases, gene expressions, proteins, drugs, and related topics. It differs from biological and life science domains that are way more generic. They consider any form of life, not only humans, and any biological process, not only those that have impact on human health.

Considering the tasks described earlier, let’s try to contextualize them in well-known applications categories. As reported from Nicholson and Green in [1], knowledge graphs can help researchers tackle many biomedical problems such as finding new treatments for existing drugs [2], aiding efforts to diagnose patients [3] and identifying associations between diseases and biomolecules [4], identifying proteins’ functions [5], prioritizing cancer genes [6] or recommending safer drugs for patients [7,8].

Each application has different business goals and, according to our CRIPS- DM model, has been built using different selected data sources. Each related knowledge graph contains the information necessary to accomplish such scopes without polluting it with irrelevant information. Based on this principle we will describe the different types of biomedical knowledge graph starting from their application and defining then the content of such graphs to accomplish such purposes. This is an important exercise for you to learn how to select the proper data sources to feed the knowledge graph and determine when the information stored is enough to accomplish the required tasks.

Figure 4.1 represents a summary of the different applications what we are

going to present later together with the most relevant piece of information stored as nodes and relationships. Use it as an easy-to-read reference.

Figure 4.1 The main types of biomedical applications of knowledge graphs. The classification uses the business goal to group them, while they have many data sources in common.

This is not an exhaustive list. The applications and datasets have been selected since they are very relevant for other sessions of this chapter, and they address the scenarios described. For each application a concrete case is presented together with the code for importing and merging the source databases and how to query and analyze the resulting graphs, in order to start our journey to answer complex questions.

4.2 Multi-omic applications

Multi-omic is used to indicates a biological analysis approach that leverage many “omes[1]” datasets, such as, for instance, genome, proteome, transcriptome.

Genome, Transcriptome and Proteome

These terms, and some related concepts, will be used often along this chapter. It is worth spending few lines to describe once per all[2]:

Genome: it represents the entire genetic complement of a living organism. Every organism possesses a genome that contains the biological information needed to construct and maintain a living example of that organism. Most genomes, including the human genome and those of all other cellular life forms, are made of DNA (deoxyribonucleic acid) but a few viruses have RNA (ribonucleic acid) genomes. DNA and RNA are polymeric molecules made up of chains of monomeric subunits called nucleotides. Transcriptome: The initial product of genome expression is the transcriptome, a collection of RNA molecules derived from those protein-coding genes whose biological information is required by the cell at a particular time. These RNA molecules direct synthesis of the final product of genome expression, the proteome, the cell's repertoire of proteins, which specifies the nature of the biochemical reactions that the cell can carry out. The transcriptome is constructed by the process called transcription, in which individual genes are copied into RNA molecules. Proteome: The proteome is the final product of genome expression and comprises all the functioning proteins synthesized by a living cell. Construction of the proteome involves translation of these RNA molecules into proteins. The proteome can be looked upon as the central link between the genome and the cell: it is, on the one hand, the culmination of genome expression and, on the other hand, the starting point for the biochemical activities that constitute cellular life.

Figure 4.2 represents how Genomes, Transcriptomes and Proteomes are related to each other.

Figure 4.2 The three primary 'omics' data types used in biomedical knowledge graphs. The diagram illustrates how genome (DNA), transcriptome (RNA), and proteome (protein) data are biologically connected through transcription and translation processes. Understanding these relationships is crucial for modeling multi-omic knowledge graphs that integrate genetic, expression, and functional protein data.

An enormous variety of multi-omic applications employ knowledge graphs to study the genome, how genes are expressed in the transcriptome, and how the products of those transcripts interact in the proteome. These graphs are used

to establish connections between -omic entities as well as diseases and symptoms. Among the many available, we would like to mention:

detecting miRNA-disease associations [4], gene-symptom prioritization [9], and protein–protein interaction prediction [10, 11, 12].

This is a very small set of all the studies and research in this area. The miRNA-disease associations use case is very relevant. It shows how to combines multiple structured data sources together in a single big knowledge graph and all the problems that this task brings. Once created, the resulting knowledge graph allows clinicians to predict miRNA-disease connections that can be, then, validated experimentally. This has a tangible impact and an enormous value on this research area. It allows researchers to save money avoiding performing several tests in vitro that could have a high cost due to the amount of possible combinations to test. For this reason, the generation of potential links between miRNA and diseases is generating a lot of interest in the research community. In the appendix C we will discuss in great details the miRNA-disease association detection and prediction applications.

Gene-symptoms prioritization application is a vital task for precision medicine since it allows to refine disease taxonomy and improve the personalized treatments of chronic diseases. There are many experimental studies on candidate genes of diseases, but the identification of symptom genes is rarely addressed. Yang et al. [9] proposed a knowledge graph model and reasoning to obtain candidate genes of given symptoms. The researchers merged many heterogenous data sources. To unify and integrate the disease terms, they mapped the original disease identifiers of the different databases to Unified Medical Language System[3] (UMLS) codes. Figure 4.3 show the process visually.

Figure 4.3 An extract of an image from Yang et al. [9] that shows, in a simplified way, how the researchers combined multiple data sources to obtain a holistic knowledge graph that could serve their purposes.

The protein-protein interactions (PPIs) were collected in the form of a knowledge graph to promote and facilitate the investigations of the molecular mechanisms of symptoms and provide candidate genes for validation in experimental settings.

The examples mentioned above show how combining multiple data sources in a holistic connected knowledge graph empower many meaningful applications. Nevertheless, even in simpler scenarios when the dataset contains a reduced amount of connection types, the power of the knowledge graph is there. This is the case of the protein-protein interaction (PPI) network in combination with protein-disease association network. It is the perfect example to show how representing information in the form of a graph enhance our capability of reasoning on existing information.

Protein-protein interaction networks and protein-disease association have been successfully and largely used in computational discovery of disease pathways [12].

Disease pathways are groups of proteins associated with a given disease. Broadly, a disease pathway in the PPI network is a system of interacting proteins whose atypical activity collectively produces some disease phenotype. A disease pathway is, hence, a subgraph of the PPI network defined by the set of disease-associated proteins. Each disease has multiple of these subgraphs related to it.

Discovering disease pathways is an important problem to solve that has the potential to provide clinically actionable insights for disease diagnosis, prognosis, and treatment. It is a perfect use case for developing an Intelligent Advisor Systems where knowledge graphs have a key role because trying to understand each disease protein in isolation cannot fully explain most human diseases. Many methods have been developed to predict which proteins are associated with a given disease, and to bring them together into pathways using the PPI network. These methods have accelerated the understanding of diseases. At the same time, this is a perfect example of how to construct and analyze a simpler knowledge graph before moving to more complex scenarios that require to merge multiple data sources (with all the issues that

this brings and that we will describe later in this chapter).

As stated before, the goal here is to discover diseases pathways starting from known pathways. The approach proposed by Agrawal et al. [12] starts from a knowledge graph represented in the figure 4.4.

Figure 4.4 Disease pathway is then a subgraph of the PPI network defined by the set of disease- associated proteins.

The diseases are connected to the associated known proteins. These proteins are connected to each other in the PPI network. A group of connected proteins related to the same disease forms a pathway component. Some protein could be isolated, i.e. not connected to any other protein associated to the same disease.

In the examples above there are single pathways for each disease but it could be that there are cases in which each disease has more pathways. Before moving ahead, consider a specific disease and see how it is connected to certain proteins/genes. For example, if we take the celiac disease[4], in the figure 4.5 there are the connections to relevant genes.

Figure 4.5 A small portion of the knowledge graph built by Agrawal [12]. Here we started from the Celiac Disease, and we found all the associated genes. On the top, slightly on the right, we see HLA-DGA1 and HLA-DGB1. These are very well-known genes since they are used to identify people susceptible to celiac disease.

Let’s put our attention now on the overall process to better understand what we are going to do with this simple knowledge graph. Figure 4.6 shows the start point – the seeds – and the result of the discovery process.

Figure 4.6 The discovery process of proteins related to diseases. The result of this task provides a better understanding of the disease and the proteins involved.

We have a set of known pathways, and the Intelligent System Advisor must predict and report to the user a set of potential proteins and related pathways associated to the disease. As we notice from the image, the discovered proteins can be part of an existing pathway or form a new one.

In this part II of the book we focus on the creation and the inspection of the seed knowledge graph, later, in part IV, we consider the algorithms for performing complex types of analysis.

The resulting knowledge graph in this case is relatively simple, it is the composition of a monopartite graph – the PPI network composed by proteins and the relationships among them – and a bipartite graph – the disease- protein association network that connects the disease with the related proteins. This has been represented in Figure 4.4 available before.

Luckily in this case we have already the data sources organized as we need. We can use the Disease Pathways in the Human Interactome available as one of the Stanford Network Analytics Project[5]. Agrawal [12] did an amazing job in creating a simpler network starting from more complex data sources. We just need to import and explore it. After that it is necessary to combine with another dataset to improve the readability of the resulting knowledge graph, since humans – as we are – prefer names of the proteins rather than their code.

The first step consists in creating the necessary constraints in the database, to assure that we have unique proteins and diseases. The following node key constraints ensure that all nodes with a particular label have a set of defined properties whose combined value is unique and all properties in the set are present. In our case the unique property is the id.

Listing 4.1 Create the constraints.

CREATE CONSTRAINT protein_key IF NOT EXISTS FOR (n:Protein) REQUI CREATE CONSTRAINT disease_key IF NOT EXISTS FOR (n:Disease) REQUI

The first file to import is the Human protein-protein interaction (PPI) network. The dataset uses the human PPI network compiled by Menche et al. [13] and Chatr-Aryamontri et al. [14]. The resulting graph contains 342,354 physical interactions experimentally documented in humans among 21,559 proteins. They come from 15 different database, such as metabolic enzyme- coupled interactions and signaling interactions. Proteins are mapped to genes. The following listings, 4.2, 4.3 and 4.4, assume that the files have been downloaded from Stanford Network Analytics Project (SNAP), decompressed and moved to a PPI directory in the import directory in Neo4j.

Listing 4.2 Import PPI Network

:auto LOAD CSV FROM 'file:///PPI/bio-pathways-network.csv' AS lin CALL {

WITH line MERGE (f:Protein {id: trim(line[0])}) MERGE (s:Protein {id: trim(line[1])}) MERGE (f)-[:INTERACTS_WITH]->(s) } IN TRANSACTIONS OF 100 ROWS

Exercise

Run the necessary queries to verify the numbers above in terms of number of distinct proteins and connections among them. Do it before the next step since it adds new proteins.

The second file to import is Protein-disease associations. It is a tuple (u, d) indicating that alteration of protein u is linked to disease d. Protein-disease associations are pulled from DisGeNET[6], a platform that centralized the knowledge on Mendelian and complex diseases. It contains over 21,000 protein-disease associations, which are split among the 519 diseases that each has at least 10 disease proteins. The diseases range greatly in associations. The average value of associations per disease is 21, but some more complex diseases, e.g., cancers, have hundreds of associations.

Listing 4.3 Import pathways

:auto LOAD CSV WITH HEADERS FROM 'file:///PPI/bio-pathways-associ CALL {

WITH line WITH trim(line["Associated Gene IDs"]) AS proteins, trim(line["Disease Name"]) AS diseaseName, trim(line["Disease ID"]) AS diseaseId MERGE (d:Disease {id: diseaseId, name: diseaseName}) WITH d, proteins UNWIND split(proteins, ",") AS protein WITH d, protein MERGE (p:Protein {id: trim(protein)}) MERGE (d)-[:ASSOCIATED_WITH]->(p) } IN TRANSACTIONS OF 100 ROWS

Exercise

As exercise we suggest running some checks to verify the numbers mentioned to describe the file and the numbers before. You should notice a change in the number of proteins. Are you able to find the new proteins? Hint: use the NOT EXISTS clause

The last file available in the SNAP dataset for pathways is the Disease categories. Diseases are subdivided into categories and subcategories using the Disease Ontology[7]. The diseases in the ontology are also mapped to one or more Unified Medical Language System (UMLS) codes, and of the 519 diseases pulled from DisGeNET, 290 have a UMLS code that maps to one of the codes in the ontology. The dataset uses the second-level of the ontology; this level consists of 10 categories, such as cancers (68 diseases), nervous system diseases (44), cardiovascular system diseases (33), and immune system diseases (21).

Listing 4.4 Import disease classes

:auto LOAD CSV WITH HEADERS FROM 'file:///PPI/bio-pathways-diseas CALL {

WITH line WITH line["Disease ID"] as diseaseId, line["Disease Class"] a MATCH (d:Disease {id:diseaseId}) SET d.class = class } IN TRANSACTIONS OF 100 ROWS

Exercise

After the import check once more the numbers for each disease’s classes and the list of diseases without classes.

The three steps above perform the ingestion of the data available in the SNAP dataset for pathways. Unfortunately, this dataset uses codes to identify proteins. To improve readability, we suggest also importing some information from gene information[8] available from NIH[9] (National laboratory of Medicine). This process allows you to convert the code in the name of the gene that codifies the protein. After downloading the file, decompress and move in the same directory PPI as before.

Listing 4.5 Import gene info

:auto LOAD CSV WITH HEADERS FROM 'file:///PPI/gene_info' AS line CALL {

WITH line WITH trim(line["GeneID"]) AS proteinId, trim(line["Symbol"]) A trim(line["description"]) AS description WITH proteinId, symbol, description MATCH (p:Protein {id:proteinId}) SET p.name = symbol, p.description = description } IN TRANSACTIONS OF 100 ROWS

With this last step the import has been completed. The resulting knowledge graph can be used for many different purposes, one of which is the discovery of disease pathways via, for example, link prediction. Nevertheless, the purpose here is not to perform the link prediction since this is one of the advanced analytics topics discussed in the part IV of the book. The chapter goal is, instead, to show concrete use cases of knowledge graph and how to construct them from simple structured data sources. It is useful, after the import has been completed, to inspect the resulting knowledge graph. This is a mandatory step, before performing complex machine learning tasks, to evaluate the quality of the graph and explore the database.

In the case something when wrong during the import or you didn’t perform all the steps above, we made a Neo4j backup available for you[10]. The next listing shows how to easily import this database using the new functionality available in Neo4j 5.x.

Listing 4.6 Create the PPI database from a neo4j backup.

# Add the following line to the neo4j.conf file # dbms.databases.seed_from_uri_providers=URLConnectionSeedProvide # then run the following command

CREATE DATABASE ppi OPTIONS { existingData: "use", seedUri: "http

There are different techniques to accomplish this task. The simplest is to start inspecting some known area of the knowledge graph. In our case, this translates to considering some disease pathways and check if they make sense. Often, this requires having a domain expert involved in the process. Of course, this qualitative exploration cannot cover the entire graph and cannot provide enough useful information for machine learning tasks evaluation. For these reasons, it is necessary to compute more quantitative measures of the

knowledge graphs. Some of these measures are generic and can be used everywhere, others are specific for the goal we have.

Our analysis of the graph starts with a generic evaluation of the PPI network. Our preferred, first algorithm for this type of evaluation is weakly connected component (WCC). It is a community detection algorithm that is used to find disconnected subgraphs within the graph. It is a deterministic algorithm, so it always produces the same result if it runs on the same graph. The results show how many independent subgraphs compose the big knowledge graph.

To run this analysis, we are going to use a graph data science (GDS) library available on Neo4j. Appendix B describes how to install it. The first step before running any GDS algorithm consists in creating an in-memory representation of the graph we are planning to analyze, called projection. This can be done running the query in Listing 4.8. Before doing that, in this case, as mentioned, we are focusing on the sole PPI network, and to do so we are marking the proteins connected via a relationship INTERACTS_WITH. The Listing 4.7 below serves the scope.

Listing 4.7 Creating a temporal label to mark the proteins in the PPI Network

MATCH (p:Protein)-[:INTERACTS_WITH]-() SET p:PPIProtein

After that we can create a proper in-memory projection. That is what the next Listing 4.8 does.

Listing 4.8 Create the graph projection in memory

call gds.graph.project(

'ppi-graph', #A 'PPIProtein', #B { INTERACTS_WITH: { #C orientation: 'UNDIRECTED' } } )

Once the subgraph has been created in-memory we can run the WCC algorithm using the query in Listing 4.9.

Listing 4.9 Run WCC on the PPI Network

CALL gds.wcc.write('ppi-graph', { writeProperty: 'componentId' }) YIELD nodePropertiesWritten, componentCount, componentDistributio

The results of the query above is available in the Table 4.1

Table 4.1 Summary results of the query in Listing 4.9

nodePropertiesWritten componentCount componentDistribution 21559 27 { "p99 [11]": 21521, "min": 1, "max": 21521, "mean": 798.481, "p90": 3, "p50": 1, "p999": 21521, "p95": 4, "p75": 2 }

Looking at the output of our algorithm, we can describe our PPI network. The 21559 proteins in the graphs can be grouped in 27 not overlapping subgraphs. Of those, one is very big with 21521 proteins. This information can be seen from the max value in the third column describing the distribution of the subnetworks. The others subgraphs are very small, either isolated nodes or small islands up to 4 components (the majority are single nodes). If we would like to summarize, this result shows that the proteins in the PPI network are quite connected forming a single subgraph that connects the greatest part of them with short or long paths.

Even though nodes are connected there could be groups of proteins connected to each other more densely than other proteins outside of this “community”. To reveal these communities, we can use another graph clustering method:

the Louvain Modularity algorithm. The Louvain algorithm [15] is one of the fastest modularity-based algorithms and works well with large graphs. It also reveals hierarchies of communities at different scales, which is useful for understanding the global functioning of a network. It works by maximizing the modularity score for each community. In this case, modularity score measures how well groups have been partitioned into communities. The modularity quantifies the quality of an assignment of nodes to communities by evaluating how much more densely connected the nodes within a community are, compared to how connected they would be in a random network. Differently the WCC, this algorithm is not deterministic since the results depend on some random selection of initial state and sequent steps. Regardless of which this can give us relevant information about our knowledge graph.

The next query shows how to run the GDS implementation of Louvain on the same in-memory graph created before. In case you restarted the database, or you didn’t run it before, you must execute the query in listing 4.8 before running the next one.

Listing 4.10 Run Louvain on the PPI Network

CALL gds.louvain.write('ppi-graph', { writeProperty: 'componentLouvainId' }) YIELD communityCount, modularity, modularities, communityDistribu

The results of this algorithm are quite different. They tell a different story.

Table 4.2 Summary results of the query in Listing 4.10

communityCount modularity communityDistribution

48 0.5464241018027929 { "p99": 3533, "min": 1, "max": 3533, "mean": 449.1458333333333, "p90": 1817,

"p50": 3, "p999": 3533, "p95": 2336, "p75": 311 }

The Louvain algorithm finds between 40 and 50 communities (as we said it is not deterministic, we run multiple ways with different results but always in those range). We have some big communities with more than 3 and half thousand proteins (in some runs also 4 thousand) and other smaller. The average size is around 450 proteins per community. The modularity score, which consider all the communities, should be around 0.40 (40%). The next one is a very simple query that allows us to inspect a bit the content of the communities.

Listing 4.11 Inspect the top 10 communities’ content.

MATCH (p:PPIProtein) WITH p.componentLouvainId as communityId, count(p) as members ORDER BY members desc LIMIT 10 MATCH (p:PPIProtein)-[:INTERACTS_WITH]-(o) WHERE p.componentLouvainId = communityId WITH communityId, members, p.name as name, count(o) as connection ORDER BY connections DESC RETURN communityId, members, collect(name)[..20] as keyMembers

The results of the above query show the top 20 most connected elements for each of the community identified by Louvain. In our case we found that in the bigger cluster we have proteins like "APP", "NTRK1", "GRB2", "EGFR", and "HSP90AA1" while in the second "ELAVL1", "MOV10", "NXF1", "VCP", "SHMT2". Doing some research with Google, since we are not expert in the domain, it is easy to find that the above groups make sense.

The above analysis is very straightforward and, thanks to the GDS library, easy to accomplish. By running those algorithms and evaluating the results it is possible to understand the global structure of the network. They are very generic and work in all the cases. We recommend this analysis as a first start to have a “description” of the graph. The WCC provides the number of sub-

graphs in the strict sense of the meaning while Louvain offers a view on the “modules” of the network, i.e., portions of the graphs that are well connected to each other and loosely connected with the rest. This is also relevant to understand.

At the same time this type of analysis doesn’t offer any specific understanding of the knowledge graph. This is a key aspect to consider. The algorithms mentioned before, like WCC or Louvain, but also other generic and well-known algorithms like PageRank, Betweenness centrality and others, treat each node in the same way, ignoring if it is, for example, a protein, a gene, or a disease, and the same is for the relationships. From now on instead we introduce several techniques that can be used to better understand a Knowledge Graph. They are more contextualized to the domain and the goal. As in the case of the algorithms before the measures introduced help to better understand the content of the graph, but in this case, they use the meaning of each node and each relationship. Therefore, the computation of these measures needs to be adjusted considering the type of nodes and type of the relationships you have in the knowledge graph you built. Nevertheless, they are generic enough to have a huge spectrum of applications. In the code associated to the book there is a library where we collected all the measures in a way that makes them easy to be used in your own project or to analyze your own knowledge graphs.

The algorithms, described before, consider the full PPI Network, and analyze it. The results give us an understanding of how the full network is structured. The next measures consider subnetworks: the disease pathways. Biologically, a disease pathway is a system of interacting proteins whose atypical activity collectively produces some disease phenotype. Translated in graph theory, given the PPI network G = (V, E), whose nodes V represent proteins and edges E denote protein-protein interactions, the disease pathway for disease d is an undirected subgraph Hd = (Vd, Ed) of the PPI network specified by the set of proteins Vd that are associated with d, and by the set of protein-protein interactions.

Where (again):

E contains all the edges in the protein-protein network only (so here we are not considering the connections between the diseases and the proteins) Vd represents the set of proteins associated with our target disease. For example, in he case of celiac disease, this list will contains all the proteins (or if you prefer related genes) associated with it.

Using our graph, we can extract this subnetwork via a simple query available in the following listing.

Listing 4.12 Extracting disease pathway from the knowledge graph

MATCH (d:Disease {id:$id})-[:ASSOCIATED_WITH]->(p) WITH collect(p) as proteins UNWIND proteins as m0 UNWIND proteins as m1 OPTIONAL MATCH (m0)-[r:INTERACTS_WITH]->(m1) RETURN DISTINCT m0, r, m1

It is important to notice that this subnetwork is a monopartite network since it doesn’t contain the connection among the proteins and the disease. Moreover, disease pathways related to different diseases can overlap, since the same protein can be involved in many disease processes. This is an interesting aspect compared with the clustering algorithms before where the resulting clusters don’t overlap.

For certain measure we need also to consider how this subnetwork is connected to the rest of the PPI network. For these measures we need to compute the pathway boundary that is defined as

Where Vd\V represents all the nodes that are in the global set V but not in Vd, which translates in all the nodes not associated with the target disease.

It considers, hence, all the edges connecting an internal node (internal to the target disease pathways) to an external protein in both directions.

The measures we are going to consider characterize connectivity of disease proteins – both insider the disease pathways and outside toward the rest of proteins in the PPI Network. Other measures instead consider the distance and the concentration in the pathways. All the measures are calculated for each disease, to characterize each and find relevant patterns. Moreover, they can be collected and statistical information on them can give us a broader overview of the network and how it is distributed over the different disease.

The first measure is the relative size of largest pathway component. This measure computes the fraction of disease proteins that lie in the Hd’s largest connected component. In other words, given the largest set of proteins connected to each other, it gives back the percentage of this proteins over the total number of proteins in the disease pathways. The formula can be summarized like this:

Where nodes(largestCC(Hd)) returns the nodes in the largest weak connected components of Hd. The code for computing this piece leverage network x function and it is available in the next listing. The full code is available in the code repository for this book at chapter/ch04/analysis/multiomic_analysis.py.

Listing 4.13 Size of the largest pathway component

class MultiOmicAnalysis(GraphDBBase): #A

def __init__(self, argv, database): super().__init__(command=__file__, argv=argv) self.__database = database

def load_hd(self, disease): #B query = """ MATCH (d:Disease {id:$id})-[:ASSOCIATED_WITH]->(p) WITH collect(p) as proteins UNWIND proteins as m0 UNWIND proteins as m1 OPTIONAL MATCH (m0)-[r:INTERACTS_WITH]->(m1) return distinct m0, r, m1

""" param = {"id": disease} return self.load_graph_and_get_nx_graph(query, param) def load_graph_and_get_nx_graph(self, query, param={}): #C data = self.get_raw_data(query, param) G = networkx_utility.graph_undirected_from_cypher(data) return G

def get_raw_data(self, query, param): #E with self._driver.session(database = self.__database) as results = session.run(query, param) return results.graph()

def compute_largest_components(self, networkx_graph): #F largest_cc = max(nx.connected_components(networkx_graph), k return largest_cc

if __name__ == '__main__':

analysis = MultiOmicAnalysis(argv=sys.argv[1:], database="ppi") disease_id = 'celiac disease' #G networkx_graph = analysis.load_Hd(disease_id) nodes_count = networkx_graph.nodes.__len__() largest_cc = analysis.compute_largest_components(networkx_graph relative_size_of_largest_cc = float(largest_cc_size.__len__())/nodes_count #H

The second measure is density of the pathway. As the name suggest, it measure how densely connected are the proteins in the disease pathways. The formula is the following:

The denominator computes the number of possible edges and the numerator consider the real edges. This measure takes values in [0; 1]. A higher density indicates a higher fraction of edges (out of all possible edges) appear between nodes in Hd. The next listing shows how to compute it, the rest is in the same code as before (in the code there are some optimization in place that make the full code running faster, in the listing we simplified for improving clarity).

Listing 4.14 Density of the pathway

def compute_density(networkx_graph): #A

nodes_count = networkx_graph.nodes.__len__() edges_count = networkx_graph.edges.__len__() density_pathway = 2.0 * float(edges_count) / (nodes_count * (nodes_count - 1)

if __name__ == '__main__':

analysis = MultiOmicAnalysis(argv=sys.argv[1:], database="ppi") disease_id = 'celiac disease' networkx_graph = analysis.load_hd(disease_id) density_pathway = compute_density(networkx_graph)

As evident from the code above, the density of the pathways is simple to compute. Once the subgraph has been loaded, we have all the information to compute it. At the same time, it offers a very relevant perspective, since it describes how well connected the nodes are inside the subgraph considered. In our biological case the subgraph is a disease pathway, so it highlights how much the participant proteins interacts to each other.

The third measure is conductance[16]. It measures the independence of the disease pathway (or more in general of the subgraph) with the rest of the graph. It uses the edges connecting a node inside the subgraph with a node outside, regardless the direction. It has been defined as Bd. Once you have it the formula is the following:

It takes values in [0; 1]: A lower conductance indicates the pathway is a more well-knit community separated from the rest of the network.

The formula above is converted in the following piece of code.

Listing 4.15 Computing conductance

def compute_bd(self, disease): #A query = """ MATCH (d:Disease {id:$id})-[:ASSOCIATED_WITH]->(p) WITH collect(p) as proteins MATCH (m0)-[r:INTERACTS_WITH]-(m1)

WHERE m0 in proteins and not m1 in proteins RETURN count(DISTINCT r) as bd """ param = {'id': disease} return self.get_data(query, param)["bd"][0]

def get_data(self, query, param={}): #B with self._driver.session(database=self.__database) as sessio results = session.run(query, param) data = pd.DataFrame(results.values(), columns=results.keys( return data

if __name__ == '__main__':

analysis = MultiOmicAnalysis(argv=sys.argv[1:], database="ppi") disease_id = 'celiac disease' networkx_graph = analysis.load_hd(disease_id) bd = analysis.compute_bd(disease_id) edges_count = networkx_graph.edges.__len__() conductance = float(bd) / (bd + 2 * edges_count) #C

There are many other measure and metrics that can be used but those before offer a good characterization of the subgraphs and the entire knowledge graph. It is worth nothing here that all the measure above generally works on monopartite graphs, so starting from a Knowledge Graph that is a multipartite graph with many different types of nodes and relationships, it is necessary to extract multiple monopartite graphs, performing projections and compute the measures for each of the resulting “perspectives” on the big knowledge graph. Following these steps, a full picture of the graph is extracted. In our examples above, we started from a graph with proteins and diseases with protein-to-protein and disease-to-protein connections. We used the second type to extract multiple monopartite subgraphs of protein-to-protein. At this point is hard to generalize rules, but we can summarize the most relevant way to extract portions of the knowledge graph in the following list:

Filtering: Selecting only a subset of nodes and relationships, based on types or other criteria. For example, considering only Protein as node type and “INTERACTS_WITH”. Projection: Applying a graph projection allows to convert a bi-partite graph in two monopartite graphs. It creates a direct relationship among two nodes that share a connection with one or more nodes. For example, we can create a disease projection where two diseases are connected if

they share some proteins. Partitioning/Fragmentation: Splitting the graph in subgraphs of nodes and relationships based on some criteria or randomly. The subgraphs could or not overlap. For example, disease pathways are partitioning of the protein-to-protein network using the disease connection as criteria. Any combination or multiple iteration of the above.

Coming back to our disease-protein example, once we compute measures for each the subgraph –the disease pathways – it is useful to look at them as a whole. A common way to do so it is to perform a frequency analysis. For the measures that assume a decimal value (all our measures have a value between 0 and 1) it is better to split them in buckets. The same would be also for integers when there are too many. Figure 4.7 shows the results.

Figure 4.7 The distribution of the three key measures for the disease’s pathways: Largest connected component, density, conductance.

Analyzing the results in Figure 4.7, we notice that disease pathways are fragmented in the PPI network, with a median of 16 connected components per disease and a median of only 21% of the proteins lying in the largest pathway component (Figure 4.7a). Only approximately 10% of pathways have over 60% of their proteins in the largest pathway component. The

disease pathways are not particularly well connected internally with only a median density of 0.07 (the overall PPI network density is 0.0015), and 90% of diseases have a density below 0.17 (Figure 4.7b). On the other side, they are well connected externally, having a median conductance of 0.96, meaning that the disease pathway has relatively as many edges pointing outside the pathway to the rest of the PPI network as it has edges lying inside the pathway.

These overlapping subgraphs obtained by considering the disease pathways are very different of what you would obtain for example clusterizing using WCC or Louvain. These algorithms split the graph in almost independent subgraphs. In order to verify this theory let’s run the same measures on the clusters we computed using Louvain by running the query in Listing 4.10. The results are showed in the next figure 4.8.

Figure 4.8 The distribution of the three key measures for the clusters obtained via Louvain algorithm: Largest connected components, density and conductance.

The results now are completely different than before, these clusters tell us a totally different story. As expected, most of the proteins in a Louvain cluster reside in one big, connected component (Figure 4.8a), that’s why the highest frequency is around 1. The density is not so connected with the clustering

algorithm, but more with the connectivity of the network in general. So, the changes here are only related to the different structure of the clusters, but not a significant change here. The conductance is improved, since it moved closer to 0 which means that the clusters are better connected inside than outside.

These considerations and measure are very useful as preliminary analysis before disease protein pathways discovery. Let us iterate once more that, this type of analysis must be executed before running more complex tasks on the network to understand the characteristic of the network and, hence, the results of the machine learning.

The next section will introduce another application and new algorithms to extract information out of knowledge graphs, in this case considering edges and paths more than single nodes.

4.3 Pharmaceutical applications

This is, most probably, one of our favorite use cases related to knowledge graphs. The COVID-19 disease is still vivid in our mind: the facial masks, the restrictions and all the other consequences that a pandemic disease brought. Since the beginning of the pandemic disease a lot of researchers focused on the reuse existing drugs to fight the new disease. The reasons for such an effort are because the production of new drugs requires an enormous amount of time for discovery, testing, validation, and approval of the new compounds before they can be used for treatments. This is true not only for new diseases but for any disease. Pharmaceutical companies spend a huge amount of money and time on those steps. The cost of developing a new therapeutic drug has been estimated at 1.4 billion dollars [17], the process typically takes 15 years from lead compound to market [18], and the likelihood of success is stunningly low [19].

A deeper drug analysis and, possibly, repurposing can drastically reduce the duration, failure rates, and costs of approval. The analysis leverages the rich preexisting information on approved drugs such as extensive toxicology profiling, preclinical models, clinical trials, and post release surveillance.

The ability to computationally predict whether a compound treats a disease would improve the economy and the success rate of drug approval. Therefore, the research around new and old compounds occupy an important role in the biomedicine. More interestingly for us and for the purpose of this book is that there are a multitude of examples where knowledge graphs have been successfully applied to identify new properties of drugs. Knowledge graphs have been successfully used for predicting drugs interacting with other drugs [20], identifying molecular targets a drug might interact with [21] and, of course, identifying new disease treatments for previously established drugs [22].

Like multi-omic applications, several approaches leveraged knowledge graphs to infer novel links between drugs and diseases. Dai et al. used recommendation systems, and in particular collaborative filtering[12], to infer drug-disease associations [21]. The authors assumed that drugs associated with a disease interact with the same genes of interest. Driven by this assumption, they constructed a drug-disease network by integrating two bipartite networks: a drug-gene interaction network and a disease-gene interaction network. Other researchers used these techniques to infer drug- target interactions [23,24] and drug-disease treatments [25,26]. Despite reported success, these approaches are limited to the drugs and diseases contained in the graph. Enriching the knowledge graph, by combining these approaches with representations of chemical structures, biological processes, and other relevant knowledge might make it possible to make predictions about novel compounds.

For the reason just mentioned, among all these approaches and examples, the one we would like to discuss deeply is the work done by Himmelstein and others [2]. This team of researchers constructed a graph encoding knowledge coming from 29 public resources to connect compounds, diseases, genes, anatomies, pathways, biological processes, molecular function, cellular components, pharmacological classes, side-effect, and symptoms. They called it hetionet. The name derives from “hetnets”, short for heterogeneous networks – the name knowledge graph was less used few years ago. It has been created by integrating knowledge and experimental findings from decades of biomedical research spanning millions of publications.

Another great advantage of this research is that it has been completely open, and the graph database not only is publicly available[13], but it is also available in a neo4j format. For this application, hence, we don’t focus on the creation of the database from multiple sources since the authors did all the work for us. Instead, we discuss the importance of having a properly designed knowledge graph that combines multiple sources in a coherent schema and the way in which the resulting knowledge graph can be analyzed. This has a crucial aspect in the post-creation evaluation of a knowledge graph, where the completeness of the information power must be evaluated before it is used for the purposes for which the graph has been carved.

In this case, the first step consists in importing of the database in neo4j. One of the options is to import the JSON files or the CSV files available on the original site, but it takes long time. A neo4j backup is also available but because this database has been created in 2017 the backup is on older version of Neo4j, so we made it available also in a more updated version[14]. The next listing shows how to easily import this database using the new functionality available in Neo4j 5.x.

Listing 4.16 Create the Het.io database.

# Add the following line to the neo4j.conf file # dbms.databases.seed_from_uri_providers=URLConnectionSeedProvide # then run the following command

CREATE DATABASE hetionet OPTIONS { existingData: "use", seedUri:

The imported knowledge graph consists of 47,031 nodes of 11 types and 2,250,197 relationships of 24 types. The nodes consist of 1552 small molecule compounds and 137 complex diseases, as well as genes, anatomies, pathways, biological processes, molecular functions, cellular components, perturbations, pharmacologic classes, drug side effects, and disease symptoms. The edges represent relationships between these nodes and encompass the collective knowledge produced by millions of studies over the last half century [2]. Figure 4.9 describes the full schema of the imported dataset.

Figure 4.9 Het.io knowledge graph schema. The details of the nodes and relationships are available in the source link[15].

For example, Compound–binds–Gene edges represent when a compound binds to a protein encoded by a gene. This information has been extracted from the literature by human curators and compiled into databases such as DrugBank[16], ChEMBL[17], DrugCentral[18], and BindingDB[19]. Hetionet combines these datasets in 11,571 edges and for each the reference

is stored ad relationship attribute.

Exercise

Explore the graph imported and check how the nodes are distributed across the different types of nodes. Once done, do the same for the relationships. Be careful since the relationships can be trickier to explore. Refer to the schema to make more accurate queries.

Hetionet has been created specifically for drug repurposing. Nevertheless, the resulting knowledge graph has broader biological applicability. This shows, once more, how much knowledge graphs can be powerful in representing human knowledge and feed many different use cases. A few of these tasks are presented later since they require easy-to-write queries and take a few seconds to be executed. Other applications, including drug repurposing, require more complex feature extraction and more advanced machine learning tools that will be introduced in the part 4 of this book. The goal in this chapter is to present how to build and analyze knowledge graphs. Although many of the functionalities we are going to explore using Hetionet can be provided by multiple services, the proposed approach leverages a single knowledge graph to answer to all the questions in few simple and performant steps.

During the path exploration examples that follows a new metric is used to order by relevance. It is called degree-weighted path count (DWPC). It has been introduced by Himmelstein [27] adapting an existing method originally developed for social network analysis called PathPredict [28]. DWPC is used for quantifying prevalence of metapaths in the Hetionet graph, but, as you’ll notice, it has very broad applicability.

First of all, it is important to clearly define the concept of metapath. Figure 4.10 helps understanding what it means.

Figure 4.10 Metagraph (A) and metapapths (B) excerpts for Hetionet database. Metagraph describes the structure of the database indicating the type of nodes and the type of relationships. Metapaths describe paths indicating the type of nodes and the type of relationships inside interested paths.

As we have seen so far, each knowledge graph is constructed following a schema, or a graph data model. Such schema is composed by node types and edge types. As we see in the figure 4.10A, the schema is represented using nodes and relationships. The difference is that instead of representing real objects and relationship instances, it represents classes of objects and types of relationships. Starting from a schema we can define a metapath a sequence of nodes and relationships that describe potential real paths between a node belonging to the first type and a node belonging to the last type. For example, a metapath can be like (Gene)-[]-(Tissue)-[]-(Disease) or can be more specific, also defining the type and the direction of relationships, like (Gene)- [EXPRESSED_IN]->(Tissue)<-[LOCALIZED_IN]-(Disease). A metapath, hence, can be described in a very precise or a loose way. Moreover, starting from this definition we can literally “query” the schema searching for patterns of connection among source type and destination type. For example, we can produce a list of possible metapaths for a generic pattern like (Gene) —a—(Disease) of max length 4. In such case we would have as result what is listed in Figure 4.10B.

It is important to note here that we are talking about description of paths not real instances of paths. The concrete paths are, instead, showed in the next Figure 4.11.

Figure 4.11 Example of how to extract paths based on a defined metapaths and how to compute the PDP and DWPC. (A) is a sample graph, (B) shows how the computation is working.

On the left (Figure 4.11A) we have a relevant portion of the knowledge graph where a specific gene, IRF1, relates to a specific disease, Multiple Sclerosis.

All the paths belong to one of the metapaths described as potential metapaths for the generic patter (Gene)—a—(Disease).

On the right side (Figure 4.11B), we grouped the paths related to each specific metapath. The first group has an intermediate node of type Tissue. In this group only one path belongs to. The second group has another gene as intermediate node. In this group the graph has 3 paths.

Now that we exploded the entire example knowledge graph, we can start the computation of the DWPC metric. At the same time, we can also consider other metrics. Some of them are part of the computation of DWPC, others are mentioned to show their limitation and understand what the spectrum of possibilities is.

The simplest metapath-based metric is path count (PC). It can be easily defined as the number of paths, of a specified metapath, between the defined source and target nodes. If they are frequent or not doesn’t change the value. In the graph of figure 4.11 for the metapath (Gene)-[]-(Tissue)-(Disease) there is only one path between IRF1 and Multiple Sclerosis, so the PC is 1. In the same graph for the metapath (Gene)-[]-(Gene)-[]-(Disease) there are 3 paths between IRF1 and Multiple Sclerosis, so PC is 3.

As you can imagine, PC does not adjust for the extent of graph connectivity along the path. Nodes with high degrees will be considered many times in different paths so they will have a high influence on PC even though they are generally irrelevant. In the case considered here for example they are biologically broad or vague entities with little informativeness.

A few approaches tried to “normalize” the PC count considering some aspects like counting the number of alternative paths starting from the source and reaching the destination, but they never provided insightful value.

DWPC individually downweights each path between a source and target node. Each path has a value associated to it (in path count each path counts for 1) called path-degree product (PDP). It is calculated using the following formula:

Based on the formula above, PDP is calculated by:

1. extracting all metaedge-specific degrees along the path (Dpath), where

each edge composing the path contributes two degrees. In the example in the figure the edge between IRF1 and IL2RA bring 2 values: 4 and 1. IRF1 has 4 outgoing edges of type interaction, while IL2RA has one incoming edge of type interaction. The edge between IRF1 and CXCR4 brings 4 and 2, because CXCR4 has 2 incoming edges of type interaction. 2. raising each degree to the −w power, where w ≥ 0 and is called the

damping exponent. 3. multiplying all exponentiated degrees to yield the PDP.

Now, let’s consider the last path of the figure 4.11, the one (IRF1)-[]- (CXCR4)-[]-(MS). Supposing that w = 0.5 The computation will look like:

For the other paths the image contains the values for the other paths. The DWPC equals the sum of PDPs for the specific metapath considered. The formula will look like the following:

This discussion is very helpful not only because it introduces the difference between metapaths and actual paths but also introduces metrics for evaluating path prevalence trying to ignore “well-known nodes” this is a very common issue that you can encounter during knowledge graph analysis. It must become one of your patterns of thinking as we did for WCC, largest connected components, projections, and other techniques you’ll learn in this

book and in the previous book graph-powered machine learning.

At this stage we have all the ingredients to perform a deep analysis of the Hetionet graph (without using advanced techniques that will be explained in the part IV). The analysis will be accomplished using cypher queries, leveraging, wherever possible, the DWPC metric introduced before. The following analysis has been inspired to the think lab project started by Himmelstein[20].

Here we'll show a query for identifying prominent GO Processes in a set of disease-associated genes. We'll use again celiac disease as an example disease. The celiac disease (CD) is a common (prevalence 1:100) chronic immune-mediated enteropathy caused by intolerance to ingest gluten that develops in genetically predisposed individuals.

First, we can see the 48 genes associated with celiac disease by querying MATCH p = (:Disease {name: 'celiac disease'})- [rel:ASSOCIATES_DaG]-() RETURN p.

Starting from the query above, we'll compute the DWPC (degree-weighted path count) between celiac disease and each GO Process for which at least 2 of the celiac-related genes participate. The results are further restricted to processes with at least 5 participating genes. Here's the Cypher:

Listing 4.17 GO Process enrichment for celiac disease.

MATCH path = (n0:Disease)-[:ASSOCIATES_DaG]-(n1)-[:PARTICIPATES_G WHERE n0.name = 'celiac disease' #B WITH [

size([(n0)-[:ASSOCIATES_DaG]-() | n0]), size([()-[:ASSOCIATES_DaG]-(n1) | n1]), size([(n1)-[:PARTICIPATES_GpBP]-() | n1]), size([()-[:PARTICIPATES_GpBP]-(n2) | n2]) ] #C AS degrees, path, n2 WITH

n2.identifier AS go_id, #D n2.name AS go_name, #D count(path) AS PC, #E sum(reduce(pdp = 1.0, d in degrees| pdp * d ^ -0.4)) AS DWPC,

size([(n2)-[:PARTICIPATES_GpBP]-() | n2]) AS n_genes #G WHERE n_genes >= 5 AND PC >= 2 #H RETURN

go_id, go_name, PC, DWPC, n_genes ORDER BY DWPC DESC LIMIT 10

The query took few milliseconds (91 to be precise) to be executed and performed 34,992 database hits. In the query w has been set to 0.4 since in some studies[21] it appears to be the best value. We tested other close values, and the results list doesn’t change that much so we took the best from the authors. It is used here and other queries.

It returned the following top 10 GO Processes:

Table 4.3 Result of the query in Listing 4.17

GO Id GO Name PC DWP GO:0031295 T cell costimulation 10 0.03 GO:0031294 lymphocyte costimulation 10 0.03 GO:0002507 tolerance induction 3 0.03 GO:0050870 positive regulation of T cell activation 14 0.02 GO:0034112 positive regulation of homotypic cell-cell adhesion 14 0.02 GO:1903039 positive regulation of leukocyte cell-cell adhesion 14 0.02 GO:0051249 regulation of lymphocyte activation 18 0.02 GO:0002684 positive regulation of immune system process 21 0.02 GO:0022409 positive regulation of cell-cell adhesion 14 0.02 GO:0050863 regulation of T cell activation 16 0.02

Let’s have a quick look at the result to explain better the value of DWPC as measure compared with PC. The GO[22] Process with ID “GO:0002684” has a high value of PC and it is connected to 880 genes. Ordering via PC this process would be on top. However, it is involved in many other processes not only in celiac disease. In the gene ontology (GO) nomenclature, we can find its definition that is: “Any process that activates or increases the frequency, rate, or extent of an immune system process”. Clearly it is not relevant for our goals. Thanks to the DWPC-based order, instead, it is in the lower part of our results list on top of which there is “GO:0031295”, T cell costimulation.

This is instead very relevant. With a quick search on internet, it is possible to find that “blockage of T-cell costimulation inhibits T-cell action in celiac disease”[23]. Apart from this top one all the top 7 are very relevant for the disease we are investigating.

As second iteration, let’s refine our search by considering a more complex paths including protein interaction relationships. In this way we are analyzing a different metapath that has a different biological meaning. The query in the following listing performs a more advanced GO Process enrichment analysis for celiac disease. First of all, the query takes only the associations between disease and gene coming from genome-wide association studies (GWAS) [29, 30], which have the advantage of being less biased by existing knowledge. Second, the protein interaction has been added to the metapath to identify genes in the neighborhood of the interactome for the celiac disease. Furthermore, the query considers only the genes that are upregulated in celiac-affected tissues (anatomy).

Listing 4.18 Tissue-specific interactomics. GO Process enrichment for celiac disease

MATCH path = (n0:Disease)-[e1:ASSOCIATES_DaG]-(n1)-[:INTERACTS_Gi WHERE n0.name = 'celiac disease'

AND 'GWAS Catalog' in e1.sources AND exists((n0)-[:LOCALIZES_DlA]-()-[:UPREGULATES_AuG]-(n2)) WITH [

size([(n0)-[:ASSOCIATES_DaG]-() | n0]), size([()-[:ASSOCIATES_DaG]-(n1) | n1]), size([(n1)-[:INTERACTS_GiG]-() | n1]), size([()-[:INTERACTS_GiG]-(n2) | n2]), size([(n2)-[:PARTICIPATES_GpBP]-() | n2]), size([()-[:PARTICIPATES_GpBP]-(n3) | n3]) ] AS degrees, path, n3 as target WITH

target.identifier AS go_id, target.name AS go_name, count(path) AS PC, sum(reduce(pdp = 1.0, d in degrees| pdp * d ^ -0.4)) AS DWPC, size([(target)-[:PARTICIPATES_GpBP]-() | target]) AS n_genes WHERE 5 <= n_genes <= 100 AND PC >= 2 RETURN

go_id, go_name, PC, DWPC, n_genes ORDER BY DWPC DESC

LIMIT 10

The query took under 300 milliseconds and required 358,201 database hits. The results are presented in the following table.

Table 4.4 Result of the query in Listing 4.18

GO Id GO Name PC GO:0031295 T cell costimulation 10 GO:0031294 lymphocyte costimulation 10 GO:0010560 positive regulation of glycoprotein biosynthetic process 6 GO:0033689 negative regulation of osteoblast proliferation 4 GO:1903020 positive regulation of glycoprotein metabolic process 6 GO:0006573 valine metabolic process 5 GO:0070884 regulation of calcineurin-NFAT signaling cascade 2 GO:0010559 regulation of glycoprotein biosynthetic process 7 GO:1903018 regulation of glycoprotein metabolic process 7 GO:0070098 chemokine-mediated signaling pathway 9

The results look much better since they are even more specific than before. The first two are the same but adding the protein interaction helped to return other disease specific aspects not captured by the query before. We can notice a strong dominance of processes related to the glycoprotein. This is very specific since the gluten is a glycoprotein and celiac disease is connected to intolerance to ingested gluten. Another interesting result is related to the 4th position, the negative regulation of osteoblast proliferation. Celiac disease is often connected to osteoporosis[24].

If we're interested more in the “positive regulation of glycoprotein biosynthetic process” relationship, we can retrieve the paths behind the DWPC:

Listing 4.19 Paths behind the DWPC for “positive regulation of glycoprotein biosynthetic process” relationship

MATCH path = (n0:Disease)-[e1:ASSOCIATES_DaG]-(n1)-[:INTERACTS_Gi WHERE n0.name = 'celiac disease'

AND n3.name = 'positive regulation of glycoprotein biosynthetic

AND 'GWAS Catalog' in e1.sources AND exists((n0)-[:LOCALIZES_DlA]-()-[:UPREGULATES_AuG]-(n2)) RETURN path

The result will look like Figure 4.12.

Figure 4.12 Result of the query in Listing 4.19 showing the paths between celiac disease and the positive regulation of biosynthetic process.

This is just an example of the many paths composing the DWPC and the results in the table.

Exercise

Changing few things in the queries above it is possible to perform the same analysis on different diseases. We would recommend running them and do some research to evaluate autonomously how much this knowledge graph can capture a lot of knowledge. Moreover, in the think-lab there are many other interesting queries to explore.

The Hetionet case and the analysis above show key aspects of knowledge graph and its construction. On one side it has been proven how much can be extracted from a well-constructed knowledge graph that has been built for specific purposes and can serve “corollary cases”. This proves the capability of KGs to capture knowledge that is easy to be navigated and used for further purposes. On the other side, the analysis also represents a way to make a qualitative evaluation of the information stored. After the queries we validated the results by searching for evidence that what we have found makes sense. The approach systematically performs a validation assuring that the next steps are constructed on solid basis, making the full process more understandable and explainable.

4.3.1 LLM-Assisted Interpretation of Pathway Analysis Results

While the DWPC-based queries provide quantitative rankings of biological processes, translating these results into clinically actionable insights requires domain expertise and contextual understanding. Large Language Models can serve as intelligent interpreters, helping to synthesize complex pathway analysis results into coherent biological narratives and clinical recommendations.

Let's demonstrate this using the GO Process enrichment results from our celiac disease analysis. The query returned processes like "T cell costimulation," "tolerance induction," and "positive regulation of glycoprotein biosynthetic process" with varying DWPC scores. An LLM can help interpret these findings in their broader biological context.

Listing 4.20 Example LLM Analysis Prompt

You are a biomedical research assistant analyzing gene ontology p

QUERY RESULTS: - T cell costimulation (DWPC: 0.03347, PC: 10, Genes: 75) - lymphocyte costimulation (DWPC: 0.03329, PC: 10, Genes: 76) - tolerance induction (DWPC: 0.03276, PC: 3, Genes: 12) - positive regulation of T cell activation (DWPC: 0.02925, PC: 14 - positive regulation of glycoprotein biosynthetic process (DWPC:

CONTEXT: These pathways were identified through DWPC analysis of

ANALYSIS REQUEST: 1. Interpret the biological significance of these top-ranked pathw 2. Explain the relationship between these processes and celiac di 3. Identify potential therapeutic implications 4. Highlight any unexpected findings that warrant further investi 5. Suggest follow-up research questions based on these results

The LLM would synthesize these results into a comprehensive analysis explaining how T cell co-stimulation relates to celiac disease's autoimmune nature, why tolerance induction appears disrupted, and how glycoprotein regulation connects to gluten intolerance mechanisms. This interpretation transforms raw analytical results into actionable biological insights. The first part of the results obtained by using Claude.ai Sonnet 4.0[25] are presented in the next sidebar.

Biological Significance of Top-Ranked Pathways

T Cell and Lymphocyte Costimulation Pathways (DWPC: 0.03347 and 0.03329)

These pathways represent the core immunological dysfunction in celiac disease. The high DWPC scores and substantial gene counts (75-76 genes) indicate that dysregulated costimulatory signaling is central to disease pathogenesis. In celiac disease, antigen-presenting cells present gliadin peptides to T cells through HLA-DQ2/DQ8 molecules, and aberrant costimulation leads to inappropriate T cell activation rather than tolerance.

Tolerance Induction (DWPC: 0.03276) Despite having fewer genes (12),

this pathway shows a remarkably high DWPC score, suggesting it's highly interconnected within the celiac disease network. This likely reflects the fundamental breakdown of oral tolerance to dietary antigens - a hallmark of celiac disease pathogenesis.

Positive Regulation of T Cell Activation (DWPC: 0.02925) With 201 genes involved, this represents the broadest immunological dysregulation. The sustained T cell activation drives the chronic inflammatory response that characterizes celiac disease, leading to villous atrophy and clinical symptoms.

Relationship to Celiac Disease Pathogenesis

1. These pathways collectively illustrate the immunological cascade in

celiac disease: 2. Initial Trigger: Gliadin peptides are presented via HLA-DQ2/DQ8 to

CD4+ T cells 3. Costimulation Failure: Normal tolerogenic signals are replaced by

inflammatory costimulation 4. Tolerance Breakdown: Loss of regulatory mechanisms that normally

prevent immune responses to food antigens 5. Sustained Activation: Chronic T cell activation perpetuates

inflammation and tissue damage

The prominence of costimulatory pathways suggests that celiac disease isn't simply about antigen recognition, but about the contextual signals that determine whether the immune response is tolerogenic or inflammatory.

This approach demonstrates how LLMs complement knowledge graph analytics by providing the contextual interpretation that transforms quantitative results into qualitative understanding, bridging the gap between computational analysis and applications.

4.4 Clinical applications

Compared with the previous applications, clinical applications are still at early stages in the use of knowledge graphs. In this case, the long-term goal is to use and analyze the knowledge graph to aid with patient care, mainly

through so-called precision medicine. The precision medicine initiative is a long-term research endeavor, involving multiple research centers, which aims to understand how a person's genetics, environment, and lifestyle can help determine the best approach to prevent or treat diseases. Implementing precision medicine requires the integration of omics data – such as proteomics, genomics, transcriptomics – into the clinical decision-making process, which involves patient data – such as electronic health record (EHR) among the others (in some cases another term is used to identify very similar type of data: Electronic Medical Record, EMR). The quantity and diversity of biomedical data, the spread of clinically relevant knowledge across multiple biomedical databases and publications, and the privacy concern connected with EHR, pose a challenge to data integration [31].

The most relevant criticality is related to the gathering and management of EHR data. According to the Official Website of “The Office of the National Coordinator for Health Information Technology (ONC)” in US[26]:

An electronic health record (EHR) is a digital version of a patient’s paper chart. EHRs are real-time, patient-centered records that make information available instantly and securely to authorized users. While an EHR does contain the medical and treatment histories of patients, an EHR system is built to go beyond standard clinical data collected in a provider’s office and can be inclusive of a broader view of a patient’s care. EHRs are a vital part of health IT and can:

Contain a patient’s medical history, diagnoses, medications, treatment plans, immunization dates, allergies, radiology images, and laboratory and test results; Allow access to evidence-based tools that providers can use to make decisions about a patient’s care; Automate and streamline provider workflow.

EHRs are built to share information with other health care providers and organizations – such as laboratories, specialists, medical imaging facilities, pharmacies, emergency facilities, and school and workplace clinics – so they contain information from all clinicians involved in a patient’s care. It is easy to understand how much sensitive this information is in regards of patient’s

privacy, especially because it is related to their health condition. There are many concerns about the way in which these records can be used for different application without sharing personal details of the patients. That is one of the reasons why clinical applications that use knowledge graphs are in early stages of development.

The privacy concern is only one of the issues in integrating EHR data. Among others, it is also difficult to interpret because the text of physician and nursing notes is less formal than that of traditional textbooks, making it difficult to consistently identify disease and symptom mentions. If textbooks and journals present simplified cases that relay only the most typical symptoms, to promote learning and dissemination, EHR data presents real patients with all the comorbidities, confounding factors, and nuances that make them individuals. Finally, there is a lot of subjectivity. The way observations are recorded in the EHR is filtered through the decision-making process of the treating physician. Information deemed irrelevant may be omitted or not pursued, leading to missing information [32]

Despite EHR data being more difficult to work with for the reasons described above, it has the advantage of being closer to the actual practice of medicine than the idealized and curated information presented in textbooks, journals, and other datasets, many of which have been described so far. Typically, knowledge graphs used in clinical application merge EHR with multi-omics datasets, multiple ontologies, and many of the other data sources we explored so far. The key elements are nodes representing patients, drugs, and diseases, while edges encode relationships such as a patient being treated with certain drugs or patient being diagnosed with a disease. There is a clear advantage in leveraging EHRs for clinical applications. In the multi-omics and pharmaceutical cases, for example, a relationship TREATS connects a compound with a disease indicating a generic guideline about treatments of a specific disease. The same disease can be treated using different compounds and all will be represented in the knowledge graph. In a clinical knowledge graph where the EHRs have been imported, it is available the information about how patients have been treated – literally the drug the physician prescribed to healing the patient – and the consequent result.

Another example that leverages EHR and uses graph as representation pattern

is the patient journey mapping on graph. The patient journey mapping, also called patient experience map, is “a map of the steps a client takes as he or she progress (sic) through different stages of a disease, often capturing diagnosis and management and interactions with health professionals” [33]. It is a rapidly growing approach for better understanding how people enter, experience, and exit health services. This journey is often compared with the related clinical pathways [34] that, instead, establish the standard of care for a patient’s clinical presentation with a specific disease and are often linked to a companion patient journey map.

In a very interesting article[27], our friend David Hughes focuses on how patient journey maps can surface meaningful clinical insights by modeling patient interactions with a healthcare system in a clinical knowledge graph, like the one presented later. While clinical knowledge graphs can be developed to explore domains like target protein interaction exploration, new drug development, clinical trial administration, and other research and healthcare use cases, they can also explore a patient’s care with a healthcare system, their disease progression, and treatments. It also drives insights into improving the Triple Aim of healthcare- the improvement of patient experience, raising the health of populations and reducing health care cost.

Figure 4.13 gives you an idea of how big a single patient journey looks like for complex diseases like cancer.

Figure 4.13 A single patient’s clinical oncology journey (courtesy of David Hughes)

These types of effort are pushing the community to find solutions to the privacy and security concerns. The benefits that can derive from clinical knowledge graph extended with EHR are so huge that justify all the effort in that direction.

A way to partially solve the privacy issue above is the deindentification of the EHR data, but they are so detailed that often it is easy to identify a patient even if his/her name is obfuscated. Moreover, the value of such precision medicine emerges when the treatments are customized to the specific characteristics and needs of the patient, they cannot be “generic” by

definition. Other approaches for building knowledge graphs including EHR, leverage a huge set of patient data to extract statistical information about diseases and symptoms or results of treatments [32]. These statistical relationships with related weights are stored in the knowledge graph, but not the real patient data. This increases the guarantee of the privacy, decreasing the opportunity to use them for dealing with specific cases [35].

Another possible approach consists in building deintentified and generic clinical knowledge graph based on fully non sensitive data, anonymized data, experimental results, statistician information coming from real data. The specific patient EHR data are used only “on demand” when they are strictly necessary and use just for those purposes, without storing in the knowledge graph permanently and after the specific consensus of the patient to the doctor in that specific moment.

In the set of research and applications using Knowledge Graphs as core of clinical applications we would like to mention Clinical Knowledge Graph (CKG) effort from Albertos Santos et al [31]. It is an entire platform with a knowledge graph at its core. CKG harmonizes and integrates data as well as user-supplied analysis. It also facilitates data sharing and visualization as well as interpretation based on detailed statistical reports annotated with biomedical knowledge, generating clinically relevant results.

Figure 4.14 The schema for the Clinical Knowledge Graph (from: https://www.nature.com/articles/s41587-021-01145-6)

CKG connects 33 different node labels with 51 different relationship types. It enables queries about experimentally determined protein hits, regarding their association to the diseases studied (ontological associations), drugs or annotated gene ontology (GO) terms and pathways. These types of queries could result in insights into altered functions, suggest drugs for regulated proteins and even connections to metabolites and diet (food) to reveal possible confounding factors.

Another interesting aspect, for us, is that it is already available in neo4j format. We took it[28] and we upgraded to the latest version available while we are writing this book. The provided dump[29] makes it easy and straightforward to import on your own machine. The following code shows how to import it in Neo4j 5.

Listing 4.21 Importing CKG database

# Add the following line to the neo4j.conf file # dbms.databases.seed_from_uri_providers=URLConnectionSeedProvide # then run the following

CREATE DATABASE ckg OPTIONS { existingData: "use", seedUri: " htt

Now that you have a brand-new database, let’s run just a couple of queries to show how simple is to extract valuable insights from such clinical knowledge graphs. Many of the examples below can be applied also to other similar knowledge graphs.

Suppose that your current clinical study is targeting a set of proteins and you are planning to perform a clinical trial. Suppose also that a portion the patients eligible for the trial suffers from cardiomyopathy-related diseases. While the incidence of these cardiomyopathies can be explained either by chance or by unknown factors, you definitively want to check whereas know associations do exist between your proteins and any cardiomyopathy-related disease.

Listing 4.22 Known protein to disease association

WITH

['A1BG~P04217','A2M~P01023','ACACB~O00763', 'ACTC1~P68032','ADIPOQ~Q15848','AGT~P01019', 'AIFM2~Q9BRQ8','APOA2~V9GYM3'] as proteins, #A 3 as minScore, #B "DOID:0050700" as parentDisease #C MATCH (protein:Protein)-[r]-(disease:Disease) #D WHERE (

(protein.name+"~"+protein.id) IN proteins) AND toFloat(r.score)> minScore AND ((disease)-[:HAS_PARENT*0..]->(:Disease {id: parentDisease})) RETURN

(protein.name+"~"+protein.id) AS node1, disease.name+" <"+disease.id+">" AS node2, r.score AS weight, type(r) AS type, r.source AS source ORDER BY weight DESC

Table 4.5 Protein to disease association results from query in Listing 4.22

node1 node2 "ACTC1~P68032" "intrinsic cardiomyopathy <DOID:0060036>" "ACTC1~P68032" "left ventricular noncompaction <DOID:0060480>" "ACTC1~P68032" "familial hypertrophic cardiomyopathy <DOID:0080326 "ACTC1~P68032" "hypertrophic cardiomyopathy <DOID:11984>" "ACTC1~P68032" "dilated cardiomyopathy <DOID:12930>" "ACTC1~P68032" "restrictive cardiomyopathy <DOID:397>"

The results show clearly that we have a specific protein in this target list that is quite strongly connected to several types of intrinsic cardiomyopathies. This finding provides a very specific starting point for possible further investigations.

As we have seen, the different components of the CKG allow individual research groups to analyze, integrate and build a database of their proteomics and other omics projects. Furthermore, the open nature and free availability of CKG also lend themselves to aggregate data and knowledge in what we term a community graph.

4.4.1 LLM-Guided Clinical Decision Support Analysis

Clinical knowledge graphs contain complex relationships between patients, treatments, and outcomes that require careful interpretation to extract actionable insights. Even in this case, LLMs can help clinicians and researchers synthesize multi-dimensional clinical data into coherent treatment recommendations and research directions.

Using our Clinical Knowledge Graph analysis where we identified strong associations between ACTC1 protein and various cardiomyopathies, an LLM can provide clinical context and decision support:

Listing 4.23 Example LLM Clinical Analysis Prompt

You are a clinical informatics specialist analyzing protein-disea

CLINICAL SCENARIO: Research study targeting proteins in patients w

KNOWLEDGE GRAPH FINDINGS: - ACTC1~P68032 shows strong associations (score: 5.0) with:

* Intrinsic cardiomyopathy (DOID:0060036) * Left ventricular noncompaction (DOID:0060480) * Familial hypertrophic cardiomyopathy (DOID:0080326) * Hypertrophic cardiomyopathy (DOID:11984) * Dilated cardiomyopathy (DOID:12930) * Restrictive cardiomyopathy (DOID:397)

TARGET PROTEIN LIST: ['A1BG~P04217','A2M~P01023','ACACB~O00763','A

CLINICAL REQUEST: 1. Interpret the clinical significance of ACTC1's broad cardiomyo 2. What does this suggest about patient stratification for clinic 3. Identify potential safety considerations for the research stud 4. Recommend additional screening or monitoring protocols 5. Suggest companion biomarkers or genetic testing approaches 6. Propose modifications to inclusion/exclusion criteria based on

This LLM-assisted analysis helps translate computational discoveries from knowledge graphs into practical clinical decision-making, ensuring that graph-derived insights effectively inform patient care and research protocols. The results provided are too long to be proposed here, please take the listing above and use it on one of your favorite LLMs tool as exercise.

It is worth noting here that, with this example of LLM-Guided Analysis, we

are not proposing to use the results from LLMs without proper interpretation of physicians and researchers. The purpose here is to show how LLMs can convert complex data extracted from large knowledge graphs into interpretable insights. Remember that our goal is to empower humans via Intelligent Advisor systems not to replace them.

4.5 Summary

Knowledge graphs built from structured data sources require systematic integration of heterogeneous datasets, as demonstrated through biomedical examples involving diseases, proteins, drugs, and clinical data. The process of merging multiple data sources involves entity resolution, schema alignment, and data quality validation to create coherent and queryable knowledge representations. Clustering algorithms like weakly connected component (WCC) and Louvain provide essential insights into the global structure and community organization of any multi-source knowledge graph. Subgraph analysis measures like density, conductance and relative size of largest connected components offer quantitative approaches to evaluate knowledge graph quality and completeness across domains. Advanced path-based metrics such as degree-weighted path count (DWPC) enable more sophisticated analysis of relationship patterns and entity relevance than simple path counting approaches. Three comprehensive knowledge graphs (hetionet, protein-protein interaction network, clinical knowledge graph) serve as valuable testbeds for demonstrating integration techniques and analytical approaches. LLM-assisted interpretation of knowledge graph analysis results helps translate quantitative metrics like DWPC scores and network topology measures into actionable domain-specific insights and research hypotheses. The combination of structured graph queries with LLM interpretation provides a powerful approach for extracting meaningful insights from complex multi-source knowledge graphs across various application domains.

4.6 Reference

1. David N. Nicholson, Casey S. Greene, Constructing knowledge graphs

and their biomedical applications, Computational and Structural Biotechnology Journal, Volume 18, 2020, Pages 1414-1428, ISSN 2001-0370, https://doi.org/10.1016/j.csbj.2020.05.017. 2. Systematic integration of biomedical knowledge prioritizes drugs for

repurposing Daniel Scott Himmelstein, Antoine Lizee, Christine Hessler, Leo Brueggeman, Sabrina L Chen, Dexter Hadley, Ari Green, Pouya Khankhanian, Sergio E BaranzinieLife (2017-09-22) https://doi.org/cdfk DOI: 10.7554/ elife.26726 PMID: 28936969 PMCID: PMC5640425 3. GRAM: Graph-based Attention Model for Healthcare Representation

Learning Edward Choi, Mohammad Taha Bahadori, Le Song, Walter F. Stewart, Jimeng Sun arXiv (2016-11-21) https://arxiv.org/abs/1611.07012v3 4. miRNA-Disease Association Prediction with Collaborative Matrix

Factorization Zhen Shen, You-Hua Zhang, Kyungsook Han, Asoke K. Nandi, Barry Honig, De-Shuang Huang Complexity (2017) https://doi.org/ggmrpm DOI: 10.1155/2017/2498957 5. Neural networks for link prediction in realistic biomedical graphs: a

multidimensional evaluation of graph embedding-based approaches Gamal Crichton, Yufan Guo, Sampo Pyysalo, Anna Korhonen BMC Bioinformatics (2018-05-21) https://doi.org/ggkm7q DOI: 10.1186/s12859-018-2163-9 PMID: 29783926 PMCID: PMC5963080 6. Network-based integration of multi-omics data for prioritizing cancer

genes Christos Dimitrakopoulos, Sravanth Kumar Hindupur, Luca Häfliger, Jonas Behr, Hesam Montazeri, Michael N Hall, Niko Beerenwinkel Bioinformatics (2018-03-14) https://doi.org/gc6953 DOI: 10.1093/bioinformatics/bty148 PMID: 29547932 PMCID: PMC6041755 7. Safe Medicine Recommendation via Medical Knowledge Graph

Embedding Meng Wang, Mengyue Liu, Jun Liu, Sen Wang, Guodong Long, Buyue Qian arXiv (2017-10-16) https://arxiv.org/abs/1710.05980v2 8. GAMENet: Graph Augmented MEmory Networks for Recommending

Medication Combination Junyuan Shang, Cao Xiao, Tengfei Ma,

Hongyan Li, Jimeng Sun Proceedings of the AAAI Conference on Artificial Intelligence (2019-07-17) https://doi.org/ggkm7r DOI: 10.1609/aaai.v33i01.33011126 9. Heterogeneous network embedding for identifying symptom candidate

genes Kuo Yang, Ning Wang, Guangming Liu, Ruyu Wang, Jian Yu, Runshun Zhang, Jianxin Chen, Xuezhong Zhou Journal of the American Medical Informatics Association (2018-10-23) https://doi.org/gfg6nr DOI: 10.1093/jamia/ocy117 PMID: 30357378 10. Predicting Protein–Protein Interactions from Multimodal Biological

Data Sources via Nonnegative Matrix Tri-Factorization Hua Wang, Heng Huang, Chris Ding, Feiping Nie Journal of Computational Biology (2013-04) https://doi.org/f4thrx DOI: 10.1089/cmb.2012.0273 PMID: 23509857 11. Protein functional properties prediction in sparsely-label PPI networks

through regularized non-negative matrix factorization Qingyao Wu, Zhenyu Wang, Chunshan Li, Yunming Ye, Yueping Li, Ning Sun BMC Systems Biology (2015) https://doi.org/gb5tvr DOI: 10.1186/1752- 0509-9-s1-s9 PMID: 25708164 PMCID: PMC4331684 12. Large-scale analysis of disease pathways in the human interactome.

Monica Agrawal, Marinka Zitnik, and Jure Leskovec. Pacific Symposium on Biocomputing. 2018. 13. J. Menche et al., Science 347, p. 1257601 (2015). 14. A. Chatr-Aryamontri et al., Nucleic Acids Research 43, D470 (2015). 15. Blondel, Vincent D; Guillaume, Jean-Loup; Lambiotte, Renaud;

Lefebvre, Etienne (9 October 2008). "Fast unfolding of communities in large networks". Journal of Statistical Mechanics: Theory and Experiment. 2008 (10) 16. S. E. Schaeffer, Computer Science Review 1, 27 (2007). 17. DiMasi JA, Grabowski HG, Hansen RW. 2016. Innovation in the

pharmaceutical industry: New estimates of R&D costs. Journal of Health Economics 47:20–33. DOI: https://doi.org/10.1016/j.jhealeco.2016.01.012, PMID: 2692 8437 18. Reichert JM. 2003. Trends in development and approval times for new

therapeutics in the United States. Nature Reviews Drug Discovery 2:695–702. DOI: https://doi.org/10.1038/nrd1178, PMID: 12951576 19. Hay M, Thomas DW, Craighead JL, Economides C, Rosenthal J. 2014.

Clinical development success rates for investigational drugs. Nature

Biotechnology 32:40–51. DOI: https://doi.org/10.1038/nbt.2786, PMID: 24406927 20. Large-scale structural and textual similarity-based mining of knowledge

graph to predict drug–drug interactions Ibrahim Abdelaziz, Achille Fokoue, Oktie Hassanzadeh, Ping Zhang, Mohammad Sadoghi Journal of Web Semantics (2017-05) https://doi.org/gcrwk3 DOI: 10.1016/j. websem.2017.06.002. 21. Matrix Factorization-Based Prediction of Novel Drug Indications by

Integrating Genomic Space Wen Dai, Xi Liu, Yibo Gao, Lin Chen, Jianglong Song, Di Chen, Kuo Gao, Yongshi Jiang, Yiping Yang, Jianxin Chen, Peng Lu Computational and Mathematical Methods in Medicine (2015) https://doi.org/gb58g8 DOI: 10.1155/2015/275045 PMID: 26078775 PMCID: PMC4452507 22. Daniel Scott Himmelstein, Antoine Lizee, Christine Hessler, Leo

Brueggeman, Sabrina L Chen, Dexter Hadley, Ari Green, Pouya Khankhanian, Sergio E Baranzini (2017) Systematic integration of biomedical knowledge prioritizes drugs for repurposing eLife 6:e26726 https://doi.org/10.7554/eLife.26726 23. Drug-Target Interaction Prediction with Graph Regularized Matrix

Factorization Ali Ezzat, Peilin Zhao, Min Wu, Xiao-Li Li, Chee-Keong Kwoh IEEE/ACM Transactions on Computational Biology and Bioinformatics (2017-05-01) https://doi.org/ggmrrp DOI: 10.1109/tcbb.2016.2530062 PMID: 26890921 24. Predicting Drug-Target Interaction Using Deep Matrix Factorization

Hafez Eslami Manoochehri, Mehrdad Nourani 2018 IEEE Biomedical Circuits and Systems Conference (BioCAS) (2018-10) https://doi.org/ggmrrn DOI: 10.1109/biocas.2018.8584817 25. DrPOCS: Drug Repositioning Based on Projection Onto Convex Sets

Yin-Ying Wang, Chunfeng Cui, Liqun Qi, Hong Yan, Xing-Ming Zhao IEEE/ACM Transactions on Computational Biology and Bioinformatics (2019-01-01) https://doi.org/ggmrrq DOI: 10.1109/tcbb.2018.2830384 PMID: 29993698 26. Neighborhood Regularized Logistic Matrix Factorization for Drug-

Target Interaction Prediction Yong Liu, Min Wu, Chunyan Miao, Peilin Zhao, Xiao-Li Li PLOS Computational Biology (2016-02-12) https://doi.org/ggmrrw DOI: 10.1371/journal.pcbi.1004760 PMID: 26872142 PMCID: PMC4752318.

27. Heterogeneous Network Edge Prediction: A Data Integration Approach

to Prioritize Disease-Associated Genes Daniel S. Himmelstein, Sergio E. Baranzini https://journals.plos.org/ploscompbiol/article? id=10.1371/journal.pcbi.1004259 28. Co-Author Relationship Prediction in Heterogeneous Bibliographic

Networks* Yizhou Sun† Rick Barber† Manish Gupta† Charu C. Aggarwal‡ Jiawei Han†Stephens M, Balding DJ. 2009. Bayesian statistical methods for genetic association studies. Nature Reviews Genetics 10:681–690. DOI: https://doi.org/10.1038/nrg2615, PMID: 19763151 29. Sawcer S. 2008. The complex genetics of multiple sclerosis: pitfalls and

prospects. Brain 131:3118–3131. DOI: https://doi.org/10.1093/brain/awn081, PMID: 18490360 30. Alberto Santos, Ana R. Colaço, Annelaura B. Nielsen, Lili Niu,

Maximilian Strauss, Philipp E. Geyer, Fabian Coscia, Nicolai J. Wewer Albrechtsen, Filip Mundt, Lars Juhl Jensen and Matthias Mann. A knowledge graph to interpret clinical proteomics data. 31. Maya Rotmensch, Yoni Halpern, Abdulhakim Tlimat, Steven Horng &

David Sontag, Learning a Health Knowledge Graph from Electronic Medical Records, 2017 32. Barton, E., Freeman, T., Baum, F., Javanparast, S., & Lawless, A.

(2019). The feasibility and potential use of case-tracked client journeys in primary healthcare: A pilot study. BMJ Open, 9(5), e024419. https://doi.org/10.1136/bmjopen-2018-024419 https://pubmed.ncbi.nlm.nih.gov/31154293/ 33. Rachel Hipp, PharmD, BCPS, Erik Abel, PharmD, BCPS, and Robert J.

Weber, PharmD, MS, BCPS, FASHP. A Primer on Clinical Pathways Hosp Pharm. 2016 May; 51(5): 416–421. doi: 10.1310/hpj5105-416 34. J. Morris Chang, Di Zhuang, and G. Dumindu Samaraweera, Privacy-

Preserving Machine Learning. Manning, 2023

[1] The suffix -ome as used in molecular biology refers to a totality of some sort. For instance, genome refers to all genetic information of an organism.

[2] Official descriptions coming from https://www.ncbi.nlm.nih.gov/books/NBK21134/

[3] https://www.nlm.nih.gov/research/umls/index.html

[4] One of the authors of this book, Alessandro, has a daughter affected by this disease. So, we can easily check the results, without being experts.

[5] http://snap.stanford.edu/pathways/

[6] https://www.disgenet.org/

[7] https://disease-ontology.org/

[8] https://ftp.ncbi.nih.gov/gene/DATA/gene_info.gz

[9] https://www.ncbi.nlm.nih.gov/gene

[10] http://downloads.graphaware.com/neo4j-db-seeds/ppi-5.26.4-2025-03- 22T08-30-33.backup

[11] Percentiles indicate the value below which a certain percentage of data points fall. For example, p99 means 99% of values are below this threshold, while p90 means 90% of values are below this threshold. In our results, p99: 21521 means that 99% of the connected components have fewer than 21,521 proteins.

[12] Collaborative filtering is a very common technique in recommendation systems development. It uses only previous purchases or likes from users to predict other relevant items. It differs from context-based techniques where instead the “properties” of the items and the “profile” of the users are used to provide recommendations. These concepts are extensively discussed in our previous book: Graph-Powered Machine learning, Manning 2021.

[13] https://het.io/

[14] The database backup for Neo4j 5.x is available here: https://downloads.graphaware.com/neo4j-db-seeds/hetionet-5.26.4-2025-03- 22T11-39-51.backup

[15] https://git.dhimmel.com/rephetio-manuscript/#tbl:metanodes

[16] https://go.drugbank.com/

[17] https://www.ebi.ac.uk/chembl/

[18] https://drugcentral.org/

[19] https://www.bindingdb.org/rwd/bind/index.jsp

[20] https://think-lab.github.io/d/220/

[21] https://journals.plos.org/ploscompbiol/article? id=10.1371/journal.pcbi.1004259

[22] GO here stands for Gene Ontology: https://geneontology.org/

[23] https://www.sciencedirect.com/science/article/abs/pii/S0016508598701350 or https://www.nature.com/articles/ejhg2014244

[24] https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2681138/

[25] https://claude.ai/new

[26] https://www.healthit.gov/faq/what-electronic-health-record-ehr

[27] https://www.graphable.ai/blog/patient-journey-mapping/

[28] It is available here: https://github.com/MannLabs/CKG

[29] https://downloads.graphaware.com/neo4j-db-seeds/ckg-5.26.4-2025-03- 27T09-51-46.backup

## Part_III:_Building_knowledge_graphs_from_text

The transformation of unstructured textual data into structured, actionable knowledge represents one of the most exciting frontiers in the development of intelligent systems. This part explores the powerful combination of Knowledge Graphs and Large Language Models in extracting, structuring, and representing knowledge from text, demonstrating how these technologies complement each other to unlock value from the vast amounts of unstructured information that organizations possess.

While Part II focused on building knowledge graphs from structured data sources, this part tackles the more challenging realm of unstructured textâ €”which comprises 80-90% of enterprise data today. The integration of LLMs has revolutionized this domain, bringing unprecedented capabilities in understanding and extracting meaningful information from natural language and reducing the necessity of human labor. However, the true power emerges when these capabilities are combined with traditional NLP techniques and knowledge graph technologies, creating systems that can both understand context and maintain structured, verifiable knowledge representations.

The chapters in this part guide readers through the complete lifecycle of text- to-knowledge-graph conversion, from initial entity extraction to sophisticated disambiguation and relationship mapping. Through real-world case studies and practical implementations, they demonstrate how to:

Transform unstructured documents into structured, queryable knowledge representations Combine traditional NLP techniques with modern LLM capabilities for optimal results Leverage external knowledge bases and ontologies to enrich and validate extracted information Apply advanced disambiguation techniques that utilize semantic relationships for enhanced accuracy

Chapter 5 demonstrates the fundamental pipeline for converting text to knowledge graphs, introducing core techniques in Named Entity Recognition (NER) and relationship extraction using both traditional NLP approaches and modern LLM-based methods. Through a compelling case study at the Rockefeller Archive Center, it shows how to extract structured knowledge from historical documents to reveal research networks and funding patterns.

Chapter 6 expands on these foundations through another application of the Rockefeller Archive Center documents, focusing on the complete workflow from document processing and OCR scanning to sophisticated graph analytics. It demonstrates a three-layer schema design and advanced techniques for data cleaning, entity resolution, and network analysis that can identify key researchers and trace influence patterns across historical scientific collaboration.

Chapter 7 explores Named Entity Disambiguation (NED) in specialized domains, particularly healthcare regulation. Using the European Commission's management of substances of human origin as a case study, it demonstrates how to go beyond simple entity recognition to link entities to structured knowledge bases like UMLS and SNOMED, enabling conceptual search and intelligent advisory systems for regulatory decision-making.

Chapter 8 concludes with an innovative approach to Named Entity Disambiguation that combines open LLMs with domain ontologies. This novel methodology overcomes limitations of traditional NED tools by leveraging relationships between entities in ontologies like SNOMED, demonstrating how general-purpose LLMs can be enhanced with structured domain knowledge for more accurate and contextually aware disambiguation.

Throughout this part, the focus remains on practical implementation while maintaining a strong theoretical foundation. The progression from basic entity extraction to sophisticated ontology-enhanced disambiguation not only solves immediate information extraction challenges but also prepares organizations for the advanced applications and integrations explored in subsequent parts of the book. The techniques presented here demonstrate the critical synergy between traditional knowledge representation and modern AI capabilities, establishing the foundation for building truly intelligent knowledge systems.

## 5_Domain-specific_knowledge_extraction_from_unstructured_data

This chapter covers

Building Knowledge Graphs from unstructured data Complexities of managing the archives: the Rockefeller Archive Center example Large Language Models for fast & accurate entity and relation extraction

Until now, we have discussed knowledge graphs (KGs) based on structured data such as tables, knowledge bases, and so forth, but what about unstructured data? Think of the various documentations, emails, chats, laws, research papers, guidelines, news articles, social media, and so on. The world is overflowing with information and knowledge locked in the unstructured form. Using these data sources could result in obtaining countless valuable observations, facts and insights important for your business.

The task of transforming unstructured data into knowledge is a complex multistep process, which consists of data ingestion and processing, various Natural Language Processing (NLP) techniques, data enrichment, ML processing and data modeling to build various downstream applications. Conceptually, this process has two main challenges:

Knowledge representation Knowledge learning and construction

As discussed in chapter 2, knowledge representation refers to the way in which information is modeled so that a computer (and humans) can access it autonomously for solving complex tasks. It plays a fundamental role in data transformation because, if properly designed, it speeds up processing by making the concepts (re)usable and extendable. In this context, KG represents an ordered and connected version of the same information that’s

otherwise isolated, distributed, and disorganized.

Knowledge learning, on the other hand, refers to the combination of frameworks and technologies such as NLP or LLM to mine concrete insights from unstructured documents. It is also one of the tasks comprising knowledge acquisition. To illustrate the complexities we will encounter when building KGs from textual data, we shall dive into one specific project and show how to overcome some of the typical challenges of mining unstructured data.

5.1 The archives challenge

The analysis of an archive is a challenging task. It covers a wide range of topics and concepts, very long timespans (current as well as historical data), multiple - but overlapping - data sources of different complexities and linguistic styles, as well as significant complexity of knowledge patterns contained within. Moreover, the data is typically unstructured - large volumes of textual documents such as books, reports, meeting minutes, grant assignments, etc. A concrete example that we will study in this and the following chapter is a subset of historical data on the origins of modern scientific dicsiplines held by the Rockefeller Archive Center (RAC).

The Rockefeller Archive Center is not just a repository of historical as well as current documents, it is also a research center dedicated to the study of philanthropy and the many (research) sectors influenced by American foundations, individual donors, and civil society organizations they support. It holds records of more than 40 philanthropic foundations, other research institutions, and cultural organizations. Among them are historical records of the Rockefeller Foundation (RF) and Ford Foundation, which are held and made accessible to researchers worldwide.

Rockefeller Foundation

The story of the Rockefeller Foundation goes more than a century back. It was founded in 1913 as one of the earliest major philanthropic institutions in the US, right after Carnegie Corporation, by John D. Rockefeller, his son, and their business advisor Frederick Taylor Gates. John D. Rockefeller was an

American business magnate considered, in his epoch, the wealthiest American of all time and the richest person in modern history. At the peak, he controlled 90% of all oil production in the US. It is not therefore surprising that he became worried for the well-being of his heirs if they inherited all this wealth: they could "dissipate their inheritances or become intoxicated with power" [1]. Later on, Gates further encouraged him to go in the direction of creating “permanent corporate philanthropies for the good of Mankind.” However, nothing like that has ever been achieved on such a scale. He had to therefore spend a significant effort on first defining, together with other industrialists such as steel magnate Andrew Carnegie, the modern systematic approach of targeted philanthropy that would have a major effect on various scientific research fields, medicine, and education. That culminated in the creation of the Rockefeller Foundation, one of the top US foundations to date.

Our aim is to shed light into how grant-making processes function behind the scenes. To achieve this, we shall need to design a high-quality domain- specific knowledge extraction system. The set of necessary steps is outlined in the mental model in Figure 5.1.

Figure 5.1 Path from domain-specific unstructured textual data towards structured knowledge. Each of the key steps relies on state-of-the-art Machine Learning models, be it for example Optical Character Recognition for document digitization, NER or Relation Extraction system.

The Rockefeller Foundation’s (RF) process of selecting projects for funding was centered around a group of so-called program officers, whose responsibility was to pinpoint auspicious areas of research that would be worth funding. For example, in the area of natural sciences, one of the officers was Warren Weaver – a mathematician and formerly a professor, who was responsible for approving grants in biology, chemistry, and physics. To achieve that, these officers had meetings, talked to scientists, visited universities and their departments, and discussed the work of the people they met, as well as asked for opinions on the projects of others. They strived to build deep knowledge about all the important activities happening within their specialization, such as natural sciences, by building a network of trusted researchers who worked on ambitious projects and asked them frequently about other people’s work – they searched for recommendations outside of their existing network. All these meetings, phone calls, and dinners were carefully recorded in their diaries, often written hastily – and on typewriters – with lots of shortcuts, abbreviations, and other domain-specific nomenclature.

These Officer Diaries thus represent literally a treasure trove of detailed knowledge about the progress, and struggles, of ongoing projects, professional as well as deeply personal observations, impressions, and feelings of the officers and the interviewed people.

All this rich information has tremendous potential if mined and modeled properly and accurately. We could start finding answers to questions such as:

“Are there any patterns that typically precede the funding of an idea?”

“Do granted projects tend to run through recommendations of influential scientists or previous grantees?”

“How many internal discussions and developments take place within the Rockefeller Foundation before the grant is awarded?”

“Are there trends in funding certain sub-disciplines? Do they change over time?”

The goal of this chapter is to showcase how state-of-the-art Machine Learning technologies can be used and adjusted to a concrete specific domain in order to mine complex knowledge from unstructured data. In the following chapter, we shall discuss how to use this extracted information to produce a Knowledge Graph and use it for answering complex questions about relational information previously locked within unstructured documents.

5.2 Key Concepts of Knowledge Extraction

The transformation of textual data into structured knowledge graphs centers on two fundamental processes: identifying entities within the text and extracting the relationships that connect them. While other natural language processing techniques such as sentiment analysis, keyword extraction, and topic tagging can provide valuable isolated pieces of information that can be layered onto a knowledge graph, the core foundation lies in entity and relationship extraction. These interconnected components form the structural backbone of any knowledge graph, creating the connected network of information that enables complex querying and analysis. The following

subsections focus on these essential building blocks of knowledge extraction.

5.2.1 Named Entity Recognition

The first fundamental step form text (and other unstructured data sources) to a Knowledge Graph is Named Entity Recognition (NER). NER is an ML classification system trained to identify mentions of named entities in raw text and to assign them to predefined categories such as person names, organizations, location, diseases, and so on. Recognizing named entity has many business use cases such as:

Discovering new insights by connecting documents from various data sources (for example, connecting people mentioned in financial documents with information from business registries). Improving a company’s information management and data governance. Laying the basis for a data compliance system. Improving search capabilities. Relating causes with effects (for example, such as weather conditions with flight delays).

Many open source NLP libraries offer high quality models for these entity categories out-of-the-box. However, although extremely useful, it is rarely enough. Each industry and even individual use cases within the same industry have their specifics and challenges. The Rockefeller Archive Center use case is no exception. To solve it, we need to build a robust domain-specific named entity recognition system.

Generic named entities, provided out-of-the-box by many open source NLP libraries, such as Person, Location, and Organization are a good start, but far from enough. A concrete example of what the officer diaries contain is shown in Figure 5.2.

Figure 5.2 A snippet of Warren Weaver's diary entry with highlighted relevant named entities.

Indeed, the identification of mentions of person and organization (university) names is important, but just as important is the ability to identify mentions of conversation topics. A Topic entity can be typical of three different subtypes: a research discipline, technology, or disease. In the provided example, there are mentions of four topics of discipline type: “aerological research”, “meteorological research”, “polar air masses” and “temperature air masses”. A simple dictionary-based NER system would not solve this task; we are dealing not only with generic high-level entities such as “meteorological research”, where a dictionary approach might be feasible due to the fairly limited number of general research disciplines but also much more fine-grained ones such as “polar air masses”, whose number is virtually limitless, especially considering also the historical nature of the documents (some then-studied topics ending up as dead ends).

To solve this task, we need a complex custom entity extraction system. That can be achieved either by training a custom NER model, or as we shall soon see, by taking advantage of the Large Language Models.

5.2.2 Relation extraction

The second indispensable step on our path from textual data to a Knowledge Graph is the Relation Extraction (RE). It is the task of identifying semantic relations between entity pairs within textual documents.

For example, consider the following sentence: “Jane Austen, Victorian era writer, is employed nowadays by Google.” This text mentions not only named entities of classes PERSON (“Jane Austen”) and ORGANIZATION (“Google”), but we also see that they are closely related – one of them is the employee of the other. Capturing this relatedness is the making of a true Knowledge Graph. There are more ways to model this type of relationships, for example: Jane Austen – WORKS_FOR -> Google. Someone else might decide to call the relationship EMPLOYED_BY, or even change its direction and call it EMPLOYS: Jane Austen <– EMPLOYS - Google. Whichever way choose, it has little importance to the outcome. The key point here is to stay consistent across documents to build a meaningful connected KG.

5.3 Building KGs with Large Language Models

What we have discussed so far regarding the customization of NER and Relation Extraction models could be called “traditional” NLP. By that we mean a whole complex process of building task-specific training datasets, identifying the best model architectures, fine-tuning the hyper-parameters, etc. Part of the process is also iteratively improving the quality of the training data. It is, to put it mildly, arduous.

During writing this book, a breakthrough in the NLP domain delivered by OpenAI company reached another peak. In winter 2022, the company published its first models from the GPT-3.5 series, including a chatbot, ChatGPT, with astonishing capabilities: based on a question or a task description formulated in natural language, they can help you draft a formal letter, summarize an article, answer factual questions, translate a recipe, generate a specified image, write Python code and many others.

This type of generative model is called a Large Language Model (LLM) and the key concept behind it is transfer learning: the ability to reuse linguistic patterns and knowledge learned in a generic task (such as predicting randomly masked tokens) in another task (such as relation extraction). This significantly reduces the required training dataset size for supervised learning. The same accuracy can be achieved with less human-labeled data since we don’t have to train the model from scratch, but the weights learned on massive datasets are reused.

Although it might seem otherwise from the hype around them, OpenAI’s GPT family of models did not introduce a revolutionary architecture when compared to Language Models (think of BERT) that have been around for several years already. What makes them exceptional, and in general all Large Language Models, are mainly two things:

Model complexity, a.k.a. number of parameters Size – and in the case of GPT also quality! - of the training corpus

As shown in [3], larger neural models (i.e., more trainable parameters) require fewer data samples to reach the same performance in terms of the

test-set loss. For example, OpenAI’s GPT family of models started with about 120 million parameters (GPT-1), continued to 1.5 billion (GPT-2), and 175 billion (GPT-3.5), and it is rumored that GPT-4, which became available in the 2nd half of 2023, had over 1 trillion of parameters. If only humanity’s wisdom grew that fast!

The same paper also proves that the size of the training corpus is of paramount importance. Not surprisingly, the more data the model has to learn from, the better it gets. But it’s not just about data quantity, but also data quality. In what we can call traditional Machine Learning, the main focus was often on identifying the best model architecture and fine-tuning its hyperparameters. This is the so-called model-centric approach, which in an extreme form ignores the potential data quality issues: it should not be surprising that if we put in faulty data, we get faulty predictions. What started gaining more and more traction over time, though, is the data-centric paradigm [3], meaning focusing primarily on data engineering with a goal to improve the quality (and quantity) of data used for training high-complexity ML models. This has been one of the key driving forces behind the success of LLMs. Due to all these factors, they became so powerful that we can simply formulate the task in natural language and the model generates an answer. No model engineering is needed at all.

To summarize, once the model architecture becomes sufficiently complex, what matters the most to accomplish a wide range of inference objectives is the training data quality, and at the inference stage the so-called prompt engineering, i.e. the formulation of a task.

Although Large Language Models in general, and OpenAI’s GPT family of models in particular, have formidable capabilities, there are certain limitations that one needs to keep in mind when designing a system dependent on them. The most important one for the task at hand is the so- called “hallucination”, which is the tendency of the model to make up “facts” and do false reasoning in cases when there is no justification for the response in the training data. For example, if we ask ChatGPT about the costs of NASA’s SLS rocket, but its training data does not contain this sort of information, it will tend to choose a random number and claim that this is the cost. The OpenAI team is working on limiting this behavior, but it remains an

open issue to date. Therefore, we need to be careful when building applications based on LLMs, especially in high-stake games such as Law Enforcement investigations.

Overall, despite their great achievements, the authors of this book would refrain from calling these models “AI”. We believe that there is still a very long way to go to get from “fancy statistics” to true artificial intelligence (if ever even possible). So instead of considering them as a final magical solution to all our problems, we suggest taking full advantage of their benefits and using them as another part of our pipelines, i.e., as one of several milestones, rather than a final goal. In our case, that means using them for building more comprehensive cleaner knowledge graphs by asking them to extract high quality named entities, relations and their properties, see mental model in Figure 5.3 as a reminder.

Figure 5.3 Mental model of transforming textual data into structured knowledge: Named Entity Recognition and Relation extraction using a Large Language Model.

5.3.1 How to use Large Language Models

LLMs can be used in the KG building effort in one of the following ways:

Out-of-the-box, meaning prompt-based inference. Fine-tuned to produce more accurate stable outputs curated to the specific domain.

Given their complexity and the amount and quality of data used to train them, these models can be used for various tasks directly, without any need to fine- tune them with domain-specific human-annotated datasets, such as in the case of standard Language Models. The LLMs typically perform very well out of the box by simply formulating the task in natural language and optionally providing one example. We can directly ask them to, for example, identify the sentiment of the provided sentence. The task formulation is referred to as a prompt and the whole iterative process of identifying the most performant

prompt for the given task is referred to as prompt engineering.

In this chapter, we are specifically interested in the so-called text completion task: we provide the model a prompt, and it generates a text according to the context and task formulation in the prompt. Our goal is to leverage the power of GPT for identifying entities and relations among them from various textual inputs. The whole process is schematically outlined in Figure 5.4.

Figure 5.4 The two roles of Large Language Models in KG generation from textual data: prompt- based approach (upper branch of the flow) vs. fine-tuning approach (lower branch).

As per usual in data science projects, we start with quick data exploration: we need to understand the domain we’re dealing with, the specific challenges ahead, the reasonable expectations for the outcomes. Remember the example of a diary entry at the beginning of this chapter? You probably noticed that

the interviewee, C. G. Rossby, was introduced by full name only at the beginning of the diary entry, and later referenced simply by “R.” abbreviation. Therefore, we need to design our knowledge extraction system in a way that can resolve these uncommon ways of referencing people.

The next step is the above-discussed prompt engineering. Typically, it takes several iterations to identify the most valuable prompt. Sometimes, even a few changed words in the task formulation can have a significant impact on the performance. We need to describe the task as precisely as possible to limit ambiguities which would give the model a chance to be too inventive in its answers.

After we’re done with prompt engineering, we have two options. Either we are satisfied with the quality of the replies and proceed directly to the KG generation step, or it will be revealed that the task at hand is too complex to rely purely on a prompt-based approach and we therefore move to the finetuning of the LLM, meaning we’ll have to prepare a small training dataset consisting of prompts (this time only pure input documents that we want to process, no task description) and expected outputs. This takes some time to prepare, and it is also costly not just for the training phase, but also for custom model serving which is priced in the case of OpenAI’s models about an order of magnitude higher than out-of-the-box models. But the clear advantage is that we get a model that is specialized to our domain, providing more stable and accurate outputs as needed by our application. Moreover, the work we spent on prompt engineering is not lost! We can leverage it to pre- annotate the future training dataset before we ask human(s) to spend their valuable time. Once the dataset is ready, we can use OpenAI’s API for fine- tuning the model and if satisfied already after the first iteration, we can use it to produce a KG. If it’s not good enough yet, we can repeat the process: add more training data and try again.

5.3.2 Prompt engineering in examples

Let’s have a closer look at the prompt engineering approach, which should be always a starting point since it can be eventually reused to pre-annotate data for fine-tuning the LLM model. We can start with a generic task: let’s ask GPT to identify all entities and relations among them using a simple prompt

in Listing 5.1.

Listing 5.1 Prompt version 1 – identify all entities and relations.

prompt_segments = dict()

prompt_segments['task'] = #A """You are an expert on constructing Knowledge ➥Graphs from texts using named entity recognition and relation ex ➥Given a prompt, identify as many entities and relations among th ➥possible and output a list of relations in the format [ENTITY 1, ➥TYPE, RELATION, ENTITY 2, ENTITY 2 TYPE]. ➥The relations are directed, so the order matters."""

prompt_segments['example'] = "J.R.Smith (Prof. Phys.) is employed ➥and mentioned another scientist, Mary Hodge, who works on cyclot ➥research." #B

prompt_segments['example_output'] = #C """["J. R. Smith", "person", "has ➥title", "Professor of Physics", "title"] ["J. R. Smith", "person", "works for", "MIT", "organization"] ["J. R. Smith", "person", "talked about", "Mary", "person"] ["Mary", "person", "works on", "cyclotron", "occupation"]"""

We shall deploy it on a snippet of a diary entry using the OpenAI API call in Listing 5.2.

Listing 5.2 OpenAI API call.

import os import time from dotenv import load_dotenv from openai import OpenAI from listing_2 import prompt_segments

_ = load_dotenv() #A

OPENAI_MODEL = "gpt-4o-mini"

def openai_query(client, prompt_segments: dict, #B ➥query: str):

messages = [ {"role": "system", "content": prompt_segments['task']}, {"role": "user", "content": prompt_segments['example']},

{"role": "assistant", "content": prompt_segments['example_o {"role": "user", "content": query} ]

t_start = time.time() response = client.chat.completions.create(model=OPENAI_MODEL, ➥messages=messages, temperature=0., max_tokens=2000) #C

print(response.choices[0].message.content) print(f"\nTime: {round(time.time() - t_start, 1)} sec\n")

return response.choices[0].message.content

if __name__ == "__main__":

client = OpenAI(api_key=os.environ['OPENAI_API_KEY'])

text = """ #D ➥ JOHNS HOPKINS UNIVERSITY Chemistry Department: ➥ Wednesday, November 9, 1932 ➥ WW visits the Dept. with Dr. Frazer (Baker Prof. Chem.). There ➥ undergraduate and 116 graduate students of chemistry, the latte ➥ cluding holders of the special State fellowships in chemistry u ➥ New Plan. D.H. Andrews (Prof.Chem.) is a physical chemist speci ➥ thermodynamics. He is not present at the time of WW's call, but ➥ assistants explains his work. He is measuring specific heats of ➥ compounds by a straight calorimetric method. This work is in it ➥ stages. He is also interested in making mechanical models of va ➥ from which can be demonstrated the theory of the Raman spectra ➥ J.B.Mayer (Assoc. in Chem.) is a former student of G. N. Lewis ➥ with Max Born at Gottingen summers. He specializes in the energ ➥ crystal lattices. His wife, last summer, prepared the new editi ➥ Born's treatise on this subject. In Mayer's laboratory Mrs. Win ➥ of the mathematician, is working on an experimental problem. An ➥ that Mayer is young and impresses one as an enthusiastic and ab ➥ """

openai_query(client, prompt_segments, text)

The results of Listing 5.2 are very encouraging. For example, the extracted relations featuring D. H. Andrews and J. B. Mayer are in Listing 5.3.

Listing 5.3 GPT output of the prompt in Listing 5.2.

["D.H. Andrews", "person", "has title", "Prof. Chem.", "title"] ["D.H. Andrews", "person", "specializes in", "thermodynamics", "f ["D.H. Andrews", "person", "is measuring", "specific heats of org

➥compounds", "research"] ["D.H. Andrews", "person", "is interested in", "mechanical models ➥various atoms", "research"] ["D.H. Andrews", "person", "demonstrates", "theory of the Raman s ➥"theory"] ["J.B. Mayer", "person", "has title", "Assoc. in Chem.", "title"] ["J.B. Mayer", "person", "is a former student of", "G. N. Lewis", ["J.B. Mayer", "person", "works with", "Max Born", "person"] ["J.B. Mayer", "person", "works at", "Gottingen", "location"] ["J.B. Mayer", "person", "specializes in", "energetics of crystal ➥lattices", "field"]

Impressive! D. H. Andrews is indeed a professor of chemistry, and he is involved in all the identified research topics. Similarly for J. B. Mayer: we received knowledge about his title, specialization, and who he studied under. Notice also that we got nice clean full person names in all these relations, even though in the actual sentence, it says just “Mayer” or “he”. In the traditional approach to the RE, we would have an NER model that would identify entity mentions (just “Mayer”), and then we’d need to use the coreference resolution model to link “Mayer” and “he” to the respective full names, and finally use the RE to receive relations with resolved names. Here, due to the generative nature of the LLMs, the coreference resolution is done implicitly – the model generates output based on its understanding of the whole context of the document. Finally, notice that the titles of both gentlemen were resolved from “Prof. Chem.” and “Assoc. in Chem.” to “Professor of Chemistry” and “Associate in Chemistry”. The model was able to deduce the correct full names of the titles from their very shortened versions! No traditional NER model alone would be able to do that.

Now let’s have a closer look at the entity relation types. Notice the extreme granularity, or level of detail, of the extracted relation types: specializes in, is measuring, is interested in, and demonstrates are all 100% true, but is this output truly useful from the KG point of view? They all actually represent the same concept. A human would probably assign them the same relation type, for example, works on, but GPT provided us with four different types! And this was just from one short paragraph. Imagine what would the KG schema look like if we processed thousands of pages, and then wanted to find answers to questions such as “Who works on research of organic compounds?” We would have to know and list all the semantically identical (or very similar) relation types in the Cypher query, since we would need to

avoid returning results that do not represent the umbrella term works on. That would be impractical, to put it mildly.

Similarly with the node labels. The four research topics from this single paragraph have been assigned three different labels: field, research, theory. In essence, they are correct. But what are they really? What do they have in common? They are all research topics of Professor Andrews, so a human might call them all by the same term such as a topic or even more specifically occupation and have a corresponding single node label in the graph for all of them. As you can see in the prompt, we opted for the label occupation in the provided example, because we observed that the model seemed to struggle to understand the definition of this node class when we called it different way, for example a topic – it missed lots of these entities.

Finally, did you notice that we are missing Mayer’s personality traits? Or that the model ignored the mention of “straight calorimetric method”? a relation featuring the “theory of Raman spectra”? Let’s try it again: let’s use the same prompt and model configuration to generate entities and their relations from the same text, in order to test the prediction stability, i.e., will the LLM produce the same output the 2nd time? The answer might be a bit disturbing: no. The “theory of Raman spectra”, correctly identified in the 1st run, is now missing, while we still miss the “straight calorimetric method”. However, the model acknowledged this time that Andrews talked about Mayer’s personality traits (we obtained relation ["D.H. Andrews", "person", "describes", "J.B. Mayer", "person"]), but it’s not very useful as it doesn’t capture what he said about him. If you run couple of experiments of this kind yourself, you might also notice that sometimes the model chooses different relation names, for example instead of was student of, it outputs it as studied under. All of this demonstrates the issue of instability of the predictions based purely on generic prompts of the style “Identify all entities and relations among them.”, which is obviously an issue when mining knowledge from documents in order to produce highly curated KG for further precise downstream analysis and tasks.

Reflecting on all the above, let’s test a new version of the prompt, see Listing 5.4. To address the normalization issue, we included lists of entity classes and relation types we are especially interested in, with some of the originally

underperforming relation types also having explanations in round brackets. Instead of these simple lists, we could state here full authoritative KG schema, but this approach leaves the door open for getting also entities and relations we haven’t thought of before. We also added two notes to the task description to help guide the model regarding the specificities of this dataset to improve the prediction stability in these cases: one explaining the habit of referring to people only by their initials, the other one mentioning the aliasing habit for university and department names (“U. Cal.” instead of “University of California”).

Listing 5.4 Prompt version 2 – identify specific entities and relations.

prompt_segments = dict()

prompt_segments['task'] = """ ... #A

Entities of interest: person, location, organization, date, occup ➥(a.k.a. person's work, specialization, research discipline, inte ➥occupation, technology). #B

➥Top relations of interest: "works for", "works with", #C ➥"student of" (link students with their teachers/advisors), "talk ➥(a person talking about another person), "talked with" (a person ➥with another person), "works on" (assignment of persons to their ➥work, specialisation, research discipline, interests etc.).

➥Note that persons are often first referenced by their full name, ➥then mentioned only by their surname or initials, for example: " ➥Richards" becomes "Richards", "ANR", or just "R.".

➥Note that organizations (universities, their departments) are of ➥shortened, for example: "University of California" is written as ➥Cal." or just "U. Cal.", "Department of Physics" is written as " ➥etc."""

prompt_segments['example'] = "..." #D prompt_segments['example_output'] = """..."""

Looking at the produced output, we see that indeed stating the entities and relations of interest in the prompt guides the LLM to produce more normalized outputs. Let’s explore again the extracted relations featuring D. H. Andrews and J. B. Mayer, see Listing 5.5.

Listing 5.5 GPT output of the prompt in Listing 5.4.

["D. H. Andrews", "person", "has title", "Professor of Chemistry" ["D. H. Andrews", "person", "works on", "thermodynamics", "occupa ["D. H. Andrews", "person", "measures", "specific heats of organi ➥compounds", "occupation"] ["D. H. Andrews", "person", "is interested in", "mechanical model ➥various atoms", "occupation"] ["D. H. Andrews", "person", "talked about", "Raman spectra", "occ ["J. B. Mayer", "person", "has title", "Associate in Chemistry", ["J. B. Mayer", "person", "is a former student of", "G. N. Lewis" ➥"person"] ["J. B. Mayer", "person", "works with", "Max Born", "person"] ["J. B. Mayer", "person", "specializes in", "energetics of crysta ➥lattices", "occupation"]

It worked. All the produced topics of work now have the same entity label – occupation. The titles are provided in their full form as “Professor of Chemistry” and “Associate in Chemistry”. And we have a single instance of works on relation, which is an improvement, though far from the desired three – we still got measures and specializes in. Notice also that this time we have simply “Raman spectra” instead of the “theory of Raman spectra” – both are correct as it is just a matter of preference, but at the very least we expect consistency across runs and documents. Moreover, where is the relation featuring the “straight calorimetric method”? Although these are still great results after just a few minutes of tweaking the prompt, we can surely do better.

Let’s see what one more iteration on the prompt side can do, this time we shall focus on expanding the example in the hope that it will provide clearer guidelines and prediction stability. Since works on relations are very important to us, we can expand this part by adding another mention of occupation, and we shall also provide examples for student of since the previous prompt failed on this one. The third version of the prompt is in Listing 5.6.

Listing 5.6 Prompt version 3 – more complex example.

prompt_segments = dict()

prompt_segments['task'] = """...""" #A

prompt_segments['example'] = "J.R.Smith, Prof. Phys. is employed ➥mentioned another colleague Mary Hodge, who studies along with h ➥master's student John Smith radioisotopes produced by cyclotron

prompt_segments['example_output'] = """["J. R. Smith", "person", ➥title", "Professor of Physics", "title"] ["J. R. Smith", "person", "works for", "MIT", "organization"] ["J. R. Smith", "person", "talked about", "Mary Hodge", "person"] ["Mary Hodge", "person", "works for", "MIT", "organization"] ["John Smith", "person", "student of", "Mary Hodge", "person"] ["Mary Hodge", "person", "works on", "radioisotopes", "occupation ["Mary Hodge", "person", "works on", "cyclotron", "occupation"]""

We shall again inspect in detail the output related to D. H. Andrews and J. B. Mayer, see Listing 5.7.

Listing 5.7 GPT-4o-mini output of the prompt in Listing 5.6.

["D. H. Andrews", "person", "has title", "Professor of Chemistry" ["D. H. Andrews", "person", "works for", "Johns Hopkins Universit ➥Chemistry Department", "organization"] ["D. H. Andrews", "person", "works on", "thermodynamics", "occupa ["D. H. Andrews", "person", "works on", "specific heats of organi ➥compounds", "occupation"] ["D. H. Andrews", "person", "works on", "calorimetric method", ➥"occupation"] ["J. B. Mayer", "person", "has title", "Associate in Chemistry", ["J. B. Mayer", "person", "works for", "Johns Hopkins University ➥Department", "organization"] ["J. B. Mayer", "person", "student of", "G. N. Lewis", "person"] ["J. B. Mayer", "person", "works with", "Max Born", "person"] ["Max Born", "person", "works at", "Gottingen", "location"] ["J. B. Mayer", "person", "works on", "energetics of crystal latt ➥"occupation"]

Eureka! We achieved everything we wanted: a high level of stability in terms of assignment of entity and relation classes, and it identified correctly all the relations we care about.

These few paragraphs were designed for illustration purposes about how prompt evolution can look like. In the real project, we did a couple more iterations and changed the output format to JSON, so that each entity and relation can also have properties where relevant. That allows us to extract

more complex knowledge, such as asking GPT to classify the sentiment of each TALKED_ABOUT relation (stored as a property) so that we can discern when people are referred to in a positive or negative sense, for example: “Lunch with Swain. He says Sollner is very good, but he has no manners – insults speakers at meetings.” Or to identify a (business) title as an attribute of WORK_FOR relation, which is especially helpful when one person is associated with more than one company. For the final version of the prompt used to generate KG presented in this chapter, have a look at our code repository.

Note that the prompts discussed in this chapter are designed for ChatGPT models by OpenAI (tested on gpt-4o-mini). By the time you are reading this book, newer models will be available, so certain adjustments to the prompt or the code might be needed, however, we tried to focus more on fundamental principles rather than stressing concrete model specificities. The readers should therefore be able to adapt the prompt to the current state-of-the-art.

5.3.3 Prompt engineering guidelines

With the rapidly evolving Large Language Models domain, there is no point in sharing concrete, technically specific guidelines – they might become obsolete even within a few months’ time. However, we do believe that there are certain generic learning points that are very likely to be transferable to newer models and which will help you to write better prompts faster. These key principles that we discovered when working on the Rockefeller Archive Center project are summarized in the following list of good practices of prompt engineering:

Task description and domain-specific guidance: The task formulation is a very important factor. A well-explained task has a large impact on the quality and stability of the outputs. It is worth experimenting with different formulations and adding the current dataset’s specificities, such as in our case the habit of referring to people by very specific abbreviations & initials. Naming of entity & relation classes: It is very useful to specify in the prompt lists of entity & relation classes in which we are interested the most. This helps to clarify the task and to provide more normalized

outputs. The chosen nomenclature matters a lot! If you notice that some entity or relation class underperforms compared to the others, try renaming it. For example, consider the topics of conversations: at first, we called these entities Topic, but soon we realized that this term is misleading or too generic to be used as an entity class. When we renamed it to Occupation, suddenly otherwise identical prompt started producing much more comprehensive and stable results. Complex and representative example: In complex tasks such as entity relation extraction, it is useful to provide as part of the prompt also an example on input and expected outputs. It can be a short synthetic example, meaning that it can be very condensed version of the actual texts that we need to process in the sense that it will contain examples of the most important entities and relations we care about. We should especially include complex linguistic formulations that can appear in the dataset to prepare the model for the real deployment and focus also on the relations with which the model systematically struggles. That said, the example should be also representative as much as possible – identify key relation types and make sure the example contains them all, otherwise you risk that the LLM will be misled to focus primarily on those and miss some other relations important to your use case. LLM configuration: There are a growing number of options of LLMs, some come with a commercial license such as OpenAI’s GPT, but there are also open-source ones. Each of them could have multiple versions, one for example finetuned for chats, the other for state-less text completion, etc. Experiment. Choose the best option for you. Also test the various parameters, especially so-called temperature, which controls the level of “creativity”: the higher the temperature, the more diverse and creative outputs it produces (also – the more propensity to “hallucination”, i.e. making up things not supported by the data). Lower temperatures move to the more deterministic side of the spectrum. Therefore, creative tasks such as text generation (summaries, essays, blog posts) are better performed with higher values of temperature. Concrete stricter tasks such as code generation require lower temperatures (say around 0.2). For exact factual focused tasks such as entity and relation extraction from a concrete document, we choose temperature 0 which produces nearly deterministic results. That means we don’t see surprises like strange entities or relations appearing out of

nowhere, not supported by the documents we supply (i.e., hallucinations) and we have less work with normalization (non-zero temperatures produce more diverse / “creative” names for entities and relation classes) and generally speaking post-processing (such as entity resolution due to creativeness of the model when composing the full entity names). Prediction stability testing: Remember that LLMs are generative models, and therefore by definition, they can have and do have their own “moods” – in principle, the same prompt can produce different results if run repeatedly on the same text. This can be limited by careful prompt engineering (non-ambiguous clear task formulation) and by setting very low values for temperature (see previous bullet point). If you choose non-zero temperature, consider testing the prediction stability properly by running the same prompt on the same test dataset multiple times and evaluating the overlap. If there’s something you really don’t want when mining factual knowledge, it is unstable prediction behavior and, therefore low confidence in the produced Knowledge Graph. Prompt unit testing: Prompt engineering should be a quick straightforward task, do not spend weeks or even months. On the other hand, it is true that sometimes, and especially in the early stages when you’re still building the know-how, it does take lots of iterations to get there. Therefore, just like in the case of code development, consider setting up a simple unit testing system: each time you update the prompt to fix some prediction issue observed in the data, add the textual snippet and the expected output to the test list, and every few iterations – a prompt milestone – run this list of tests in bulk, calculate success rate and output failures so you can quickly inspect them. This way you can make sure you did not lose some of the achievements of the previous iterations. And remember - it is still a generative system, so you might get fake “failures”, for e.g. slightly different formulation of the relation type, but still overall correct. Eyeballing a mini-KG: Each time you feel you have reached a prompt milestone, we recommend deploying it on a small sample of documents, for example few tens of pages, and produce a KG. Why? Well, we humans are sensory creatures – to actually see and touch is sometimes more useful than to just think, assume, and imagine. Graphs are made for seeing and touching: they offer a new unique angle to view and

manipulate data and knowledge. We observed that one quick look at a produced mini-KG can help discover further opportunities for improvement. For example, navigating through the graph, we immediately noticed that there were quite a few organization names – universities – in highly abbreviated forms, such as “U. of PA”. GPT, given its significant general knowledge acquired during the extensive training phase, should be able to fill in the blanks and identify this as “University of Pennsylvania”, if properly instructed. Similarly, we immediately noticed that there were quite a few abbreviated person names of the type “S.” instead of full names. Or that several people were seemingly working for the same organization, but the organization was just “Department of Physics” – but which university? Therefore, for the next iteration, we considered all these observed failures and added more complexity to the prompt example to help guide GPT to give us the desired outputs. Evaluation: When you’re satisfied with the prompt, do a proper quantitative evaluation. Process a few tens of pages worth of text, nominate someone as quality assurance manager (or some other fancy- sounding title), and ask him to go through the predictions and mark correct, incorrect, and missing entities and relations, and calculate per- class precision, recall and F1 score for entities and relations. This will give you confidence in the prediction quality before spending time and money on producing a KG from a big dataset, serve as a basis for future monitoring of model drift, and also will give you one more chance to improve prompt since the QA manager (data annotator) will be well positioned to spot systemic failures that couldn’t be identified when testing on a handful of examples. Initial explorative KG: In order to do prompt engineering, you obviously need to have some minimal understanding of the data, its content, knowledge you want to extract from it. This is not always the case: we’ve seen many use cases where the companies had just vague idea of the content, they knew there must be something useful in there, but didn’t have an exact idea of what to mine, i.e., what should be the KG schema. In such cases, you need to dive into the data and spend some time on initial exploration and understanding. This potentially long and tedious task can be streamlined if you design a quick generic entity and relation extraction prompt (simply ask it to extract all entities and

relations, and just give it very short generic example to rather demonstrate the output format than concrete entity and relation types to avoid biasing it too much), produce a KG from a sample of documents and explore it, navigate it, draw inspirations from it. Yes, it will be highly unnormalized (for e.g., many different, but semantically identical relation types), but it will help you to quickly understand the content of that dataset, the kind of entities and relations you could mine from it and start the prompt engineering from there. Last but not least, be ambitious! Think big, don’t assume that something would be too difficult for the LLM – just try it. Remember that the LLMs were trained on vast amount of data, which helped them to build a deep linguistic understanding and encode within their hundreds of billions of parameters general human-like knowledge about the world around us, such as that Rome is located in Italy, even though the document you want to analyze doesn’t explicitly say so. They can handle things such as typos in person or organization names, deduce from the context that “Prof. Chem.” is a title “Professor of Chemistry”, “U. of PA” is referring to the “University of Pennsylvania”, or that a mention of “Stanford” is actually “Stanford University” and not a location or person name. Therefore, when you provide an example in the prompt, be ambitious. Provide relations with nice full clean entity names, so that you get a cleaner more accurate KG right from the start, reducing the need for thorough entity cleansing and resolution in the post-processing phase.

Despite best efforts on prompt engineering, we need to keep in mind that it is a simplistic technique called zero-shot learning (just task description) or one- shot learning (in our case where we provided one example) – we expect a model pre-trained on large amounts of data to perform well on our very specific domain and task just based on a task description and optionally one or two examples. Despite the power of LLMs, this is still very hard when dealing with complex reasoning tasks such as entity relation extraction for building KGs. We know upfront that no matter what we do, there will always be some notable level of imperfections – misclassified entities, missing or incorrect relations. One-shot learning simply cannot prepare the model for all the complexities and ambiguities that await it in the full dataset, typically at least thousands of pages worth of text. Therefore, one last advice is: do not

overengineer and overthink the prompt development. Do a couple of quick prompt engineering iterations, verify every milestone by producing and eyeballing a mini-KG to help you identify other possible systemic failures and therefore candidates for prompt improvements, and move on.

When you get to the point that despite all the best efforts, prompt engineering is not yielding satisfactory results (typically when you’re in a mission-critical domain), it’s better to invest the remaining time of the project to fine-tuning the LLM rather than continuing the “never-ending” prompt improvements.

5.3.4 KG building: Traditional NLP or LLMs?

At this point, our readers might be wondering how to determine whether to follow the traditional NLP path or the LLM path. Or even – does the traditional NLP still have a place in the modern “AI” world? Let’s start by highlighting the key aspects – both positive and negative - of each approach.

Traditional NLP

Advantages

Prediction speed: smaller simpler models are faster at prediction time, even on CPU, can be deployed (and some even trained) on your laptop. Infrastructure simplicity and costs: simple and cheap infrastructure is enough to train and deploy models, often even no need for GPUs. Prediction costs: due to simpler cheaper infrastructure requirements, the prediction costs are very low. Security: easier and cheaper to configure on-premises solutions for isolated secure deployments, where the companies can’t (or don’t want to) risk sending data outside of their premises. Disadvantages

In-house data science expertise: to design, train, deploy, and maintain a significant highly specialized (ideally in-house) expertise is needed. Complex NLP pipeline configuration with multiple custom models: several specialized and customized models are needed, for example, NER, RE, Coreference Resolution, Entity Resolution, and

Disambiguation. Data annotation complexity & costs: each of the models needs to be trained for your domain with your specific data, which requires building multiple high-quality training datasets. This process takes a very significant amount of time (count at least several months for NER and min ½ year for RE), highly specialized (expensive) data science expertise throughout the process, and certain infrastructure and software costs (such as good data annotation tools). The initial investment is very high and the outcomes are not guaranteed, especially for complex tasks such as RE (some relation types are easy, others are a tough challenge for traditional RE models).

Large Language Models

Advantages

Initial domain customization costs: a much quicker road to production on your specific domain thanks to the transfer learning principle of LLMs and simplicity of prompt engineering (or fine- tuning which requires much smaller annotated datasets). Shallow learning curve: fast to get started/build know-how, no need for expensive expert data scientists, anyone with minimal technical thinking and skills can learn how to do prompt engineering and even fine-tuning. All-in-one NLP: no need to engineer a sequence of separate (but dependent) processing steps to achieve the goal, that is NER, RE, Coreference etc. – with one pass through the data, we get it all included. The generative nature of LLMs: not only that we have a much simpler processing pipeline, but the results are also of unprecedented quality thanks to the transfer learning and generative nature of the LLMs – they can easily handle misspellings and typos, reformulation of named entities when needed (remember the “U. of PA” vs “University of Pennsylvania”), filling the blanks (incomplete information, which can be complemented from LLM’s prior knowledge), complex contextual and linguistic understanding and reasoning, all of which leads to producing cleaner more accurate KGs than any traditional NLP model would have been

capable of, all this right out-of-the-box. Simpler post-processing/normalization: the generative nature and formidable language understanding of LLMs not only results in more accurate entities and relations, but also as a direct consequence implies lower requirements on various post- processing (cleansing, normalization, entity resolution) Disadvantages

Prediction speed: massive models imply much slower prediction speed, despite being served on powerful GPUs. Infrastructure complexity: since GPUs are a must-have not just for training, but also for prediction, it implies also higher infrastructure configuration complexity and costs. Fine-tuning costs: if prompt engineering turns out insufficient for the output quality required by the stakeholder (typically mission- critical use cases), the model needs to be finetuned, which implies costs on preparing training dataset (still of much smaller size than the traditional NLP models need), high training infrastructure costs (re-training these beasts is not that easy) and also high prediction costs (OpenAI charges 10x more for running predictions with your custom model; even for on-premise deployments, you need to consider costs related to maintaining and managing model versions). Prediction costs on very large datasets: it is very likely that for small to medium-sized datasets, using LLM will turn out cheap compared to the immense initial investment needed to create a custom traditional NLP pipeline, however, the numbers can look quite different if you have a massive dataset, such as in long continuous streaming scenarios. Security / on-premises deployment costs: if you work in a high- security domain, well – good luck. Especially if you can’t utilize even cloud provider and have to setup, deploy, and maintain your own GPU cluster. Note: Nowadays, many of the closed source as well as open source LLMs are made available through hyperscalers such as AWS, Azure or GCP, which alleviates some of the pain points mentioned above.

Some people might wonder whether we didn’t forget other aspects such as MLOps: data and model versioning, data and concept drift monitoring, and related model re-training (or LLM prompt updates), etc. These topics are however not differentiators: they are present in both approaches. While it is true that the traditional ML is already in adulthood, the currently teenage-like LLM domain is growing up fast and topics such as the LLMOps, enterprise vendor diversity and high-quality open-source alternatives are catching up very fast.

To answer the introductory question, “Does traditional NLP still have a place in modern world?”, we believe the answer is “Absolutely!”. We foresee a great future for LLMs for KGs, and not just for KG building, but we believe there remains and will remain substantial space for other technologies, especially considering security requirements, costs (massive datasets), and streaming scenarios (prediction speed). If you cannot use LLMs for whatever reason, consider leveraging them to increase the efficiency of your data annotation process by doing pre-annotation based on quick prompt engineering.

5.4 Summary

Producing Knowledge Graphs from unstructured textual data requires the identification of custom domain-specific named entities and relations among them. A whole range of traditional NLP models, such as Named Entity Recognition, Relation Extraction, or Coreference Resolution, have to be trained and chained in a complex workflow. Training each of them requires a high initial investment for building high-quality human- annotated training datasets, and specialized data science skills to train, evaluate, deploy, and monitor the models. Large Language Models, such as OpenAI’s GPT, can be used out-of- the-box with iterative prompt engineering (or alternatively fine-tuning) to build domain-specific Knowledge Graphs of high accuracy with much lower initial costs compared to the traditional NLP. Traditional NLP and LLMs both have their pros and cons, depending on the concrete domain and various requirements and business considerations. They can even coexist side by side, or one can be used to

prepare a training dataset for the other. The generative nature of LLMs means that we get cleaner more accurate KG with minimized need for entity normalization and even resolution.

5.5 References

1. Chernow, Ron (May 5, 1998). Titan: “The Life of John D. Rockefeller

Sr.,” New York: Random House. pp. 563–566. ISBN 978-0679438083. 2. A. Vaswani et al., “Attention Is All You Need,” 2017,

arXiv:1706.03762 [cs.CL] 3. J. Kaplan et al., “Scaling Laws for Neural Language Models,” 2020,

arXiv:2001.08361 [cs.LG]

## 6_Building_knowledge_graphs_with_large_language_models

This chapter covers

Transforming an archive into a Knowledge Graph: the Rockefeller Archive Center example Graph modeling Normalization, cleansing, entity resolution Analysis of the intellectual network

In the previous chapter, we discussed the topic of extracting complex relational knowledge from unstructured data using state-of-the-art Machine Learning technologies, including the Large Language Models. Specifically, we looked into knowledge extraction from the historical typewritten documents of the Rockefeller Archive Center (RAC). These documents contain very detailed descriptions of conversations that the representatives of the Rockefeller Foundation (RF), so called program officers, held with various researchers from a wide range of universities and other institutions. Based on the information they collected during the various meetings, they made a decision whether to recommend given research project for funding or not. In final stage, the Board of Directors of the RF approved the grants. For detailed description of the RAC use case and the goals of the project, please revisit the RAC discussion in the previous chapter.

Figure 6.1 Path from domain-specific unstructured textual data towards KG insights. Each of the key steps relies on state-of-the-art Machine Learning models, be it for example Optical Character Recognition for document digitization, NER and Relation Extraction systems, Entity Resolution and Graph ML.

As highlighted by the mental model in Figure 6.1, we shall start where we ended in the previous chapter, i.e. with the knowledge extracted from the textual documents, and explore how to transform it into a Knowledge Graph and how to leverage the KG to the benefit of our organization.

6.1 Transform an archive to a Knowledge Graph

Previously, we discussed specific NLP tasks – Named Entity Recognition and Relation Extraction. However, we faced many other challenges in this project. The overview of the outstanding ones is summarized below:

Analog typewritten documents. Historical documents. Uncommon linguistic conventions. Domain-specific named entities. High relational complexity of the knowledge within. KG normalization, cleansing, entity resolution, and disambiguation. Matching / linking two unstructured data sources.

Given the analogue nature of the documents, they first had to be scanned and processed by Optical Character Recognition (OCR) system to produce digital textual corpus. There are various OCR technologies out there ready to be used, and although they are out-of-scope of this book, we can point the curious reader to cloud providers such as Amazon or Microsoft who both have advanced ready-to-use AI services such as OCR. For the adherents of open-source software, we can recommend for example Tesseract OCR library.

The historical nature of the documents can be a challenge because many of the research disciplines (occupations) discussed are not pursued anymore – they turned out to be dead ends. Therefore, there is no hope of ever compiling a comprehensive enough NER dictionary or knowledge base that could be used as a reference for disambiguation and entity resolution.

The officers also had very specific writing styles, as already briefly discussed in the prompt engineering section. For example, some of them had the habit

of referring to people by abbreviations such as “S.” instead of “J. R. Smith” or writing “U.Cal.” instead of “University of California”. For the person names, none of the off-shelf coreference models we tested was able to handle that. Using GPT for named entity recognition and relation extraction tasks, however, significantly reduces the need for custom coreference model since it is fully capable of doing this task implicitly when dealing with our prompts (or with training data in case of fine-tuning).

The domain specificity is always an important factor. In our case we deal with descriptions of research projects in natural sciences domain. The main custom named entities are thus what we called by an umbrella term Occupation in the prompt engineering chapter, and include especially research disciplines, technologies, treatments, and diseases. Not only that these are entities for which there is no traditional NER model available (except diseases), but they are also of varied granularity. For example, some diary entries or Board of Directors minute books entries might refer to high- level research topics such as “meteorological research”, while others might mention a very specific sub-discipline “polar air masses”. The only option is to do proper custom ML-based NER and then design unsupervised entity resolution system that will help to cluster semantically similar occupations and thus allow further downstream analyses.

There is also a very high density and complexity of relational knowledge locked within these documents. Even a fairly short single page can contain tens of very relevant relations – such as who talked about/with whom, what they are working on, what is their title and university affiliation, what date they were talking on etc. It is important to define the RE schema properly, so we don’t spend time on retrieving useless knowledge while potentially missing opportunities to get higher accuracy for what is essential. This is especially important when going the traditional way, that is training RE models by labelling manually large amounts of data: based on our experience with such projects, the fewer the merrier from the point of view of producing high quality human-labeled dataset for all the defined entities and relations.

Even once we overcome these challenges, and the readers won’t be surprised at this point why we chose the LLM approach and not the traditional NLP one, there is still significant effort remaining in terms of KG normalization,

cleansing, and entity resolution and disambiguation (think of all the variants of people names, such as “Bohr”, “N. Bohr”, “Niels Bohr” – each diary entry might be using a different variant, but they are all the same person). Or to do further downstream analysis, such as identification of conversations the officers held: thanks to the RE, we know who talked with whom, about what, on which day, which is a good definition of a Conversation (new node in the KG), but some conversations are much more complex, such as having multiple participants who speak about their shared research in semantically similar, yet different, wording. Moreover, the conversations are often part of whole follow-up conversation chains – they typically talk multiple times over the months or even years before the officer is confident that this research deserves a grant. Unfortunately, this kind of analysis is out of the scope of this book, where we will focus on producing and analyzing people interaction networks.

Finally, there is the challenge of reconciling (matching) knowledge extracted from two different textual data sources, Officer Diaries and Board of Directors minute books, in a single KG, so that we can identify which conversations resulted in which grants and therefore close the circle, which allows us to answer questions such as “Are there any patterns that typically precede the funding of an idea?”. Again, this is out of the scope of this book, but it serves as an illustration of various possibilities that lie ahead when we start leveraging unstructured data sources.

6.1.1 Graph modeling

The KG schema of the Rockefeller Archive Center project is in Figure 6.2. It is simplified for the purposes of this book since describing and reconstructing the whole project in all its detail and glory is way out of the scope of this book. Here, we will focus on one aspect of the overall KG – the so-called influence network, i.e., relations among people interviewed by the Rockefeller Foundation representative such as who talked with whom about whom and so on.

We decided to design the graph in three layers:

Document-level Layer: the result of initial data ingestion, where each

officer diary file is represented as a File node with properties such as file name, its location, or author, and each Page node extracted from each file by the OCR model with properties such as the final clean text of the page. Meta-graph Layer: consists of unmerged GPT entities (Entity nodes) and relations among them, as well as their linkage to the original page from which they were extracted. The Knowledge Graph Layer: final normalized cleansed resolved entities (Person, Title, Organization, Occupation) and relations among them (WORKS_ON, WORKS_FOR etc.).

Figure 6.2 Simplified Knowledge Graph schema of the Rockefeller Archive Center project.

As we discussed before, the LLM does its best to produce the desired output, but it cannot be used to produce a KG directly. First, normalization and entity resolution steps, as we shall see below, need to occur, which is enabled by this schema setup. You can think of this schema as an onion of three layers: the inner-most layer being metadata about the original files, the middle layer

being the meta-graph of GPT-identified entities and relations without any post-processing, and finally the outer-most layer – the shiny brand new Knowledge Graph ready to be utilized for further downstream analytics or served as a visualization to the final (typically non-technical) users, who don’t really care about normalization, entity resolution and such. They just need to see and interact with fully resolved and normalized entities (only one “Niels Bohr” node) and relations, and all the lower layers are typically hidden from them by the proper configuration of your graph visualization software.

6.1.2 Data processing and meta-graph creation

Altogether, we had over 10,000 pages in this project, but for the purposes of this book, we selected a small subset of 150 pages of Warren Weaver’s diary from the year 1939. You will find the OCRed dataset and full ingestion code in our GitHub space[1]. Let’s just discuss briefly how we create the meta- graph.

When dealing with texts longer than few pages, the first step is to define a chunking strategy to avoid running into max tokens limit or the deterioration of processing quality for very long texts. Here we show the simplest approach – splitting by page. After each page is processed, we create (not merge!) all the identified entity mentions (nodes called Entity in the schema with property name), link them to their page, and create also their relations extracted from the given page (called RELATED_TO_ENTITY with type property representing the relation class). This allows us to do normalization and entity resolution based on knowledge about each entity mention, and then when creating the final KG, we can easily aggregate all the knowledge extracted about all the resolved entity mentions across all the pages while preserving the underlying information about the origin of these entities and especially relations. That will allow us to design the graph visualization platform in an easily explainable manner by being able to show – on demand – all the original text snippets from which the selected entities and relations were identified. It is also useful for data scientists maintaining the whole KG system to be able to easily track the origin of nodes and relationships in the graph and, for example, fine-tuning the entity resolution whenever needed, since the final KG can be easily recreated anytime from the meta-graph.

6.1.3 Normalization and cleansing

Once the meta-graph is created, we can start to explore it and inspect basic statistics such as top entities per class, top relation classes etc. This gives us a quick overview of the structure of the knowledge within, and whenever we notice an opportunity to increase the connectivity of the future graph, we shall implement it. It can be simple as lowercasing those entity names where the case is irrelevant, such as the Occupations, since there’s no practical difference between “cyclotron” and “Cyclotron”. This type of normalization is important for ensuring data (KG) consistency and efficient integration across different documents. Similarly, we noticed that GPT occasionally included a person’s title as part of their name, even though it was instructed to treat them separately. That’s why we implemented a cleansing strategy that strips irrelevant tokens from people names, thus avoiding having the same person represented in the KG by multiple nodes (once with and once without the title). Let’s write simple Cypher queries to fix those for us, see Listing 6.1.

Listing 6.1 Normalization of persons and occupations.

from neo4j import GraphDatabase

URI = "bolt://localhost:7687" AUTH = ("neo4j", "password") NEO4J_DB = "neo4j"

REMOVE_TITLES = ["dr.", "prof.", "dean", "president", "pres.", "s ➥ "mr.", "mrs."] QUERY_NORM_PERSONS = """ #A ➥ MATCH (e:Entity {label: "Person"}) ➥ WITH e, CASE WHEN ANY(title IN $remove_titles WHERE toLower(e ➥ STARTS WITH title) THEN apoc.text.join(split(e.name, " ")[1..] ➥ ELSE e.name END AS name [CA} SET e.name_normalized = name ➥ “””

QUERY_NORM_OCCUPATIONS = “”” #B ➥ MATCH (e:Entity {label: "Occupation"}) ➥ SET e.name_normalized = toLower(e.name) ➥"""

if __name__ == "__main__":

with GraphDatabase.driver(URI, auth=AUTH) as driver: with driver.session(database=NEO4J_DB) as session: #C print("Normalising Person names") session.run(QUERY_NORM_PERSONS, remove_titles=REMOVE_TITLES print("Normalising Occupations") session.run(QUERY_NORM_OCCUPATIONS)

As you can see, we create new node properties called name_normalized, which will be later used instead of unnormalized name properties when linking to final nodes in the KG layer.

6.1.4 Graph-based entity resolution

The generative nature of Large Language Models is of great help when it comes to producing cleaner more accurate Knowledge Graphs. One of the reasons is that, unlike the traditional NER and RE models, they are able, if properly guided by the prompt or fine-tuning, to return only full clean entity names and not all their mentioned variations. Even though the full name, “Eleanor Smith”, is mentioned only once in the given document and then she’s referred to as “Eleanor”, “Smith” or simply “she”, LLMs are able to understand from the context that all these mentions refer to “Eleanor Smith”, and therefore all the extracted relations use this full version. You can think of it as that coreference resolution, one of the key tasks in the standard NLP pipelines, is performed implicitly. That greatly helps to produce cleaner more normalized KGs out-of-the-box, reducing the need for entity resolution.

Reducing, but not removing. We still need to be able to resolve entities across documents: is “Eleanor Smith” mentioned in one document the same person as “E. Smith” in another? The key part of any KG creation process is, therefore, entity resolution, the ability to judge that two names are the same subject (person, company, etc.), or even entity disambiguation, the ability to link each subject to a concrete concept in some knowledge base such as WikiData.

We propose to take advantage of the graph structure and design a graph- based entity resolution system. The deeper discussion is unfortunately out of the scope of this chapter but let us at least outline the general approach and start with an initial baseline. We shall take full advantage of the fact that we have a highly accurate reliable Relation Extraction system in place. Most of

the entity mentions in the meta-graph layer have one or even more relations to other mentions, most of them highly useful for entity resolution. Think of the WORKS_FOR relations – if there is a high level of string similarity between two names (“Eleanor Smith” and “E. Smith”) and both work for the same university, that’s a strong signal. Similarly WORKS_ON relations – it is unlikely that people with identical or very similar names work on the same research topic. Not to mention cases when we manage to amass multiple signals of this type – the confidence of such resolution grows even more.

Although based on the previous paragraph it might seem that doing this resolution is a matter of performing one-hop checks (which can be easily done in any relational database), this is far from true. The reality is a bit more complicated. For example, this approach relies on the initial linking of nodes in the meta-graph layer based on their string similarities. We link two nodes with META_SIMILAR relationships if they have identical or very similar string representations. We also take advantage of domain knowledge when defining similarity thresholds or rules. For example, we know that a person’s name is composed of first name, middle name(s), and surname. On very formal occasions, their full name is stated, but at other times middle names are abbreviated or even skipped, the same is true for first names. On the other hand, relying only on surnames would result in lots of false positives – it’s a combination of surname and at least the first name (or its abbreviation), that gives us higher confidence that they could be the same person. Given all these considerations, we can easily create a set of rules for creating META_SIMILAR edges between nodes representing mentions of person names.

Similar reasoning can be applied to other entities, for example, Organizations. In one document, we see “Carnegie Institution”, while in another it says “Carnegie Institution of Washington”. They are clearly similar enough for our purpose, but beware: sometimes generic words appear among Organization names, such as “Foundation”, where from the overall context (documents from the Rockefeller Foundation) it is clear which concrete foundation it is, but there are also many other foundations with this keyword in their name, in which case it would be counter-productive to create similarity links among them. It is therefore important to analyze the situation and define a stop-words list before creating the SIMILAR relationships.

Different datasets and domains might require a slightly adjusted approach. What works well in our use case might not be enough in sensitive mission- critical domains, such as law enforcement, where not only do you have a different set of entities and relations, but you might also encounter a requirement for more stringent decision criteria. You might design a complex weighting scheme (each edge type having assigned different weight) because, for example, if you manage to relate two similar mentions to the same Social Security Number, this has much stronger significance than when they work at the same company.

Enough of hypothesizing, let’s examine a concrete case in Figure 6.3. We see three different mentions of nuclear physicist Ernest Lawrence, who, by the way, won the Nobel Prize for the invention of the cyclotron, on pages 26, 99, and 126: Ernest Orlando Lawrence, Ernest O. Lawrence, and simply Lawrence. How do we know that these three names refer to the same person? There is a strong string similarity among these based on the rules and patterns we discussed above, so the first tick box is checked. Then we see that Ernest Orlando Lawrence and Ernest O. Lawrence are three hops away since in both cases they were identified as employees of the University of California. Similarly, we can find a relation towards the last mention, Lawrence, through the WORKS_ON relationships and due to the similarity between Occupations “cyclotron” and “100,000,000 to 200,000,000 volt cyclotron”. By the way, notice that Ernest Orlando Lawrence and Lawrence are 6 hops away. This kind of traversal would be much harder to do in relational databases.

Figure 6.3 Graph-based Entity Resolution example.

Would you like an even more “graphy” approach? How about taking advantage of the intellectual network relations – TALKED_ABOUT and TALKED_WITH – and running graph community detection algorithms, such as Louvain, in order to identify clusters of people who interact or in general are connected to each other? The reasoning behind is that John Doe working on maritime research in Antarctica will quite probably be part of a different interaction network than John Doe specializing in cosmology. Membership in communities would give us another signal, another evidence for deciding whether they are the same person or not.

How about adding machine learning? One obvious option is to design a semantic similarity approach to identifying similar Occupations, so that we can create the META_SIMILAR links even when there is zero string similarity, yet the topics are highly related, for example “fertility” and “human ovulation”. A frequent choice would be to use embeddings, for example provided by GPT, and then cluster them based on their similarities (we observed very good results with agglomerative clustering approach).

As you can see, there are many options how to design graph-based entity resolution, we just scratched the surface. To conclude our baseline approach outlined above for resolving people (without considering the interaction network and semantic entity similarities, which are out of the scope of this chapter), we need these last few ingredients:

Create META_PERSONS_SIMILAR relationships among person mentions that comply with the above-defined criteria (string similarity and relatedness through RE outputs). Run the Weakly Connected Components (WCC) algorithm on the META_PERSONS_SIMILAR meta-graph to identify groups of mentions that should be resolved to the same KG entity (final graph layer). For each WCC group, select a common name to represent it: we choose the longest one (in the example above, that means “Ernest Orlando Lawrence”) Create the final KG layer with fully resolved entities.

These tasks are straightforward, see our code repository for full flow.

6.2 Intellectual network analysis: the value of graphs

We finally reached the goal – a Knowledge Graph. What now? How can it be utilized? At this point in the book, the general value of KGs should be clear. We shall therefore explore here one specific analytical aspect –graph data science.

Certain specific parts of the produced KG are clearly highly suitable for graph analytics, especially the so-called intellectual network of people (scientists) formed by TALKED_ABOUT, TALKED_WITH, WORKS_WITH, and STUDENT_OF relations. Neo4j’s Graph Data Science Library was used to analyze this network using various graph algorithms such as PageRank, Eigenvector centrality, node degree, or betweenness centrality in order to identify influencers, people who stand out in terms of recommending other people’s work, “influencees”, popular targets of other people referrals (or professional gossip), or bridges, that is those who act as connectors among different communities of people. Different visualization styling options can be designed to help guide one’s eye in exploration and analysis to acquire insights.

Figure 6.4 is the largest connected component of the extracted intellectual network with styling based on betweenness centrality, where nodes are the bigger the more shortest paths among any pair of nodes that pass through them. This styling thus highlights people acting as connectors, or bridges, among different sub-graphs representing groups of researchers who’d otherwise be very loosely connected or even disconnected. Not surprisingly, famous scientists such as Niels Bohr (the father of atomic physics) and Ernest Lawrence (the inventor of a particle accelerator called cyclotron) are among them, so even though these two specifically do not come as a surprise, they are the usual suspects when thinking about influencers and now we have it clearly confirmed by the data. More interestingly, less famous people get highlighted, which starts bringing additional – potentially surprising - insights worth investigating by the analysts.

Figure 6.4 Intellectual influence network made of relations TALKED_ABOUT, TALKED_WITH, WORKS_WITH and STUDENT_OF. Node styling is based on their

betweenness centrality score.

We can also make use of the intellectual network for answering more concrete focused questions, for example: “Who played an important role related to the cyclotron research and its funding?” This question can be answered by a simple few-hop query, see Listing 6.2.

Listing 6.2 Show influence network related to the cyclotron research.

MATCH path = ()<-[:WORKS_ON|WORKS_FOR]-(p2:Person) ➥ -[:TALKED_ABOUT|TALKED_WITH|WORKS_WITH|STUDENT_OF*1..2]-> (p:Person)-[:WORKS_ON]->()-[:SIMILAR_OCCUPATION*0..1]-(o:Occupati WHERE o.name = toLower($occupation) AND ➥ NOT ANY(x IN nodes(path1) WHERE x.name = "WW") RETURN path

The graph representation of the 2-hop influence network neighborhood of the cyclotron-related research is shown in Figure 6.5, along with additional information such as occupations and university affiliations of the people. The PageRank centrality calculated on the full influence network graph was used to scale the nodes. We see that besides Ernest Lawrence and, again not too surprisingly, Niels Bohr, there appear other important people in the very close vicinity, for example, Harlow Shapley, astronomer and head of Harvard College Observatory, or James B. Conant, organic chemist and 23rd President of Harvard University, where they also later built a cyclotron, which president Conant later sold, after secret negotiations with General Leslie Groves, for $1 to the U.S. government to help the development of the first nuclear bomb.

Figure 6.5 Influence network in the 2-hop vicinity of the cyclotron research. The person node sizes are based on global PageRank centrality to highlight popular nodes in the graph.

There is also an element of surprise. Notice the presence of Laurence Irving (the first name mis-spelled by GPT as “Lawrence”, confusion apparently caused by presence of Ernest Lawrence on the same page), a pioneer in comparative physiology. Is it possible that he, without doubt, a very important person in his field of research, played a role in the influence network of cyclotron invention? That would be quite a surprise. A closer look reveals, however, that this is an example of a failure in the Relation Extraction task, he should not appear in this sub-graph. That is an important lesson for us. Large Language Models are no magic boxes, they do make mistakes occasionally, and sometimes even very silly ones. Even in present times, RE remains a challenging task. It is therefore important to design a feedback loop in your KG applications, so that the analysts can, whenever needed, validate or invalidate the content of the graph.

Let’s conclude this section with one more example. Imagine a situation where a person with direct experience and know-how of a particular domain, in our case the Rockefeller Foundation’s officer Warren Weaver, leaves their post and someone new comes in. He or she might be asked to handle a project that spans more than one university, for example, Johns Hopkins University and Harvard University. Where to start? Who is the right person to approach first? It is a physics research project, therefore what is needed is to sound out to someone informally about it, ideally a person with likely exposure to both the physics domain at Johns Hopkins and Harvard universities. The question can be answered by inspecting the relevant part of the influence network: we take all employees of both universities (WORKS_FOR relations) and search for any connections among them (relations TALKED_ABOUT, TALKED_WITH, WORKS_WITH and STUDENT_OF) with, say, up to three hops, where at least one of the persons works on physics research. The result is in Figure 6.6.

Figure 6.6 Physics research influence network connecting Johns Hopkins University and Harvard University with no more than three hops among persons. The green and red relationships indicate positive and negative sentiments.

If it was a bigger network, we could run a betweenness centrality algorithm on this sub-graph to help us identify the important connectors, but in this case, it is a simple task. There are only a handful of people who stand out at the first look as useful connectors between the two universities. A good start could be Irving Langmuir, chemist, physicist, and engineer who won the Nobel Prize in Chemistry in 1932, who talked positively about Dorothy M. Wrinch studying insulin and protein structures using X-rays, and who has direct 1-hop link to both universities Notice, by the way, that we have two

nodes representing this scientist: once we have his full name, another time only his surname was mentioned at a page without any additional relation that could be used in our entity resolution approach discussed above. Moreover, there is a certain Bernal who has a negative attitude toward Dorothy Wrinch, but Irving Langmuir has a negative one towards him in turn. If we’d like to obtain a balanced insight into the topic, we might want to interview both.

In this section, we demonstrated a few options for utilizing Knowledge Graphs for further downstream analyses and “question-answering”. Of course, there are many more, this was just for inspiration. We wanted to show that KGs and LLMs can seamlessly complement each other. And not just when it comes to the KG creation … but that’s for another time.

6.3 Next steps in the Rockefeller Archive Centre project

The results presented in the previous section are based only on about 150 pages of text yet look at how impressive and complex the KG is. There is indeed a lot of relational knowledge locked within those textual documents. The whole dataset has about 10,000 pages, and that’s just one decade's worth of data only from the natural sciences domain. The Archive holds much more data than that, and also of other foundations, so this is still a small yet significant leap forward because once the processes and flows are defined, they can be easily extended to accommodate more data with minimal human labor.

What we presented in this chapter is the mental model and guidance for the most important steps that need to happen when harvesting the content of any textual data. In the full production-quality project, more needs to happen, especially:

Improve knowledge extraction: More iterations on prompt engineering or moving to fine-tuning the LLM to improve the accuracy of the KG. Handle multi-page documents: Each diary has hundreds of pages, but there is a token limit of how much data we can send to and retrieve from the LLM (typically few pages worth of text), what do we do? Especially

for cases when the knowledge (such as full person name or first part of the relation) is at one page and the rest of a diary entry is at another page? We could design a sliding-window approach (process 2 pages at a time, then shift the window by one page), give the LLM previous page only for context and ask it to extract entities and relations from the current page only, or chunk the document in different way than by the pages. In RAC project, we used ChatGPT-3.5-Turbo to identify boundaries of individual diary entries, which typically have less than 3 pages, and process those in one go. Entity Resolution: Improve the baseline approach presented in this chapter, expand it to other entities, and potentially complement it with Entity Disambiguation against WikiData or similar knowledge base. Add grants: Board of Directors Minute Books is an additional unstructured data source which contains valuable details about grants awarded by the Rockefeller Foundation – how much was awarded to which university for which project. Mining this information and designing an approach to link the grants to the conversations in officer diaries will unable to use the KG to answer questions such as “Are there any patterns that typically precede the funding of an idea?” or “Do granted projects tend to run through recommendations of influential scientists or previous grantees?” Entity Resolution of Occupations: The Occupations are named entities unlike any other in the KG. They have varied granularity, for example we have “nuclear physics”, “isotopes” and “heavy nitrogen”. Both “isotopes” and “heavy nitrogen” are part of “nuclear physics”. Similarly, “heavy nitrogen” is an isotope of nitrogen. Depending on the document (diary entry or grant information), any of these could be mentioned, yet in order to be able to answer complex questions such as what happens before a grant for studying isotopes is approved, we need to be able to link (resolve) them. This will give us access to whole history of the given topic, regardless of whether it was discussed as studying concretely heavy nitrogen or any other isotope. This is especially important when linking the officer diaries with grants, since the grants are usually described on high level, such as that certain amount is awarded to specific university for nuclear research to study isotopes. The approach that gave us best results so far is to create embeddings of occupations (using SentenceBERT or our good old GPT)

and cluster them using agglomerative hierarchical clustering, which has two main advantages: it is unsupervised, so we don’t have to train models based on human-annotated dataset, and it allows us to choose the clustering granularity, for e.g. whether we shall link all “nuclear physics”, “isotopes” and “heavy nitrogen” (bigger clusters), or just “isotopes” and “heavy nitrogen” (smaller more focused clusters). Create Conversations: Final key ingredient in this project is to create Conversation nodes. This can be achieved by leveraging high-quality relation extraction and other information: a conversation needs to have a date, interviewer, interviewee(s) and a topic (occupations of the interviewees). Once we have conversations, we also identify their whole follow-up chains and link them to grants. Both tasks are achievable thanks to the unsupervised resolution of occupations.

We reached a moment when the whole extent of the RAC project is clear. Bleeding-edge knowledge mining from historical typewritten documents, reconciling two different unstructured data sources, various entity resolution systems, graph modelling, graph data science, complex visualization, and styling. It’s challenging, it’s tough, and most importantly – it’s fun!

6.4 Knowledge Graphs value in the LLMs era

Some of our readers might be wondering why, in the era of the super- powerful LLM models (some even call them AI), we should still strive to build Knowledge Graphs instead of just feeding these models our data and asking questions directly, without any intermediate steps. The answer to this question is complex and this whole book, as a testament to Knowledge Graphs, is the big answer. However, let’s provide here at least the “smaller” answer, related directly to this chapter. It can be summarized in the following points:

Explainability: Applications based on well-designed Knowledge Graphs have a huge advantage in being natively explainable. They always have the ability to provide users with the tools to inspect and verify the underlying data and reasoning that led to the observed insights or can be configured to handle appropriately (according to use case requirements) conflicting sources of information, while providing the

whole chain of “thought” when required. Demystification / de-blackboxing: Many advanced ML models are often perceived with caution by people since they see them as black boxes that they are supposed to blindly trust. This is especially an issue when it comes to the LLMs. If we tried to obtain answers simply by fine-tuning an LLM model on our dataset and asking it questions, we would not have a way to assess the confidence of such responses. Also, remember the prompt engineering discussed earlier and how hard it was to explain to the model the various specificities in the dataset (such as that “R.” refers to “C. G. Rossby”): could we really be sure that the “AI” won’t miss a crucial part of the information in our data if left on its own? Instead, leveraging the language understanding power of these models in order to extract specific factual information from concrete documents to produce a KG, is the intermediate step that allows us to build confidence in the generated insights. In a way, Knowledge Graphs can be considered helpful in “de-blackboxing” the LLMs. Democratization: It is well known that the LLMs are massive beasts that are expensive to train and fine-tune. We can think of KGs as one of the ways to democratize their usage, so that even organizations without massive funds can profit from them: we can use the expensive model only once to produce the KG, which we will then keep using for a long time (perhaps with occasional cheap batch updates) for various downstream tasks and analyses. Explorability: Graphs are a great natural way to allow users to view and touch their (relational) data from completely new angles. They provide global views as well as concrete drill-down investigations. Finally, KG visualization and explorability incite people to hypothesize, and provide them with the ability to easily verify or disprove their theories. Advanced analytics: Perhaps the most importantly, KGs empower data scientists and analysts to perform various downstream graph-based analyses and Machine Learning, allowing them to have full control over the generation of answers to user questions, contrary to the black box approach when we would rely solely on an LLM. Think back to our network analysis such as the identification of influencers or bottlenecks: would GPT really be able to give us correct answers, just by “reading” the articles? How would we even communicate to it what it means for us

“influencer” or “bottleneck”? Yes, there can be multiple definitions, depending on from which angle” we look at the problem, and having control over it is what ensures the quality of the generated insights. And this example was just a simple graph algorithm problem. The reasoning is even stronger for more complex ML tasks, for example temporal graph analysis where we’d like to shed light on how the influence network or the granted topics evolve over time.

6.5 Summary

Knowledge Graph schema design, a.k.a. graph modelling, is a key step that ensures the information is stored in the most optimal way for the specific use case at hand. Well-designed schema simplifies the KG creation, entity, and relation cleansing, normalization, and resolution, as well as ensures efficient downstream analyses and insights discovery optimized for the specific use case. Extracting high-quality entity relations from textual documents helps to design an unsupervised graph-based entity resolution approach. Various graph data science and graph Machine Learning techniques help to analyze patterns and draw insights from Knowledge Graphs, which serve as an intermediate step towards answering complex questions without relying too heavily on black-box tools such as LLMs.

[1] https://github.com/alenegro81/knowledge-graphs-and-llms-in-action

## 7_Named_entity_disambiguation

This chapter covers

The key ideas of named entity disambiguation combined with knowledge graph technologies Building a knowledge graph from multiple sources Performing advanced analysis

Natural Language Processing (NLP) techniques play a critical role in the automatic construction of knowledge graphs (KGs) from unstructured data. A key task in this process is Named Entity Recognition (NER), which is essential for identifying mentions of relevant named entities in raw text. NER assigns these entities to predefined categories such as people, organizations, locations, or diseases. While NER is an important component in building KGs, it offers a necessary but incomplete contribution to achieving a precise understanding of text within a specific application domain.

Imagine developing an Intelligent Advisory System (IAS) to support the diverse activities of stakeholders in the healthcare field. One of the critical attributes of such IAS is interactivity, which is the practical ability to exchange information with humans effectively through multiple interactions.

A subset of the essential features to enable this exchange includes:

1. the capacity to detect meaningful entities in natural language; 2. the ability to retrieve information on these entities from different

knowledge sources;

NER inference is not able to fully cover these specific features. For example, consider the following paragraph from a weekly bulletin released by the European Centre for Disease, Prevention, and Control (ECDC) [3].

“In the week of 13 April, Belize reported for the first time mosquito-borne Zika virus transmission. Update on the observed increase of congenital Zika

syndrome and other neurological complications Microcephaly and other fetal malformations potentially associated with Zika virus infection.”

The term “Zika” appears three times in this short text. However, its meaning changes depending on the surrounding context. Assuming we can achieve high accuracy for this specific task, the output of a NER model will identify that the first instance refers to a “virus.” In contrast, the second and third instances refer to a “disease” entity. In this case, do they represent the same disease? Apparently, yes, because they both mention the term “Zika.” Nevertheless, we must be as accurate as possible in critical domains such as medicine. For some use cases, it is crucial to distinguish between the traditional Zika disease or infection (the third mention) and its congenital form (the second mention). Disambiguating such terms could be straightforward for a domain expert, but it becomes impractical when the volume of documents increases. Moreover, our IAS system can provide contextual information about the identified entities once these terms have been correctly disambiguated. For example, it is useful to know that the “Zika virus” is a mosquito-borne flavivirus and, as a further step, we can access all congenital diseases caused by a flavivirus.

Both these problems are addressed by another NLP task: the Named Entity Disambiguation [6] (NED). The goal of the NED task is to automatically remove the uncertainty or ambiguity of the meaning of “Zika” by examining the context of the mention and connecting such mention against a distinct entity in a knowledge base. An example of this ground entity and its related reference knowledge base is represented by the “Congenital Zika Virus Infection” located in the Unified Medical Language System (UMLS) [1]. This entity is different from “Zika Virus” and “Zika Virus Infection”, which are both specified in the UMLS knowledge base. Figure 7.1 shows a screenshot from the UMLS website providing details about the “Zika Virus” entity.

Figure 7.1 Screenshot from the UMLS website showing information about the “Zika Virus” entity. This information includes the name, the id (CUI), the semantic type, and a set of definitions, broader concepts, and narrower concepts.

By enabling the mapping with the “Zika” expression in the raw text, we explicitly satisfy the first requirement of IAS related to detecting meaningful entities, in our specific case, a UMLS entity. But that is not all. Through this mapping, we implicitly created the conditions necessary to satisfy the second requirement of the IAS, which involves retrieving further information from various knowledge sources. Starting from the detected UMLS entity, we can explore the contextual knowledge located in multiple biomedical ontologies

related to the extracted entities. Medical ontologies allow you to retrieve, for instance, the connections between the disambiguated entities mentioned in our example. In some cases, they explicitly define a trivial relationship, specifying that the “Zika Virus” is the “CAUSATIVE_AGENT” of both syndromes. In other cases, there are less trivial relationships: for instance, two diseases such as Campylobacter infection and AIDS can be connected because the first affects the intestinal tract like a specific form of the second one, known as “AIDS with intestinal malabsorption.”

This chapter demonstrates how to apply NED models to textual content in the healthcare domain, facilitating the interlinking step through multiple ontologies. Building on our concrete experience with the European Commission, we will demonstrate how to leverage these principles to automatically construct a real-world case knowledge graph (KG) that can be effective in a specific scenario. The scenario considered is the development of an IAS designed to support the regulation at the European level on standards of quality and safety for substances of human origin intended for human application. An automatically built KG is beneficial because it allows you to extract and integrate into a unified and holistic view information from multiple technical documents in a context where information extraction and harmonization promptly is fundamental.

More precisely, the chapter is structured as follows. It presents high-level ideas of the NED model, setting a warm-up example for disambiguation and interlinking with biomedical ontologies. Regarding the business domain in which NED and KG features are applicable, the chapter introduces a real- world scenario related to the support of regulators in the SoHO domain. After describing context details, the chapter presents the documents and the ontologies involved in the process. Finally, we explain how to concretely build the KG and apply it to the proposed use cases.

Exercise

The following exercise is proposed to stimulate the reader to identify different perspectives of NED. In the Zika example, we have seen that a single term can refer to distinct entities based on the surrounding context. However, the opposite situation may also arise in the medical domain: in

some cases, different terms can refer to the same entity. Can you spot an example (SUGGESTION: check the examples of medical entities mentioned in the chapter)? How does the KG become useful in modeling these cases?

7.1 From recognition to disambiguation

Knowledge bases play a crucial role in collecting a structured representation of entities within a specific domain. We already mentioned that the NER does not solve the uncertainty related to the mentions recognized as named entities. For this reason, to tackle this uncertainty, the goal is to link mentions within the text to the correct entities in the reference knowledge base. This linking step is enabled by NED systems, which usually include three main phases:

1. the candidates selection; 2. the candidates ranking; 3. the ontology integration.

Figure 7.2 shows the mental model of a typical KG-based NED system.

Figure 7.2 Architecture of the NED system including three main phases: (i) the candidate selection; (ii) the candidate ranking; (iii) the ontology integration.

The Candidates Selection aims to identify the best candidates for the recognized named entity mentions. This selection is performed against an existing knowledge base that includes useful structural information, enabling the precise identification of different entities. The second main phase, called Candidates Ranking, consists of assigning a score to each candidate based on

the contextual information, which involves the words surrounding the recognized entity. The entity that achieves the best score represents the target entity of the detected mention.

Considering the application domain, and for the sake of simplicity, this chapter will use a Python-based library called Scispacy [5] for disambiguating named entities. The model implemented in ScispaCy incorporates multiple features, including the recognition of named entities, the selection of various candidates against a knowledge base, and the ranking of these candidates to identify the target entity. An example of a Python script using the ScispaCy model on our running example is provided in listing 7.1.

Listing 7.1 Candidate selection and ranking with ScispaCy model

import spacy from scispacy.linking import EntityLinker

nlp = spacy.load("en_core_sci_md") nlp.add_pipe("scispacy_linker",

config={"resolve_abbreviations": True, "linker_name"

linker = nlp.get_pipe("scispacy_linker") linker_dict = linker.kb.cui_to_entity

doc = nlp("In the week of 13 April, Belize reported for the first

for ent in doc.ents:

if "Zika" in ent.text: print("Recognized entity:", ent.text, ent.start_char, ent.end_ print("Ranked target candidates:") for kb_ent in ent._.kb_ents: print('-', linker_dict[kb_ent[0]][0], linker_dict[kb_ent[0]][1

By running the code from listing 7.1, you retrieve a list of ranked candidates associated with it. The output is available in listing 7.2.

Listing 7.2 ScispaCy model results for candidate selection and ranking

Recognized entity: Zika virus 75 85 Ranked target candidates: - C0318793 Zika Virus #A

- C0276289 Zika Virus Infection - C4687930 Zika Virus Antibody Measurement

Recognized entity: congenital Zika syndrome 135 159 Ranked target candidates: - C4546023 Congenital Zika Syndrome #B

Recognized entity: Zika virus infection 268 288 Ranked target candidates: - C0276289 Zika Virus Infection #C - C0318793 Zika Virus - C4687930 Zika Virus Antibody Measurement

Each recognized entity is associated with a list of candidates ranked using the ScispaCy model score. The first result for each mention (see the annotations in listing 7.2) is the best one reached by the model, and you can notice that a different UMLS entity id has been associated with each mention of the “Zika” term. After this phase, we potentially obtain an annotated text where each detected entity is linked to the UMLS knowledge base. This result can be exploited to automatically create, for instance, links from contents located on a web page to a domain reference knowledge. However, we need a further step to leverage the extracted information from a KG-based perspective.

The Ontology Integration phase aims to incorporate the knowledge from domain ontologies where the structural and contextual information of the extracted entities is integrated into a unique KG. Let’s see how to apply these principles concretely. The UMLS provides key terminology, classification, and coding standards from multiple sources. This allows the creation of interoperable biomedical information systems where the vertical information available in each source can be easily accessed and explored starting from another source. To better understand how the UMLS collects this information, let’s look at a sample of the UMLS entity file in listing 7.3.

Listing 7.3 UMLS – Sample of the entity file

C0276289|ENG|S|L0388876|VC|S0517846|Y|A2985635|8552019|3928002 ➥||SNOMEDCT_US|PT|3928002|Zika virus disease|9|N|256|

C0276289|ENG|P|L13115709|PF|S16069662|N|A27369917||C128423 ➥||NCI|PT|C128423|Zika Virus Infection|0|N|256|

C0276289|ENG|S|L0392793|VW|S16069660|Y|A26676017||M000613823

➥|D000071243|MSH|ET|D000071243|Zika Fever|0|N|256|

In listing 7.3 we highlighted the most important fields of a UMLS entry for our ontology integration phase. From left to right, the highlighted terms are the entity id, the ontology, and the name associated with the entity id in the ontology. As you can see from the first entry, the UMLS id that represents the “Zika Virus Infection” entity is mapped to the id 3928002 from the SNOMEDCT_US [2] ontology, which defines one of the possible names of this entity (“Zika virus disease”). SNOMEDCT_US, or SNOMED, is one of the most comprehensive, multilingual clinical terminology, encompassing more than 450,000 concepts. It also provides a rich set of relationship types between these concepts, including interesting semantic connections from a clinical point of view, such as “CAUSATIVE AGENT “and “FINDING SITE“. Let’s look at two file samples related to the SNOMED ontology, including the SNOMED entities/relationships descriptions and the SNOMED edges (or triples). In listing 7.4, you can find a sample of the SNOMED descriptions file:

Listing 7.4 SNOMED – Sample of the descriptions file

84087010 20020131 1 900000000000207008 50471002 en ➥ 900000000000013009 Zika virus 900000000000017005

8552019 20020131 1 900000000000207008 3928002 en ➥ 900000000000013009 Zika virus disease 9000000000000170

367784012 20020131 1 900000000000207008 246075003 ➥ en 900000000000013009 Causative agent 9000000000000

Listing 7.4 highlights the key values within an entry of the SNOMED descriptions file. You can see the SNOMED id and the related name from left to right. The file includes both ids for entities and the relationship between the entities. In this sample, the first entry represents an entity (“Zika virus”), while the third entry represents a relationship (“Causative agent”). Listing 7.5 shows an entry from the edge file:

Listing 7.5 SNOMED – Sample of the edges file

769900023 20020131 1 900000000000207008 3928002 ➥50471002 0 246075003 900000000000011006 900000000000

From left to right, the highlighted values in listing 9.5 represent the triple elements, including the source id for “Zika virus disease” (3928002), the target id for “Zika virus” (50471002), and the relationship id representing “Causative agent” (246075003). Based on these examples, you can see that by incorporating external ontologies, the output of the NED model becomes the entry point for exploring and discovering more tailored information and creating the conditions to build connections between natural language contents. In other words, we can create a KG that allows us to bridge unstructured and structured knowledge in a unified view. Figure 7.3 shows an intuitive example of this KG, which includes all the previous information pieces.

Figure 7.3 Example of KG built integrating information from the text processed using ScispaCy model, the UMLS, and the SNOMED ontologies.

This chapter will show how to build a large-scale KG that integrates unstructured and structured content. This KG will be adopted to perform advanced analyses that are useful in a real application scenario related to the definition of standards and regulations by a policy officer in a crucial healthcare area associated with managing substances of human origin (SoHO). Figure 7.4 provides a mental model to illustrate the entire process described in this chapter, from business understanding to KG creation and

querying. As already discussed in other chapters of this book, this mental model represents a specification of the CRIPS-DM model adapted to KG, which was introduced in chapter 2.

Figure 7.4 The mental model describing the KG construction process as a specification of the CRIPS-DM model, from understanding the business goal to defining the KG queries that support the SoHO Policy Officer activities. The key steps in this model include the business understanding, the data understanding, the data preparation, and the KG creation and update.

Before starting our journey by introducing the business application, we recap the opportunities and the limitations of LLMs introduced in chapter 6 applied to the concrete case discussed in this chapter. LLMs represent a modern

alternative to what we showed before using ScispaCy. However, they still require to be combined with KG technologies for adding concrete value in this specific context.

7.2 Domain-based NED and LLMs

At the beginning of this chapter, we discussed how the capacity to precisely detect the concepts in the textual content is an essential feature of an Intelligent Advisory System (IAS) that supports regulators or decision makers to take the right decision, especially for critical domains. For this reason, we did a simple experiment to test the capability of ChatGPT, the most prominent application based on LLM technologies, to reach this goal. A screenshot of the ChatGPT application is available in figure 7.5.

Figure 7.5 Screenshot that includes the main features of the ChatGPT application.

Here is the basic prompt we have defined based on our running example[1]:

"In the week of 13 April, Belize reported for the first time mosquito-borne Zika virus transmission. Update on the observed increase of congenital Zika syndrome and other neurological complications Microcephaly and other fetal malformations potentially associated with Zika virus infection." Disambiguate all the medical entities you can detect.

The output from ChatGPT is the following:

Zika virus Mosquito-borne transmission Congenital Zika syndrome Neurological complications Microcephaly Fetal malformations

As you can see from this initial result, the model does not perfectly recognize that the first “Zika” mention refers to a virus entity, while the last refers to the virus infection. Moreover, considering the complexity of the medical domain, where we can use different expressions to identify the same entity and similar terms to identify distinct entities, we need to be confident that the model “understands” the meaning of this expression. From the NED perspective, we must check if these entities are correctly mapped against the proper knowledge base. For these reasons, we defined a further prompt to test this aspect:

“Can you assign UMLS ids to these extracted entities?”

The answer is the following: “As an AI language model, I am not able to assign UMLS ids to extracted entities. However, UMLS Metathesaurus is a biomedical and health ontological resource that provides normalized names and mappings to concepts in various biomedical ontologies. It can be used to map extracted entities to their corresponding UMLS concepts and obtain their unique identifiers (UMLS ids). This requires domain-specific knowledge and expertise in utilizing the UMLS resources”.

As you can notice from the response, the UMLS knowledge base is not currently incorporated in the ChatGPT model. Moreover, the last sentence of this response looks particularly critical: it suggests that we need to develop models characterized by domain-specific knowledge and expertise on the UMLS resources. In this chapter, by combining well-suited NED technologies and KGs, we try to provide a method that includes these specific features and that can be easily integrated into an IAS.

7.3 Business and domain understanding

This section describes a real application scenario related to the definition of standards and regulations in the management of SoHO. These substances include blood, tissues, cells, and organs that can be potentially used in various medical therapies. Treatments based on these SoHO save lives (e.g., blood transfusion in case of trauma), improve the quality of life (e.g., kidney transplants), and even help create life (gametes and in vitro fertilization). Figure 7.6 depicts the business understanding step.

Figure 7.6 The business understanding (step 1) is crucial to clarify the problems that we want to solve. This phase is not strictly related to the technical aspects, but it is fundamental for the next steps.

The KG technology can respond to specific requirements in this scenario through its unique features, such as representation flexibility and the harmonization from multiple sources in a unified view. After presenting the context, the section provides a set of use cases that can be addressed by

combining NED and KG technologies.

7.3.1 Context

One of the critical areas in the healthcare domain is the safety of millions of patients undergoing blood transfusion, transplantation, and medical-assisted reproduction. From the substance donation to the patient application, components such as Blood, Tissues, and Cells (BTC) are employed in different treatments across the EU. As recently reported by the European Commission, therapies based on these SoHOs have been massively adopted for various purposes, including:

life-saving and life-enhancing therapies: from 25 million units of blood transfused, especially adopted for surgery or trauma care, to 36,000 stem cell transplants for blood cancers; life-improving therapies: from 14,500 cornea transplants for restoring sight to 2,000 skin transplants for burn wounds and other types of injury; life-creating therapies: 940,000 in vitro fertilization cycles (IVF) and other medically assisted reproduction treatments.

To understand the complexity of this application domain, consider the various aspects related to this healthcare sector, as illustrated in Figure 7.7. From the donor evaluation, multiple dimensions can be analyzed, including procurement, quality criteria, distribution, traceability, and biovigilance.

Figure 7.7 The main steps involved in the SoHO supply chain. The central components of this figure describe the main steps of this chain, from the “Donor evaluation” to the “Application for the patients”. The edge components specify the characteristics of these phases. For the “Donor evaluation”, we should consider “General criteria”, “Donor characteristics”, “Exclusion criteria”. Among the “Applications for patients”, we can mention “Somatic cells”, “Encapsulated cells”, and “Gene therapy”.

Considering the initial step of the donation process, this BTC sector relies on the availability of citizens for donations. Such availability is significantly reduced during public health crises, such as the COVID-19 pandemic, due to disease outbreaks. At the same time, the BTC quality should be kept up to date with the continuous and incremental technology developments, which lead to new methods for collecting, testing, and processing SoHOs. Therefore, as new risks and technology trends will continue to emerge, it is fundamental to create a legal framework that is “effectively implemented, future proof, crisis resistant and agile enough” to provide appropriate

requirements continuously.

In 2022, the European Commission released a proposal for a regulation on the standard and the quality of SoHo for human applications to repeal Directives 2002/98/EC and 2004/23/EC. This proposal aims to ensure the safety of patients treated with SoHO therapies and protect them from avoidable and predic8risks. More in detail, this proposal underlines the need for enough flexible rules to accommodate new changes promptly, including new epidemiological treatments, novel technology for testing, or updated scientific evidence. Among the explored alternatives for specifying the technical rules, the best policy option identified by the SoHOs stakeholders is represented by a joint regulation built on the expertise available in different bodies, such as the European Centre for Disease Prevention and Control (ECDC) and the European Directorate for the Quality of Medicines & HealthCare (EDQM). These two agencies play a complementary role in releasing specific guidelines. Considering their core activities, these two bodies produce different types of documents. In many cases, the ECDC provides short reports of sudden outbreaks of communicable diseases caused by a virus, such as Ebola, West Nile Virus, or Zika Virus, for rapid assessment. The EDQM releases detailed guidelines and training programs on applying the recent discoveries for treating SoHOs in any phase. For this reason, the capacity to extract useful information and get contextual details from such multiple textual contents is fundamental to identifying rules and rapidly updating regulations that guarantee the safety of patients and protect them from avoidable risks.

The mission of the ECDC is to strengthen Europe’s defenses against communicable diseases, such as COVID-19 or AIDS. For this reason, it must develop and update rules for SoHOs’ safety and quality from a communicable disease perspective. As a main consequence, the ECDC contribution involves health surveillance, responses to health threats, detection of emerging trends, and information campaigns related to the safety of SoHOs. According to this perspective, the eligibility of donors requires investigating adverse occurrences related to the suspected transmission of communicable diseases. From another perspective, the work of the EDQM is to set up guidelines to address issues of quality and safety beyond the risks of communicable disease transmission. On the one hand, these guidelines

include criteria for preventing cancer or other non-communicable disease transmissions. On the other hand, EDQM rules must ensure precise technical standards for the collection, processing, storage, and distribution of SoHOs.

7.3.2 Use cases definition

A framework to ensure the quality of SoHOs requires following the specific technical guidelines provided by agencies such as the ECDC and the EDQM. This approach facilitates and speeds up the development of innovative SoHO therapies, monitors epidemic trends, and simultaneously allows practice harmonization among EU Member States.

Let’s imagine a health policy officer who must identify the specific guidelines and possible risks related to the transplantation of pancreatic islets – also known as islets of Langerhans – and, in parallel, needs to analyze the spread of the Zika virus in a particular region.

As we will see in the following subsections, adopting NED and KG technologies can directly support the activity of SoHO stakeholders for specific use cases by providing different functionalities.

Conceptual search

The conceptual search mechanism allows users to overcome different names associated with pancreatic islets, also known as islets of Langerhans, or dengue fever, also called breakbone fever. This feature can be helpful for two reasons: reconciling multiple expressions to the same entity or, in the opposite case, distinguishing between entities with similar naming but distinct meanings based on the context.

Structured knowledge-based search

The structured knowledge-based search consists of the capacity to retrieve information in the text by leveraging the formalized knowledge structured in the domain ontologies. Such information enables us to create non-trivial relationships between different pieces of text from multiple documents. For example, by navigating the ontology paths, we can easily identify different

types of disorders due to diabetes. Then, we can retrieve all the documents that mention these disorders to provide the user with a complete overview of the relevant textual content.

KG-based interpretability and discovery

Integrating unstructured and semantically structured information opens many interpretability and discovery opportunities for the available data. Consider, for instance, the medical entity co-occurrence in the same sentence or textual paragraph. By exploring the related formalized knowledge in the ontologies, we can detect why a specific body structure appears alongside a virus or a disease. The relationships or paths in this formalized knowledge can reflect the essential information in textual content (interpretability) or provide more insights with connections that enrich or complete the information in the text (discovery). For instance, in the interpretability perspective, T1D and islets of Langerhans entities reasonably co-occur because the former is a syndrome that affects the latter. From the discovery perspective, diseases such as AIDS and T1D could co-occur because some pathologies associated with T1D can involve the immune system.

New Knowledge Uncovering

More exciting examples emerge when the knowledge of co-occurring entities is not yet formalized in ontologies. Still, it is available in the EDQM guidelines or related to discoveries in the field. In the case of the EDQM, the pancreatic islets entity can be mentioned along with valuable information related to the SoHO management, whose main aspects are mentioned in Figure 7.7. Moreover, SoHO’s stakeholders could be interested in observing unusual co-occurrences between communicable and non-communicable diseases, such as COVID-19 and diabetes mellitus. As a further step, the stakeholder wants to quickly access and consult the ECDC bulletins in the same (graph-based) view to understand if infections are increasing in a specific Member State and consequently make timely decisions for blocking the import of organs and tissue from a particular country.

7.4 Data understanding

Developing an IAS in the SoHO domain requires integrating heterogeneous information from various repositories into a unified source, combining both unstructured and semantically structured data. Figure 7.8 provides details related to the data understanding phase, which plays a crucial role in KG generation.

Figure 7.8 The data understanding (step 2) enables us to comprehend the key characteristics of the available information, which is essential for building the KG. For this specific scenario, the available data include SoHO regulations, reports, and guidelines released by the EC and its agencies, as well as medical ontologies such as SNOMED and HPO.

More specifically, this section will provide an overview of documents released from the EC and its agencies and the medical ontologies adopted in this context, such as SNOMED, and HPO.

7.4.1 Unstructured data

Unstructured contents include different types of documents:

Impact assessment reports in the BTC field and related regulatory proposals. Reports on the stakeholders’ positions on the regulation proposal. Guidelines and newsletter for SoHO management from the EDQM. Reports and bulletins from the ECDC to monitor the progress of infectious diseases.

In the next paragraphs, we discuss the main characteristics of these documents to clarify the types of unstructured information that is processed and transformed into a KG. Analyzing these aspects will enable you to build an intuition on the potential use cases that can build on top of these documents, in addition to those proposed and discussed in this chapter (see subsection 7.3.2).

The impact assessment reports provided by the EC are fundamental in the application domain. These documents outline the political and legal context surrounding problems in the BTC field in the EU. They collect ideas to revise the previous legislation in the field and highlight the issues related to the COVID-19 pandemic, such as the interruptions of the BTC supply in the EU. The drivers of these problems are discussed, including new diseases and developments in science and technology. These documents also outline general and specific objectives for improving the BTC sector, as well as available policy options. The impacts of these policy options are analyzed in terms of social-health, economic, digital, citizen fundamental rights, and environmental impacts. Moreover, these options propose a valuable regulation at the European level.

The regulation proposal is evaluated by different stakeholders that provide multiple documents in the form of position papers, lessons learned, and general comments. For instance, in a position paper, the International Society for Stem Cell Research (ISSCR) raised concerns about unproven cellular therapies and businesses making unsubstantiated claims for clinical effectiveness. They state that many companies often escape safety and

efficacy testing requirements for advanced therapy medicinal products (ATMPs). For this reason, in their opinion, the EU's BTC rules should ensure a connection with the ATMPs regulations and to enforce universal compliance. The ISSCR highlights the potential of stem cell-based therapies and regenerative medicines but stresses the importance of rigorous clinical trials before the donation. The paper suggests that EU expert bodies must consult with other regulators and harmonize standards with international norms and that consultations for guidance documents should be simplified to encourage feedback on new proposals.

Documents such as the “Guide to the quality and safety of tissues and cells for human application” [7] from the EDQM provide healthcare professionals with technical guidance on ensuring the safety and quality of human tissues and cells applied to patients. The guide presents minimum standards that align with EU directives, best practices consistent with current scientific knowledge, experts’ opinion, and the results of international projects. Through education and recommendations, it supports ongoing improvements in the donation, procurement, testing, processing, preservation, storage, and distribution of tissues and cells.

Communicable disease threat reports (CDTRs) from the ECDC provide every week a consolidation of all data collected via epidemic intelligence activities related to infectious diseases that are significant to the EU. This summary also offers information on worldwide conditions and alterations in the epidemiology of communicable diseases that could impact Europe.

7.4.2 Domain ontologies

Chapter 3 introduced the adoption of ontologies as a reference schema for potentially integrating different sources. In this chapter, we defined ontologies as collections of structured knowledge that can complement and add value to unstructured content. For the scope of the scenario we are addressing, we leverage the following ontologies and vocabularies:

Unified Medical Language System (UMLS) Systematized Nomenclature of Medicine (SNOMED) Human Phenotype Ontology (HPO)

These are described in the following subsections.

Unified Medical Language System (UMLS)

UMLS is a meta thesaurus composed of multiple controlled vocabularies in the biomedical domain. It provides a mapping structure among these vocabularies, simplifying the translation among the various terminology systems. The code examples available in this chapter use the 2022AA version of UMLS.

For the application in our scenario, we will use these two files.

MRCONSO.RRF: this file includes a list of biomedical entities from multiple vocabularies. For each entity naming, you have the entity id from which this naming comes. MRSTY.RRF: this file includes a list of the semantic types, categorizing the UMLS entities.

These files are formatted using delimiter-separated values (DSVs), in which the separator is a vertical bar or pipe. For this reason, this file can be processed as a traditional CSV file. Listings 7.6 and 7.7 provide samples of MRCONSO.RRF and MRSTY.RRF respectively.

Listing 7.6 UMLS – MRCONSO.RRF

C0022131|ENG|P|L0022136|PF|S0054489|N|A2883106|130586015|78696007 ➥||SNOMEDCT_US|SY|78696007|Islets of Langerhans|9|N|256|

C0022131|ENG|S|L7933100|PF|S9245679|Y|A15439829||76489||FMA|SY ➥|76489|Insulae pancreaticae|0|N|256

C0022131|ENG|S|L0826072|PF|S0870037|N|A0928304||||RCD|PT|Xa1Ij ➥|Endocrine pancreatic structure|3|N|256|

---

C0011311|ENG|P|L0011312|VC|S0000287|Y|A2872183|63434017|38362002 ➥||SNOMEDCT_US|SY|38362002|Dengue fever|9|N|256|

C0011311|ENG|S|L0286841|VO|S14576929|N|A24118377||M0005831|D00371 ➥|MSH|PM|D003715|Break Bone Fever|0|N|256|

C0011311|ENG|S|L0294785|VW|S4069775|Y|A4402397||||ICPC2ICD10ENG|P ➥|MTHU021113|dandy fever|3|N|256|

---

C0018681|ENG|S|L0290365|PF|S0362835|N|A2926207|41994011|25064002 ➥||SNOMEDCT_US|SY|25064002|Cephalgia|9|N|2304|

C0018681|ENG|S|L1406212|VO|S1680379|Y|A1641924||M0009824|D006261 ➥|MSH|PM|D006261|Cranial Pains|0|N||

C0018681|ENG|P|L0018681|PF|S0046854|N|A24679981|||HP:0002315|HPO ➥|PT|HP:0002315|Headache|0|N|256|

The MRCONSO.RRF aggregates information on the multiple ontologies, codes, and entity names related to specific medical entities. Each entry of MRCONSO.RRF is identified by the UMLS id located in the first column of each row. This UMLS id is crucial for the NED because the ScispaCy model provides the disambiguation results using these ids. In our example, listing 7.6 shows a set of rows related to the “Islets of Langerhans” (C0022131), “Dengue fever”, “Headache”. Codes and names associated with these entities are located in the following sources:

Systematized Nomenclature of Medicine (SNOMED - https://www.nlm.nih.gov/healthit/snomedct/us_edition.html) Foundational Model of Anatomy (FMA - http://si.washington.edu/projects/fma) Read Codes (RC - http://www.connectingforhealth.nhs.uk/systemsandservices/data/readcod Medical Subject Headings (MSH - https://www.nlm.nih.gov/mesh/) International Classification of Primary Care, Second Edition – International Classification of Diseases, Tenth Revision (ICPC2ICD10ENG - https://www.who.int/standards/classifications/other- classifications/international-classification-of-primary-care) Human Phenotype Ontology [4] (HPO - https://hpo.jax.org/app/)

Each of these ontologies is focused on a subset of aspects related to medical entities. For instance, the Human Phenotype Ontology (HPO) collects information on the symptoms associated with a disease, while ignoring the

body structures affected by such a condition. For this reason, integrating different ontologies specialized in peculiar aspects of the medical domain can be extremely useful for enriching and exploring your data from diverse perspectives.

The examples proposed in this chapter will include a subset of the ontologies indexed by the Unified Medical Language System (UMLS), such as the Systematized Nomenclature of Medicine (SNOMED) and the Human Phenotype Ontology (HPO), which provide interesting semantic relationships between medical entities and, at the same time, are particularly relevant in our application domain. Listing 7.7 shows how “Islets of Langerhans”, “Dengue Fever” and “Headache” are categorized in UMLS.

Listing 7.7 UMLS – MRSTY.RRF

C0022131|T023|A1.2.3.1|Body Part, Organ, or Organ Component|AT196 C0011311|T047|B2.2.1.2.1|Disease or Syndrome|AT41932582|256| C0018681|T184|A2.2.2|Sign or Symptom|AT17639733|256|

For each entry of the MRCONSO.RRF file, the MRSTY provides a semantic type code and name. In our running example, “Body Part, Organ, or Organ Component (T023)”, “Disease or Syndrome (T047)”, and “Sign or Symptom (T184)” correspond to semantic types of “Islets of Langerhans”, “Dengue Fever”, and “Headache”, respectively.

Systematized Nomenclature of Medicine (SNOMED)

SNOMED is one of the most comprehensive, multilingual clinical terminology, encompassing more than 450,000 concepts. Moreover, SNOMED provides a rich set of relationship types between these concepts, including interesting types from a clinical point of view, such as “CAUSATIVE AGENT“ and “FINDING SITE“. The SNOMED ontology is provided utilizing the UMLS free license and can be downloaded from https://www.nlm.nih.gov/healthit/snomedct. We use the SNOMED version released on 2022-09-01.

For the application in our scenario, we will use these two files:

sct2_Description_Full-en_US1000124_20220901.txt: this file includes all the entities' names (and aliases) and relationships formed in the triplet file that define the relationship between entities. sct2_Relationship_Full_US1000124_20220901.txt: this file lists a set of triplets (plus other metadata) that define all the relationships between the SNOMED entities. Each entity and relationship is identified using a numerical code.

These files are released using a TSV format. Listings 7.8 and 7.9 provide samples of the description and the relationship files, respectively.

Listing 7.8 SNOMED – sct2_Description_Full-en_US1000124_20220901.txt

130586015 20020131 1 900000000000207008 78696007 e 63434017 20020131 1 900000000000207008 38362002 en 41993017 20020131 1 900000000000207008 25064002 en

Listing 7.6 showed how SNOMED information is integrated into the UMLS metathesaurus. Instead, Listing 7.8 details how such information is initially provided by the original data source from which UMLS has been generated. This listing reports the details of the SNOMED entry, including the code and name of “Islets of Langerhans”, “Dengue Fever”, and “Headache” (or “Cephalgia”), respectively. Listing 7.9 illustrates how these entities serve as either source or target entities within specific semantic relationships.

Listing 7.9 SNOMED – sct2_Relationship_Full_US1000124_20220901.txt

169174023 20020131 1 900000000000207008 360555004 182243021 20020131 1 900000000000207008 20927009 3 424787021 20020131 1 900000000000207008 54012000 2

Listing 7.9 shows that “Islets of Langerhans”, “Dengue Fever”, “Cephalgia” can be target entities of a “IS_A” relationship (id: 116680003). The source entities are “Endocrine pancreas cell” (id: 360555004), “Dengue hemorrhagic fever” (id: 20927009), and “Posttraumatic headache” (id: 54012000), respectively.

Human Phenotype Ontology (HPO)

The HPO ontology is released as RDF/XML file called hpo.owl (http://purl.obolibrary.org/obo/hp.owl). This ontology contains standardized information on phenotypic anomalies. Listing 7.10 shows a subset of the information in the hpo.owl file related to Type I diabetes mellitus. The data is serialized from RDF/XML to Turtle (Terse RDF Triple Language) for improving readability.

Listing 7.10 Type I diabetes mellitus details reported in hpo.owl

obo:HP_0100651 a owl:Class ; #A

rdfs:label “Type I diabetes mellitus” ^^xsd:string ; obo:IAO_0000115 “A chronic condition in which the pancreas pr little or no insulin…” ^^xsd:string ; #B oboInOwl:created_by “doelkens”^^xsd:string ; #C oboInOwl:creation_date “2010-12-29T06:37:55Z”^^xsd:string ; oboInOwl:hasDbXref “MSH:D003922”^^xsd:string, #D “SNOMEDCT_US:46635009” ^^xsd:string, “UMLS:C0011854” ^^xsd:string ; oboInOwl:hasExactSynonym “Diabetes mellitus Type I”^^xsd:stri “Juvenile diabetes mellitus” ^^xsd:string, “Type 1 diabetes”, “Type I diabetes”; oboInOwl:hasRelatedSynonym “Insulin-dependent diabetes mellitus”^^xsd:string ; oboInOwl:id “HP:0100651”^^xsd:string ; rdfs:comment “The onset of type 1 diabetes is typically durin adolescence…” ^^xsd:string ; rdfs:subClassOf obo:HP_0000819 . #E

7.5 SoHO knowledge graph building

We defined a process that includes multiple steps for the construction of the KG and the development of advanced use cases on top of this source of truth:

1. Definition of the KG schema 2. Documents processing and ingestion 3. Medical entities disambiguation and ingestion 4. Ontologies processing, loading, and mapping 5. Generation of co-occurrence relationships

Figure 7.9 shows the key steps for the knowledge graph building, which

involves the data preparation and ingestion.

Figure 7.9 Data preparation (step 3) and KG creation and update (step 4) are the critical technical steps for constructing the KG. The data preparation step includes processing the available data, while the KG creation and update step takes as input the result of this data process and produces the database for performing the analysis phase.

You can build the KG from scratch following each of these phases. Alternatively, you are invited to proceed through this section, skipping certain steps. The goal is to understand the key ideas to construct the KG and

then directly focus on the use cases (section 9.6). To achieve this goal, you can begin with an intermediate version of the KG, which includes documents processed using ScispaCy. Then, you must integrate the medical ontologies into this KG because we cannot directly release them due to licensing reasons. In this specific case, the reader can start with Phase 1 (subsection 7.5.1) to understand the schema of the KG and then focus only on Phase 4 (subsection 7.5.4) for loading the ontologies and mapping the nodes of these ontologies to the extracted medical entities. The full code for building the KG combines Python scripts and Cypher queries, and it is available on the GitHub repository of the book: https://github.com/alenegro81/knowledge- graphs-and-llms-in-action/tree/main/chapters/ch07.

In this chapter, the reader will Python scripts wrapping Cypher queries and pure Cypher queries that can be run in the Neo4j browser.

7.5.1 Schema definition

The schema definition is a theoretical step that enables us to define a graph- based model for our data, which is beneficial for use cases related to our application domain (section 7.6). Figure 7.10 illustrates the schema that models the primary components (nodes and relationships) of the KG we aim to construct.

Figure 7.10 Knowledge Graph schema, including relevant node labels and relationship types, to enable multiple types of analysis.

The KG schema in Figure 7.10 reports the information related to the ingested documents. These documents are structured in a graph shape by connecting the File node and the related Page nodes. Each Page is linked to a collection of EntityMention nodes that define all the recognized entities in the text. These EntityMention nodes are disambiguated and connected to the MedicalEntity nodes. The DISAMBIGUATED_TO relationship allows us to explicitly model the cases where mentions with the same string refer to

different entities and, on the other hand, entities that are characterized by different strings that point to the same entity. As we have seen in the initial example, the term “Zika” can refer to multiple entities, while the terms “AIDS” and “Acquired Immunodeficiency Syndrome” are two different expressions of the same entity. The MENTIONS_ENTITY relationship connects the Page node and the disambiguated MedicalEntity node.

The remaining nodes and relationships are defined to map the extracted MedicalEntity to SnomedEntity, HpoEntity, and HpoDiseaseEntity. We decided to model HpoEntity and HpoDiseaseEntity. Compared to chapter 3, we chose a different label for the nodes from the HPO ontology in order to maintain consistency with the other ontology nodes used in this chapter. To specify the mapping between the extracted MedicalEntity and the ontology entities in the schema, we defined the following relationships: IS_SNOMED_ENTITY, IS_HPO_ENTITY, and IS_DISEASE_ENTITY. After this clarification on the KG schema, we can start to ingest our data.

7.5.2 Documents processing and ingestion

The first phase includes the ingestion of the documents and the modeling of the related content in a graph-based structure. Most of the documents available for our purpose are released in PDF or DOCx format. For this reason, before loading the data into Neo4j, we extracted the raw content by using the Amazon Textract OCR[2] service and processed the result. We decided to use this service to achieve a standard representation of different types of documents to simplify the text processing step. To reconstruct the full text, we created some basic Python scripts for managing documents with a different structure, for instance, 1-column or 2-column documents. Figure 7.11 shows the key aspects of this preparation phase for reconstructing the full text from PDF and DOCx documents.

Figure 7.11 The extraction of the text from SoHO documents has been already performed using the Amazon Textract service. You will need to run on your local machine the text processing script for reconstructing the full text of the pages and then load the results into the Knowledge Graph.

The full code implementation related to text processing, along with further details, is available on GitHub. Listing 7.11 shows the Cypher query to ingest ingesting raw text or processed documents and structuring this information in a graph-based format.

Listing 7.11 Load textual content into Neo4j

[…] class DocsImporter: […]

def set_constraints(self): queries = ["CREATE FULLTEXT INDEX pageText FOR (n:Page) ON EA

for q in queries: self.connection.query(q, db=self.db) def load_docs(self): with open(self.docs_file) as json_file: docs = json.load(json_file)

query = """ MERGE (f:File {id: $name}) SET f.type = $type, f.path = $name WITH f

UNWIND $pages as page MERGE (p:Page {id: replace($name, '.pdf', '') + '_' + page. SET p.page_idx = page.page_idx, p.text = page.text MERGE (f)-[:CONTAINS_PAGE]->(p) """

for i in tqdm(docs): name = i['name'] type = i['type'] pages = i['pages'] self.connection.query(query, parameters={'name': name, 'typ

At the end of this process, files and page texts are loaded into the KG.

7.5.3 Medical entities disambiguation and ingestion

The next phase includes the extraction and the disambiguation of the medical entities from the OCR-ed text. The strategy is to process all the documents directly from the JSON file produced as an output of the OCR, store the results of the process in a Python dictionary, and then load the result into Neo4j. A sample of the outcome of this processing is reported in Listing 7.12.

Listing 7.12 Python dictionary resulting from the document processing

{'id': 'sample_dataset-PublicUse/ECDC Documents/west nile virus/E 'ents': [{'sentenceIndex': 0,

'value': 'zoonoses', 'lemma': 'zoonosis', 'label': 'ENTITY', 'beginCharacter': 60, 'endCharacter': 68, 'selected_ned_id': 'C0043528', 'selected_ned_name': 'Zoonoses', 'selected_ned_definition': 'Diseases of non-human animals that 'selected_ned_aliases': ['Zoonotic Disease', 'Zoonosis, NOS', 'Zoonoses',…], 'selected_ned_types_id': ['T047'], 'selected_ned_types': ['Disease or Syndrome']… }

As you may notice from these results, we have stored various details related to the extracted entities, including the sentence index and the position of the mentions in the text, by specifying the start and end characters. In addition to information entities such as type and aliases, these details will be used to perform advanced queries, as discussed in the use case section. After the disambiguation phase, we can load the results into Neo4j, as shown in Listing 7.13.

Listing 7.13 NED Data Loading

[…] class NLPImporter(BaseImporter): […]

def load_nlp_res(self): print("Loading data into Neo4j...") with open(self.file,'rb') as file: processed_pages = pickle.load(file)

query = """ UNWIND $rows as item

MATCH (page:Page) // #A WHERE page.id = item.id AND NOT page:NEDProcessed SET page:NEDProcessed WITH page, item UNWIND item.ents as entity

MERGE(mention:EntityMention {name_normalized: toLower ON CREATE SET mention.name = apoc.text.join(apoc.text MERGE (page)-[s:MENTIONS_MENTION {from_model: "ned"}] ON CREATE SET s.start_chars= [entity.beginCharacter], s.end_chars= [entity.endCharacter], s.sentence_index = [entity.sentenceIndex] s.type = toLower(entity.label) ON MATCH SET s.start_chars = s.start_chars + entity.b s.end_chars = s.end_chars + entity.endCha s.sentence_index = s.sentence_index + ent WITH page, mention, entity FOREACH(medical in entity | MERGE (dis:MedicalEntity {id: medical.selected_ned_id ON CREATE SET dis.name= apoc.text.join(apoc.text. dis.type_id = medical.selected_ned_types_ dis.types = medical.selected_ned_types, dis.type = medical.selected_ned_types[0], dis.original_mention = medical.value, dis.definition = medical.selected_ned_def dis.aliases = medical.selected_ned_aliase dis.start_chars= [entity.beginCharacter], dis.end_chars= [entity.endCharacter], dis.sentence_index = [entity.sentenceInde ON MATCH SET dis.start_chars = dis.start_chars + enti dis.end_chars = dis.end_chars + entity.en

MERGE (mention)-[r:DISAMBIGUATED_TO]->(dis) // #F SET r.confidence = medical.selected_ned.confidence MERGE (page)-[t:MENTIONS_ENTITY]->(dis) // #G ON CREATE SET t.sentence_index = [medical.sentenceInd ON MATCH SET t.sentence_index = t.sentence_index + me

"""

self.load_in_batch(query, processed_pages, len(processed_

The query defined in Listing 7.13 stores the results of Scispacy processing in Neo4j. Firstly, it creates the EntityMention nodes, which are connected to the Page nodes. Then, it creates the MedicalEntity nodes, enriches these nodes with UMLS data, and finally, connects the medical entities to EntityMention and the Page nodes. As we introduced in the schema discussion, we decided to keep both the EntityMention and the disambiguated MedicalEntity in the graph to improve flexibility in representing data.

7.5.4 Ontologies processing, loading, and mapping

This section provides details on how to load UMLS, SNOMED, and HPO ontologies into the KG. The UMLS serves as the entry point to access specific information across multiple ontologies. For this reason, we first load the SNOMED and the HPO ontologies, and then we map each entity from these ontologies to UMLS. Listing 7.14 shows how to create nodes and relationships into Neo4j from the sct2_Relationship_Full_US1000124_20220901.txt.

Listing 7.14 SNOMED ingestion – Load relationships

[…] class SnomedRelationshipsImporter(BaseImporter): #A

[…] def set_constraints(self): #B queries = [ "CREATE CONSTRAINT IF NOT EXISTS FOR (n:SnomedEntity) REQUI "CREATE INDEX snomedNodeName IF NOT EXISTS FOR (n:SnomedEnt "CREATE INDEX snomedRelationId IF NOT EXISTS FOR ()-[r:SNOM "CREATE INDEX snomedRelationType IF NOT EXISTS FOR ()-[r:SN "CREATE INDEX snomedRelationUmls IF NOT EXISTS FOR ()-[r:SN for q in queries: self.connection.query(q, db=self.db) def import_snomed_rels(self): #C query = """ UNWIND $batch as item MERGE (e1:SnomedEntity {id: item.sourceId})

MERGE (e2:SnomedEntity {id: item.destinationId}) MERGE (e1)-[:SNOMED_RELATION {id: item.typeId}]->(e2) FOREACH(ignoreMe IN CASE WHEN item.typeId = '116680003' TH MERGE (e1)-[:SNOMED_IS_A]->(e2) // #D ) """ size = self.get_csv_size(snomedRels_file) #E self.batch_store(snomed_rels_query, self.get_rows(snomedRels_file

SNOMED includes hundreds of relationships. For this reason, to keep the graph schema as simple as possible, we decided to create a unique SNOMED_RELATION, and we store the relationship name as a type property. As the reader noticed, in Listing 7.14 we created a specific SNOMED_IS_A relationship to define the hierarchical connections. We will see in a while why this choice is handy to propagate information from the root to the leaf nodes. After the ingestion of nodes and relationships to shape the graph, we need to enrich such structure by adding names and aliases. This is the goal of the class in Listing 7.15 that extracts information from the sct2_Description_Full-en_US1000124_20220901.txt file.

Listing 7.15 SNOMED ingestion – Load names and aliases

[…] class SnomedNamesImporter(BaseImporter):

[…] def import_snomed_names(self, snomedNames_file): snomed_names_concepts_query = """ UNWIND $batch as item MATCH (e1:SnomedEntity)-[r:SNOMED_RELATION {id: item.conc WHERE item.conceptId <> '116680003' AND r.id = item.conce SET r.type = CASE WHEN r.type IS NULL THEN item.termAsType ELSE r.type END, // #A r.aliases = CASE WHEN item.termAsType IN r.aliases THEN r.aliases ELSE coalesce(r.aliases,[]) + item.termAsType END """

snomed_names_entities_query = """ UNWIND $batch as item MATCH (e:SnomedEntity {id: item.conceptId})

SET e.name = CASE WHEN e.name IS NULL THEN item.term ELSE e.name END, // #C e.aliases = CASE WHEN item.term in e.aliases THEN e.aliases ELSE coalesce(e.aliases, []) + item.term END // # """ size = self.get_csv_size(snomedNames_file) self.batch_store(snomed_names_concepts_query, self.get_row self.batch_store(snomed_names_entities_query, self.get_row

[…]

Class in Listing 7.15 adds names and aliases to nodes and relationships loaded into the graph. The following step is to propagate information from the root nodes to all the sub-nodes down to the leave nodes. The first-level nodes represent the archetypal entities of SNOMED in the medical domain, such as diseases, body structures, substances, and events. From another point of view, these root nodes define the semantic types of the entities available in SNOMED. However, this information is implicit in the data we ingested because for each other entity there are only names and aliases in the original data. Therefore, we need a mechanism to transfer this information through the ontology's tree structure. In this way, we can easily detect that a deep entity is a disease or a product. Figure 7.12 clarifies this propagation mechanism.

Figure 7.12 A sample of the hierarchical structure of the SNOMED ontology. Leveraging this hierarchical structure, nodes located on a deeper level, such as “Ecallantide” and “Retinopathy associated with AIDS”, can be categorized using the information from the first-level nodes, such as “Pharmaceutical product” and “Disease”, which represent the archetypal entities of the ontology.

Listing 7.16 illustrates how to retrieve all nodes following the hierarchical structure to propagate information from first-level nodes to deeper nodes.

Listing 7.16 SNOMED ingestion – Label propagation from the first-level nodes

[…] class SnomedLabelPropagator(): […]

def get_rows(self): propagation_query = """ MATCH p=(n:SnomedEntity)<-[:SNOMED_IS_A]-(m:SnomedEntity) WHERE n.id= "138875005" // Root node WITH distinct m as first_node

CALL apoc.path.expandConfig(first_node, { // #A relationshipFilter: '<SNOMED_IS_A', minLevel: 1, maxLevel: -1, uniqueness: 'RELATIONSHIP_GLOBAL' }) yield path UNWIND nodes(path) as other_level // #B WITH first_node, collect(DISTINCT other_level) as uniques UNWIND uniques as unique_other_level WITH first_node,unique_other_level WHERE not first_node.name in coalesce(unique_other_l RETURN unique_other_level.id as id, first_node.name """ with self._driver.session(database=self._database) as session: result = session.run(query=propagation_query) for record in iter(result): yield dict(record) […]

The SNOMED_IS_A relationship has been leveraged to propagate the semantic types through the tree structure by leveraging the hierarchical connections between entities.

The following listings outline the primary steps for ingesting the HPO ontology. More details, including the creation of the proper indices, are discussed in chapter 3 and reported in the Cypher code associated with this chapter. Listing 7.17 demonstrates how to import the HPO ontology by utilizing the Neosemantics plugin.

Listing 7.17 HPO Ingestion – Loading of the ontology

CALL n10s.rdf.import.fetch("http://purl.obolibrary.org/obo/hp.owl

The HPO ontology is available in the RDF/XML format, and it is converted into a Neo4j graph structure to be processed using Cypher. Listing 7.18 adds the HPO label to the nodes representing phenotypic feature entities.

Listing 7.18 HPO Ingestion – Add HpoEntity label to phenotypic features

MATCH (n:Resource) WHERE n.uri STARTS WITH "http://purl.obolibrary.org/obo/HP" SET n:HpoEntity,

n.id = coalesce(n.id, replace(apoc.text.replace(n.uri,'(.*)obo/'

After ingesting the ontology data, we need to process the annotation file that describes the connections between the phenotypic features and diseases. To make this step more efficient, using the query in Listing 7.19, we load the disease entities as graph nodes.

Listing 7.19 HPO Ingestion – Create HpoDiseaseEntity nodes from the annotation file

LOAD CSV FROM 'https://github.com/obophenotype/human-phenotype-on FIELDTERMINATOR '\t' WITH row SKIP 5 MERGE (dis:Resource:HpoDiseaseEntity {id: row[0]}) ON CREATE SET dis.label = row[1];

The next step is to create the relations between the already available phenotypic features and disease nodes using the query in Listing 7.20.

Listing 7.20 HPO Ingestion – Relations between HpoEntity and HpoDiseaseEntity

LOAD CSV FROM 'https://github.com/obophenotype/human-phenotype-on FIELDTERMINATOR '\t' WITH row SKIP 5 MATCH (dis:HpoDiseaseEntity) WHERE dis.id = row[0] MATCH (phe:HpoEntity) WHERE phe.id = row[3] MERGE (dis)-[:HAS_PHENOTYPIC_FEATURE]->(phe)

To establish the connection between the disambiguated entities extracted from the text and the ontology nodes, we need to perform the query available in Listing 7.21.

Listing 7.21 SNOMED integration through UMLS

MATCH (m:MedicalEntity) WITH m MATCH (d:SnomedEntity) WHERE m.id in d.umls_ids WITH m, d MERGE (m)-[:IS_SNOMED_ENTITY]->(d)

An analogous process is performed for the HPO ontology (see the full code for more details). The query in Listing 7.22 allows us to connect the MedicalEntity nodes and the DiseaseEntity nodes from the HPO annotation file.

Listing 7.22 Create connections between MedicalEntity and HpoDiseaseEntity nodes

MATCH (m:MedicalEntity) WITH m MATCH (d:HpoDiseaseEntity) WHERE m.id in d.umls_ids WITH m, d MERGE (m)-[:IS_DISEASE_ENTITY]->(d)

The next subsection discusses the generation of the co-occurrence relationship between entities located in the same sentence.

7.5.5 Entities co-occurrence generation

The co-occurrence identification of medical entities in the text is a fundamental step in enabling advanced use cases that combine the unstructured knowledge from textual content with the structured knowledge established in domain ontologies. The query defined in Listing 7.23 enables us to create a new relationship, called COOCCURR, between medical entities identified in the same sentence. Formally, as you have seen many times in this book, co-occurrence refers to the projection of the Page nodes onto the Entity nodes.

Listing 7.23 Create medical entities co-occurrence relationships at the sentence level

CALL apoc.periodic.iterate(

"MATCH (n:Page) WHERE exists( (n)-[:MENTIONS_ENTITY]->(:Medical RETURN n", "MATCH (n)-[r:MENTIONS_ENTITY]->(m:MedicalEntity) WITH n, r.sentence_index as sentences, m UNWIND sentences as sentence WITH n, sentence, collect(distinct m) as entities

UNWIND range(0, size(entities)-2) as i UNWIND range(i+1, size(entities)-1) as j

WITH n, sentence, entities, i, j MATCH (m1) WHERE id(m1) = id(entities[i]) MATCH (m2) WHERE id(m2) = id(entities[j])

WITH n, sentence, entities, i, j, m1, m2 MERGE (m1)-[s:COOCCURR]-(m2) ON CREATE SET s.count = 1, s.sentences = [sentence] ON MATCH SET s.count = s.count + 1, s.sentences = s.sentences + sentence", {batchSize: 50})

Query in Listing 7.23 produces more than 25,000 relationships in the KG, making explicit the connections between medical entities located in the same sentence. As we will discuss in the use KG-based use cases (section 7.6), the analysis of the ontology connections between the co-occurring entities allows us to discover the established knowledge related to these entities and, from the opposite perspective, enables us to uncover new information that is not yet established in biomedical ontologies.

7.6 Knowledge graph-based use cases

The defined KG-based use cases applied to the BTC and the SoHOs domain become fundamental for the involved stakeholders. They can extract valuable information from guidelines, reports, and publications and combine these data with the formalized knowledge depicted within ontologies to identify and expand the information available in the documents. Here we introduce the related use cases:

Conceptual search Structured knowledge-based search KG-based interpretability and discovery New knowledge uncovering

Figure 7.13 includes the analysis step related to the use cases defined in this application domain.

Figure 7.13 After the generation of the KG, we can perform the analysis phase. This analysis consists in the application of defined use cases that are useful address the business problems and provide a solution based on the KG.

In the following sub-sections, we discuss through code examples how we deal with these use cases leveraging KGs in combination with NED and the value delivered to the users through them.

7.6.1 Conceptual search

The conceptual search is related to the capacity to reconcile – and return to the user – different expressions with the same meaning or distinguish similar terms that refer to distinct entities. For example, a viral infection that spreads from mosquitos to people called “dengue fever” can also be referred to as “breakbone fever.” Similarly, “islets of Langerhans” and “pancreatic islets” refer to the same group of cells in the pancreas. On the contrary, the “Zika” word can mean both the infection and the virus. For these specific cases, the conceptual search can help us to expand or focus the search of specific documents and the related portions of text that mention the entity. Figure 7.14 provides a high-level description of the conceptual search, compared to the traditional full-text search.

Figure 7.14 Mental model reporting the differences between the traditional full-text search and the conceptual search. The full-text search query is directly performed on the set of documents, while the conceptual search query leverages information from ontologies to expand the search space before searching into the group of documents.

Let’s try to understand the impact of the conceptual search starting from a traditional full-text search mechanism. The full-text search query available in listing 7.24 tries to retrieve all the documents mentioning “breakbone fever.”

Listing 7.24 Basic full-text search query with “breakbone fever” as the input string

CALL db.index.fulltext.queryNodes("PageText", "breakbone fever") YIELD node, score WITH node as p, score as score

MATCH (f:File)-[:CONTAINS_PAGE]->(p) RETURN f.id as `File ID`, p.page_idx as `Page index`, score as Sc LIMIT 5

In table 7.1, the reader can see the first results of the full-text search ordered by the score values are the following:

Table 7.1 Top-5 scored pages retrieved with the full-text search query using “breakbone fever” as an input string.

File ID Page index

Score

“sample_dataset-PublicUse/ECDC Documents/west nile virus/emerging-vector-borne-diseases_annual- epidemiological-report-2014.pdf”

5 2.12

“sample_dataset-PublicUse/ECDC Documents/west nile virus/emerging-vector-borne-diseases_annual- epidemiological-report-2014.pdf”

4 2.09

“sample_dataset-PublicUse/ECDC Documents/zika virus/Communicable-disease-threats-report-26-mar- 2016.pdf”

10 2.08

“sample_dataset-PublicUse/ECDC Documents/ebola/communicable-disease-threats-report-13- 19-mar-2016.pdf”

9 2.08

sample_dataset-PublicUse/ECDC Documents/zika virus/Communicable-disease-threats-report-26-mar- 2016.pdf

10 2.08

The first column of table 7.1 includes the document’s path, the second column shows the page index within the document, and the third column reports the score computed by the full-text search algorithm. This type of search identifies the keywords within the available set of documents by exploiting fuzzy logic. However, the “breakbone” string is never mentioned in the text, but the term “fever” appears multiple times within the high-scored documents. Nevertheless, this is not the main purpose of our search: we are interested in identifying documents that mention this specific disease or, eventually, the causative agent of this disease.

We aim to retrieve all documents mentioning the “breakbone fever” entity. In the UMLS KB, the C0011311 code identifies this entity, which can appear in various forms, including “dengue fever,” “dungero,” or “dandy fever”. For this reason, we can run the query in listing 7.25 that retrieves all the medical entities identified by the SciSpaCy model whose alias is “breakbone fever.”

Listing 7.25 Result of different mentions of the entity whose alias is “breakbone fever”

MATCH (f:File)-[:CONTAINS_PAGE]->(p)-[r:MENTIONS_MENTION]->(m)-[: WHERE "breakbone fever" IN [x IN e.aliases | toLower(x)] // #A UNWIND range(0, size(r.start_chars) - 1) AS mention // #B WITH f, p, e, m, r, mention RETURN DISTINCT

f.id AS `File ID`, p.page_idx AS `Page index`, apoc.text.join( collect( substring( p.text, apoc.coll.max([r.start_chars[mention] - 100, 0]), // #C r.end_chars[mention] - r.start_chars[mention] + 200 // #D ) )[0..3], // #E '\n\n' ) AS `Mention contexts`, size(collect(m.name)) AS `Number of mentions` ORDER BY `Number of mentions` DESC LIMIT 5

Table 7.2 displays the top-scored pages and an example of a paragraph that mentions the UMLS entity representing breakbone fever, listed in order by the number of mentions.

Table 7.2 Top-scored pages retrieved with the entity-driven search using “breakbone fever” as an entity alias

File ID Page index

Mention contexts Number of mentions “sample_dataset- PublicUse/ECDC Documents/hepatitis-

11 "In 2017, Cambodia reported over 3 200 suspected dengue

22

a/communicable-disease-threats- report-feb-24-2018.pdf”

cases.” "sample_dataset- PublicUse/ECDC Documents/ebola/communicable- disease-threats-report- 17%20may-2014.pdf"

11 "Singapore has reported more than 1000 dengue cases nationally from January to April this year which is 15 per cent less cases compared with […]”

20

"sample_dataset- PublicUse/ECDC Documents/ebola/Communicable- disease-threats-report-19-jul- 2014.pdf"

12 "An epidemic of dengue fever in Malaysia has now infected nearly 47 000 people which is more than double the number of cases […]"

20

"sample_dataset- PublicUse/ECDC Documents/ebola/communicable- disease-threats-report-21-jun- 2014.pdf"

13 "Cuba has recorded 67 imported cases of dengue fever up to 8 June, according to media quoting the Cuban government.”

20

"sample_dataset- PublicUse/ECDC Documents/west nile virus/communicable-disease- threats-report-21-jun-2014.pdf"

12 Oceania: As of 13 June 2014, 1 762 suspected dengue cases have been reported in Solomon Islands since January 2014.

19

Table 7.2 shows the results of the conceptual search. Comparing the results with the full-text, we notice that the most relevant pages are completely different. Let’s try to update the conceptual search query to show and explore the full-result set (remove the LIMIT clause). We notice that the scored-first

result in the full-text query (page five from the document entitled “sample_dataset-PublicUse/ECDC Documents/west nile virus/emerging- vector-borne-diseases_annual-epidemiological-report-2014.pdf") is located at the 19th position in the conceptual search query. Indeed, for this specific document, the number of mentions of the UMLS entity is equal to 17. Instead, the scored-first result of the conceptual search query is “sample_dataset-PublicUse/ECDC Documents/ebola/communicable-disease- threats-report-21-jun-2014.pdf", which includes 22 mentions of the C0011311 UMLS entity.

Adopting conceptual search has a huge impact on our application domain. In the context of SoHO management, timely retrieving key information is fundamental because it allows users to make informed decisions. In this example, we have seen that traditional full-text searches did not work properly for our purpose. On the contrary, the conceptual search enables immediate, exhaustive, and detailed information. Moreover, as you can see from the column fields, we can retrieve more precise and detailed information compared to the full-text search. In addition to the document path and the page index, we can transparently identify the portion of text that mentions the entity and the number of occurrences of this entity within the page. Therefore, we observe that the conceptual search enables other features that improve the explainability of the retrieved document. The reason is that the NED model allows you to identify the position of the detected entity in the text. Indeed, we can identify the specific portion of text in which the entity has been mentioned and, eventually, perform precise debugging tests to identify the cases in which the model did not perform as expected.

The conceptual search enhances the search process from an alternative perspective. For instance, we can consider a scenario where the same word or expression has different meanings based on context. Let’s take the word “islands.” Besides its traditional meaning, this word takes on a completely different semantic context when used in an expression like “islands of Langerhans.” Therefore, using a traditional full-text search might lead the reader to irrelevant results, as they could refer to land masses surrounded by water. By utilizing the conceptual search mechanism, one can filter out these irrelevant results and broaden the search scope to include other expressions, such as “pancreatic islets,” to capture more relevant content. As the reader

will observe in sub-section 7.6.2, the core concept of the conceptual search can be expanded by linking pages that include medical entities connected through ontology relationships.

7.6.2 Structured knowledge-based search

The structured knowledge-based search involves retrieving information from text by leveraging formalized knowledge organized in domain ontologies. Such information allows us to create non-trivial relationships between different pieces of text across multiple documents. As illustrated in the previous examples, we can aggregate information referring to the same concepts, such as dengue fever or pancreatic islets, regardless of their ambiguous or multiple names. However, by utilizing the knowledge embedded in ontologies like SNOMED, we can aggregate and connect unstructured content driven by ontology relationships and create value in our business domain. For instance, given the importance of donating cells like the islets of Langerhans, it would be beneficial to aggregate all text portions that mention diseases that can affect these specific cells and compromise the donation process. Figure 7.15 illustrates a mental model that shows the difference between the conceptual search query and the structured knowledge-based query.

Figure 7.15 Mental model reporting the differences between the conceptual search and the structured knowledge-based search. In this case, the search process involves medical entities connected by ontology relationships to the entity detected in the search query. Starting from the “islets of Langerhans” search, the system searches for “diabetes type 1” and “hyperglycaemia”, which are two diseases that can affect “islets of Langerhans”.

Listing 7.26 reports the query to retrieve these portions of text that mention diseases that can affect islets of Langerhans.

Listing 7.26 Get textual contents about diseases that can affect islets of Langerhans

MATCH (m1:MedicalEntity)-[:IS_SNOMED_ENTITY]->(s1:SnomedEntity)<- WHERE m1.name = "Islets of Langerhans" AND r1.type = "FINDING_SIT WITH e MATCH path = (f:File)-[:CONTAINS_PAGE]->(p)-[r:MENTIONS_MENTION]- UNWIND range(0, size(r.start_chars) - 1) AS mention // #B WITH f, p, r, e, mention,

apoc.coll.max([r.start_chars[mention] - 100, 0]) AS start, / apoc.coll.min([r.end_chars[mention] + 100, size(p.text)]) AS WITH f, p, r, e, mention, substring(p.text, start, end - start) A WITH f, p,

collect(DISTINCT e.name) AS `Mentioned entities`, // #F collect(context)[0..3] AS contexts // #G RETURN DISTINCT

f.id AS `File ID`, p.page_idx AS `Page index`, `Mentioned entities`, apoc.text.join(contexts, '\n\n') AS `Mention contexts` ORDER BY size(`Mentioned entities`) DESC LIMIT 5

A subset of the results of the query provided in Listing 7.26 are available in table 7.3.

Table 7.3 Textual contents about diseases that can affect Islets of Langerhans

File ID Page index

Mentioned entities Mention contexts " sample_dataset- PublicUse/EDQM Documents/BTC/guide- to-the-quality-and- safety-of-organs-for- transplantation-7th- edition.PDF"

"Metabolic syndrome, arterial hypertension diabetes mellitus, albuminuria (see below) other chronic kidney disea or systematic disease […]” "sample_dataset- PublicUse/EDQM

137 ["Diabetes Mellitus", "Metabolic Syndrome X"]

144 ["Hyperglycemia", "Diabetes Mellitus, Non-Insulin-

" […] Many patients with

Documents/BTC/guide- to-the-quality-and- safety-of-organs-for- transplantation-7th- edition.PDF"

Dependent"] severe head trauma beco hyperglycae and require insulin thera despite norm pancreatic function and history of diabetes. […] On the other hand, manifestatio of diabetes mellitus type is possible at age of over 5 65 years."

In Listing 7.26, we extended the logic of the conceptual search query taking a collection of entities from the ontology as input and retrieving the related details within the documents. One of the most interesting results from the previous query includes page 144 of the “Guide to the quality and safety of organs for transplantation, 7th edition” reported in row 4 of table 7.3. This page mentions the “Hyperglycemia” disease, which is explicitly connected to the “Islets of Langerhans” entity through the “FINDING_SITE” relationship, in the context of the management of the potential donor who has suffered trauma. The reference sentence is the following:

“Many patients with severe head trauma become hyperglycaemic and require insulin therapy, despite normal pancreatic function and no history of diabetes.”

This sentence is located in a section dedicated to pancreas donation focused on donor-maintenance protocols. This critical information would not have been retrieved through the conceptual search mechanism. However, we leveraged the connection of the “Hyperglicemia” medical entity to the

starting point of our search, which was the “Islets of Langerhans”, to reach this information. The other results mention different forms of Diabetes Mellitus, which is particularly relevant in the donation of pancreatic islets.

The structured knowledge-based search can be performed by leveraging the longest paths within an ontology. Let’s consider the Zika virus example, which has been discussed multiple times in this chapter. This virus can be categorized as “Togavirus” and it is transmitted to humans via some species of arthropods that feed on blood, especially insects and arachnids. Considering the relevance of this topic in the field of SOHO management, we can potentially be interested in all the diseases mentioned in the document that are caused by “Togavirus”. To reach this goal, we can discover and follow a complex path concatenating multiple CAUSATIVE_AGENT relationships in the SNOMED ontology and leverage the result of this exploration to acquire the documents we are interested in. An example of SNOMED ontology path characterized by a CAUSATIVE_AGENT chain is reported in Figure 7.16.

Figure 7.16 SNOMED ontology paths including only the CAUSATIVE_AGENT relationship. The navigation of this subset of relationships is particularly useful for identifying diseases that are carried by the same virus types.

Figure 9.14 shows that compared to the examples related to “islets of Langerhans”, we can expand the search involving entities such as “Yellow fever”, “Rift valley fever”, and “Rubella” that do not have a direct connection with the entity specified in the search query (“Zika virus”). Listing 7.27 shows the query to leverage these complex paths for retrieving relevant documents. This example related to “Zika virus” is particularly exciting for our application domain because it allows you to retrieve documents mentioning similar diseases based on the type of virus that they have in

common.

Listing 7.27 Get documents and pages mentioning diseases caused by Togaviruses

MATCH (m1:MedicalEntity)-[:IS_SNOMED_ENTITY]->(s1:SnomedEntity)

-[r1:SNOMED_RELATION*3..3]-(s2:SnomedEntity)<-[:IS_SNOMED_E WHERE m1.name = "Zika Virus" AND all(x IN r1 WHERE x.type = "CAUSA WITH DISTINCT e

MATCH path = (f:File)-[:CONTAINS_PAGE]->(p)-[r:MENTIONS_MENTION]- WITH f, e, collect(p.page_idx) AS pages_list // #C RETURN DISTINCT

f.id AS `File ID`, pages_list, collect(DISTINCT e.name) AS `Mentioned entity` ORDER BY size(`Mentioned entity`) DESC LIMIT 5

The result of this query is reported in table 7.4.

Table 7.4 Documents and pages mentioning diseases caused by Togaviruses.

File ID Pages list

Mentioned entities

"sample_dataset- PublicUse/ECDC Documents/west nile virus/TER- Mosquito-surveillance- guidelines.pdf”

[10] ["Murray valley encephalitis", "Japanese Encephalitis", "Venezuelan equine encephalomyelitis", "Encephalomyelitis, Eastern Equine", "Ross river virus infection"] "sample_dataset- PublicUse/ECDC Documents/ebola/ebola- preparedness-belgium.pdf"

[38] ["Yellow Fever", "Rift Valley Fever", "West Nile Fever", "Dengue Fever", "Chikungunya Fever"] "sample_dataset- PublicUse/ECDC Documents/west nile virus/communicable-disease- threats-report-18-august-2019-

[1] ["Rubella", "Yellow Fever", "Japanese Encephalitis", "Dengue Fever", "Chikungunya Fever"]

updated-26-august-2019.pdf" "sample_dataset- PublicUse/ECDC Documents/ebola/communicable- disease-threats-report-18-august- 2019-updated-26-august- 2019.pdf"

[1] ["Rubella", "Yellow Fever", "Japanese Encephalitis", "Dengue Fever", "Chikungunya Fever"]

"sample_dataset- PublicUse/ECDC Documents/west nile virus/communicable-disease- threats-report-15-december- 2018.pdf"

[1] ["Yellow Fever", "Dengue Fever", "Mosquito-Borne Diseases", "Chikungunya Fever"]

The use case described in this section is particularly useful for retrieving in an automatic way relevant information leveraging ontology connections. Starting from the entity mentioned in the search string, it is possible to navigate between multiple pieces of text that mention entities related to this search entity, opening new possibilities to enrich the user experience within an IAS system.

7.6.3 KG-based interpretability and discovery

Conceptual and structured knowledge-based search improve the retrieval mechanism of textual contents based on the knowledge in the ontologies. Conceptual search allows you to reconcile diverse strings with the same meaning or distinguish similar expressions that refer to distinct entities. Structured knowledge-based search enables the user to capture multiple pieces of text mentioning different entities connected by complex ontology relationships.

The KG-based interpretability and discovery adds a further step: starting from the co-occurrence of entities in the same sentence, it allows you to analyze how these entities are connected within the ontologies and what is the nature of this connection. In some cases, the ontology connections reflect the reason for the co-occurring entities in a specific sentence (interpretability). From another perspective, this analysis also represents a

validation task of co-occurring entities against an ontology. In other cases, ontology connections add further knowledge that expands the information provided by the sentence (discovery).

To better understand the value of this use case, consider the following example, which includes sentences from the document entitled: “Proposal for a Directive of the European Parliament and of the Council setting standards of quality and safety for the collection, testing, processing, storage, and distribution of human blood and blood components and amending Council Directive 89/381/EEC”. In this document, the entities “AIDS” and “Hepatitis” co-occur two times (see Listing 7.28):

Listing 7.28 Example of sentences where “AIDS” and “Hepatitis” co-occur

SENTENCE 1: The reasons why they should not donate which put reci

[…]

SENTENCE 2: Infectious diseases persons suffering or having suffe - Babesiosis - Hepatitis B (HBsAg confirmed positive) - Hepatitis C - Hepatitis, infectious (of unexplained aetiology) - HIV/AIDS

In these sentences, “AIDS” and “Hepatitis” appear closely because they both represent a risk factor in the donation process, being both infectious diseases. This information is directly encoded in SNOMED, and it is expressed by different ontology paths. Listing 7.29 shows a subset of these paths:

Listing 7.29 Paths connecting “AIDS” and “Inflammatory disorder of liver” (interpretability perspective)

(AIDS)-[:PATHOLOGICAL_PROCESS]->(Infectious disease)<-[:DUE_TO]-(

(AIDS)-[:PATHOLOGICAL_PROCESS]->(Infectious disease)<-[:DUE_TO]-(V

Ontology paths in listing 7.29 define connections between these two entities and clearly encode the meaning of the co-occurrence of “AIDS” and “Hepatitis” (labeled as “Inflammatory disorder of liver” in the SNOMED ontology). From another perspective, we validated the extracted entities

against the ontology, which enables us to understand why these entities were extracted in the same sentence. Other types of SNOMED paths reveal further information that allows you to discover interesting connections between “AIDS” and “Hepatitis”. For example, the SNOMED paths reported in Listing 7.30 show that “AIDS” is related to a disorder defined as “Hepatomegaly associated with AIDS” that affects the liver. Moreover, the second entry in Listing 7.30 reports that a specific form of hepatitis called “Lupus hepatitis” involves the immune system as “AIDS”.

Listing 7.30 Paths connecting “AIDS” and “Inflammatory disorder of liver” (discovery perspective)

(AIDS)<-[:ASSOCIATED_WITH]-(Hepatomegaly associated with AIDS (di

(AIDS)-[:HAS_DEFINITIONAL_MANIFESTATION]->(Immune system finding)

The examples of paths reported in listing 7.29 and 7.30 show that the ontology supports us in understanding why a pair of medical entities exists in the sentence and, on the other hand, allows us to discover new details related to this specific pair. Figure 7.17 provides a description of the opportunity to integrate domain ontologies into a KG built from natural language content. It shows “Dengue” and “Zika virus” as examples of co-occurrence that will be discussed later in this section.

Figure 7.17 Mental model describing the KG-based interpretability and discovery process. In this case, co-occurring entities in the text can be connected by multiple ontology paths in the SNOMED ontology.

This use case has practical applications for analysts. They immediately access further knowledge related to a specific portion of the text. As we have seen from the example, this knowledge can be a synthesis of the information located in the text (“AIDS-[:PATHOLOGICAL_PROCESS]->Infectious disease<-[:DUE_TO]-Viral hepatitis-[:IS_A]->Inflammatory disorder of liver”), or it is additional knowledge that can be integrated into the analysis (“AIDS<-[:ASSOCIATED_WITH]-Hepatomegaly associated with AIDS

(disorder)-[:FINDING_SITE]->Liver<-[:FINDING_SITE]-Inflammatory disorder of liver”).

Analysts can derive interesting patterns from co-occurring entity types. For example, in the case of multiple co-occurring diseases, we can verify what are the ontology paths that connect these diseases. From the opposite perspective, the absence of interesting paths between co-occurring entities represents a signal of new knowledge that is not already encoded in biomedical ontologies (this specific case is discussed in subsection 9.6.3). Let’s dive into the details and see how to apply practical solutions in the SoHO domain to enable interpretability and discovery with a real KG. Listing 7.31 retrieves the top-5 entity types that co-occur with the “Zika virus” entity.

Listing 7.31 Query for retrieving the top co-occurring entity types of Zika virus

MATCH (m1:MedicalEntity)-[r:COOCCURR]-(m2:MedicalEntity) WHERE m1.id= "C0318793" RETURN m2.type as `Entity Type`, count(m2.type) as `Number of co- ORDER BY count(m2.type) DESC LIMIT 5

The query provided in listing 9.31 computes the number of co-occurring entity types in order to identify the most relevant ones. Results are reported in table 7.5.

Table 7.5 Top co-occurring entity types of Zika virus

Entity Number of co-occurrences “Geographic A rea” 255 “Qualitative Conc ept” 132 “Disease or Syndrome” 125 “Functional Conc ept” 106 “Finding” 98

Results available in table 7.5 show that most of the results include the “Geographic Area”. This is related to the fact that the “Zika virus” entity is mentioned in many cases within the bulletins that report the diffusions of communicable diseases in different areas of the world. Other categories, such

as “Qualitative Concept” and “Functional Concept” include broader concepts that, in most cases, are not strictly related to the medical domain and that are not relevant to our specific purpose. For this reason, we will focus our investigation on the co-occurring “Disease or Syndrome” entities. Listing 7.32 shows the query to the co-occurring entities and the portion of text that shows from which this co-occurrence has been extracted.

Listing 7.32 Get disease entities that co-occur with “Zika virus” and show the context

MATCH (n1:MedicalEntity)-[r:COOCCURR]-(n2:MedicalEntity) WHERE n1.id = "C0318793" AND n2.type = "Disease or Syndrome" // #A WITH n1, r, n2 ORDER BY r.count DESC MATCH (f:File)-[:CONTAINS_PAGE]->(p:Page)

-[r1:MENTIONS_MENTION]->(m1)-[:DISAMBIGUATED_TO]->(n1), (p)-[r2:MENTIONS_MENTION]->(m2)-[:DISAMBIGUATED_TO]->(n2) WHERE r1.sentence_index = r2.sentence_index // #B WITH f, p, r1, r2, n2 RETURN DISTINCT

f.id AS `File ID`, p.page_idx AS `Page index`, n2.id AS `Co-occurring entity id`, n2.name AS `Co-occurring entity name`, CASE WHEN r1.end_chars[0] > r2.start_chars[0] THEN substring(p.text, r2.start_chars[0], r1.end_chars[0] - r ELSE substring(p.text, r1.start_chars[0], r2.end_chars[0] - r END AS `Mentions context`

In table 7.6, we report only a subset of the results that are relevant to our discussion, but we invite the reader to analyze the full result set by running the query in listing 7.32.

Table 7.6 Disease entities that co-occur with “Zika virus” and related mentions

File ID Page index

Co- occurring entity id

Co-occurring entity name

M

"sample_dataset- PublicUse/ECDC Documents/zika

8 "C0009450" "Communicable

" D T

Diseases"

virus/Communicable-disease- threats-report-23-apr-2016.pdf"

C t r a o "sample_dataset- PublicUse/ECDC Documents/ebola/Communicable- disease-threats-report-01-jul- 2017.pdf"

" D T Z b "sample_dataset- PublicUse/ECDC Documents/zika virus/Questions and answers about Zika virus disease.pdf"

17 "C0009450" "Communicable

Diseases"

“ R i t T E i i t m a v "sample_dataset- PublicUse/ECDC Documents/west nile virus/Communicable-disease- threats-report-2-jul-2016.pdf"

4 "C0276289" "Zika Virus

Infection"

1 "C0018378" "Guillain-Barre

" o G s

Syndrome"

"sample_dataset- PublicUse/EDQM Documents/BTC/optimal-use-of- clotting-factors-and-platelets- european-symposium- proceedings-wildbad-kreuth- initiative-iv-freising- germany.PDF"

300 "C0011311" , "C0008055"

"Dengue Fever", " Chikungunya Fever"

" d i a d e s

Table 7.6 reports the result of the query. We notice that “Zika virus” and

“Communicable Diseases” (UMLS id: “C0009450”) occur many times in the same sentence. This is due to the fact that many of the ingested bulletins have a similar structure, such as: “COMMUNICABLE DISEASE THREATS REPORT Zika: No mosquito-borne Zika virus […]”.

However, other co-occurrence examples are more related to the purpose of interpretability and discovery. By analyzing the results, we notice that “Zika virus” co-occurs many times with the following entities: “Zika Virus Infection” (UMLS id: “C0276289”), “Dengue Fever” (UMLS id: “C0011311”), and “Chikungunya Fever” (UMLS id: “C0008055”).

The co-occurrence with “Zika Virus Infection” is clearly understandable. However, the semantic connection between these two entities is directly available in the SNOMED ontology. Listing 7.33 reports a query to show the SNOMED connection between “Zika Virus Infection” (SNOMED id: “3928002”) and “Zika virus” (SNOMED id: “50471002”).

Listing 7.33 Get paths between “Zika Virus Infection” and “Zika virus”

MATCH (s1), (s2) WHERE s1.id = "3928002" AND s2.id = "50471002" WITH s1, s2, allShortestPaths((s1)-[:SNOMED_RELATION*1..8]-(s2)) A UNWIND paths AS path WITH relationships(path) AS path_edges,

nodes(path) AS path_nodes WITH

[n IN path_nodes | n.name] AS node_names, [r IN path_edges | COALESCE(r.type, 'IS_A')] AS rel_types, // # [n IN path_edges | startnode(n).name] AS rel_starts // #C WITH [i IN range(0, size(node_names) - 1) |

CASE WHEN i = size(node_names) - 1 THEN '(' + node_names[i] + ')' // #D WHEN node_names[i] = rel_starts[i] THEN '(' + node_names[i] + ')' + '-[:' + rel_types[i] + ']->' ELSE '(' + node_names[i] + ')' + '<-[:' + rel_types[i] + ']-' END ] AS string_paths RETURN DISTINCT apoc.text.join(string_paths, '') AS `Extracted pa

The query in listing 7.33 returns all the shortest paths between “Zika virus” and “Disease, Zika Virus” in a pretty-printed format. In this case, we used the

corresponding SNOMED ids and labels to execute the query. The result is available in listing 7.34.

Listing 7.34 Paths connecting “Zika virus disease” and “Zika virus”

(Zika virus disease)-[:CAUSATIVE_AGENT]->(Zika virus)

The result available in listing 7.34 clearly shows the connection between these entities. In this case, the SNOMED ontology provides a relationship that clearly “interprets” the meaning of this co-occurrence. In other words, such a connection reflects the linguistic and conceptual proximity of the disease and its causative agent in clinical and epidemiological texts. In our scenario, we provide graph-based augmented information, which makes explicit why these co-occurring entities can be validated. As reported in table 7.6, the “Zika Virus” medical entity also co-occurs with another disease, such as “Dengue Fever”. For this reason, we can explore the ontology paths connecting these two entities by running the query in listing 7.33 replacing the current WHERE clause with s1.id="38362002" AND s2.id="50471002". The result of the query is reported in listing 7.35.

Listing 7.35 Paths connecting “Dengue” and “Zika virus”

(Dengue)-[:IS_A]->(Disease due to Flavivirus)<-[:IS_A]-(Zika viru

(Dengue)-[:IS_A]->(Mosquito-borne flavivirus fever)<-[:IS_A]-(Zik

(Dengue)-[:IS_A]->(Viral disease)<-[:IS_A]-(Zika virus disease)-[

(Dengue)-[:PATHOLOGICAL_PROCESS_(ATTRIBUTE)]->(Infectious process

(Dengue)-[:CAUSATIVE_AGENT]->(Arthropod-borne organism)<-[:CAUSAT

(Dengue)-[:PATHOLOGICAL_PROCESS_(ATTRIBUTE)]->(Infectious process

(Dengue)-[:PATHOLOGICAL_PROCESS]->(Infectious disease)<-[:PATHOLO

(Dengue)-[:CAUSATIVE_AGENT]->(Togavirus)<-[:CAUSATIVE_AGENT]-(Zik

From the results reported in listing 7.35, there are multiple paths that can connect “Dengue” and “Zika virus”. These paths include key entities such as “Viral disease” and “Infectious disease” that allow us to interpret why these

diseases co-occur in the same sentence (see the last row of table 7.6). From another perspective, other key entities such as “Disease due to Flavivirus”, “Toga virus”, and “Mosquito-borne flavivirus fever” allowed us to discover further details to understand “Dengue” and “Zika virus” can be connected, regardless of the sentence in which they are mentioned.

This validation process can be extended to other biomedical ontologies, including the Human Phenotype Ontology (HPO). A representative use case, as illustrated in the results from the query available in listing 7.36, demonstrates how co-occurrence patterns between phenotypic features (e.g., Renal cell carcinoma, Leukemia) and associated disease entities (e.g., von Hippel-Lindau syndrome, Colorectal cancer, RETINOBLASTOMA) can be systematically extracted and validated. This approach enables the identification of clinically relevant phenotype–disease associations grounded in document-level evidence.

Listing 7.36 HPO disease entities that co-occur with HPO phenotypic features in text

MATCH (d:HpoDiseaseEntity)<-[:IS_DISEASE_ENTITY]-(n1:MedicalEntit

-[r:COOCCURR]-(n2:MedicalEntity)-[:IS_HPO_ENTITY]->(h:HpoEn WHERE EXISTS((d)-[:HAS_PHENOTYPIC_FEATURE]->(h)) AND n2.id in ["C WITH n1, n2, d, h

MATCH (f:File)-[:CONTAINS_PAGE]->(p:Page)

-[r1:MENTIONS_MENTION]->(m1)-[:DISAMBIGUATED_TO]->(n1), (p)-[r2:MENTIONS_MENTION]->(m2)-[:DISAMBIGUATED_TO]->(n2) WHERE r1.sentence_index = r2.sentence_index // #B WITH f, p, r1, r2, n1, n2, d, h

RETURN DISTINCT

f.id AS `File name`, p.page_idx AS `Page index`, h.label AS `HpoEntity name`, d.label AS `HpoDiseaseEntity name`, CASE WHEN r1.end_chars[0] > r2.start_chars[0] THEN substring(p.text, r2.start_chars[0], r1.end_chars[0] - r ELSE substring(p.text, r1.start_chars[0], r2.end_chars[0] - r END AS `Mentions context`

As defined for previous examples, listing 7.36 provides the textual context in which these entities co-occur. However, compared to the earlier examples,

we specify that one medical entity has to be a disease and a phenotypic feature indexed in the HPO annotation file. The result of this query is available in table 7.7.

Table 7.7 HPO disease entities co-occurring with Astrocytoma

File ID Page index

Hpo name HpoDise

"sample_dataset-PublicUse/EDQM Documents/BTC/guide-to-the-quality-and- safety-of-organs-for-transplantation-7th- edition.PDF"

275 "Renal cell carcinoma"

"von Hip syndrome

"sample_dataset-PublicUse/EDQM Documents/BTC/guide-to-the-quality-and- safety-of-organs-for-transplantation-7th- edition.PDF"

228 "Renal cell carcinoma"

"Colorec

“sample_dataset-PublicUse/BTC key docs/CELEX_52002PC0319_EN_TXT.pdf”

12 "Leukemia" "RETINO

The use case discussed in this subsection is particularly useful for twofold reasons. In some cases, the connection between co-occurring medical entities in the ontology defines the semantic nature of this relationship, and it explains why these entities should appear together in a sentence. A SNOMED path such as “AIDS-like syndrome-[:CAUSATIVE_AGENT]-> Human immunodeficiency virus” is a clear example in which a disease appears together with the virus that causes it. In this case, we can create a sort of graph-based augmented information, which make explicit why these co- occurring entities can be validated.

In other cases, the ontology provides further information compared to the trivial one. As discussed at the beginning of the subsection, “AIDS” and “Hepatitis” (also known as “Inflammatory disorder of liver””) usually co- occur because they are both related to an infectious process. However, more complex paths such as (“AIDS<-[:ASSOCIATED_WITH]-Hepatomegaly associated with AIDS (disorder)-[:FINDING_SITE]->Liver<- [:FINDING_SITE]-Inflammatory disorder of liver” allow us to discover new information that is not so explicit in the textual content.

In the next subsection, we will discuss a different use case in which co- occurring entities do not have a relevant connection in an ontology. This is particularly useful to uncover new information directly from the text that can be potentially integrated into domain ontologies to improve the established knowledge related to medical entities in a rapidly changing environment.

7.6.4 New knowledge uncovering

The KG-based interpretability and discovery use case allows us to identify ontology paths that connect co-occurring entities in the same sentence. However, in a context that changes very quickly due to research advancements, part of the knowledge located in the text is not yet consolidated in medical ontologies. As a consequence, entities that co-occur in the same sentence may not have a direct match in terms of interpretability and discovery in the domain ontologies.

For this reason, instead of using the ontology to enrich the information of the KG, we can use KG built from textual content to suggest new facts that can

be integrated into the ontology. This use case is particularly relevant because it allows us to identify new patterns in the text that can be useful in the medical domain. For instance, we can discover that information that appears repeatedly in the text, including the causes related to a specific disease, is not yet defined by a medical ontology. To identify this type of pattern, let’s recap the diseases that co-occur most with “Zika Virus” by running the query in listing 7.37.

Listing 7.37 Top co-occurring diseases with Zika Virus in the text

MATCH (n1:MedicalEntity)-[r:COOCCURR]-(n2:MedicalEntity) WHERE n1.id= "C0318793" and n2.type = "Disease or Syndrome" RETURN n2.name, r.count ORDER BY r.count DESC LIMIT 5

The result of the query in listing 7.37 is available in table 7.8.

Table 7.8 Result of the top co-occurring diseases with Zika Virus

Disease entity Occurrences "Communicable Diseases" 606 "Zika Virus Infection" 520 "Dengue Fever" 206 "Guillain-Barre Syndrome" 195 "Chikungunya Fever" 155

In the results reported in table 7.8, several diseases are associated with the “Zika Virus” entity. Among them, Guillain-Barré Syndrome stands out as the most interesting because, unlike the others, it is not a vector-borne disease. This makes its co-mention potentially unexpected and clinically significant, suggesting a possible complication rather than mere co-endemicity.

For this reason, it is helpful to get further details by exploring the textual content that mentions both “Zika Virus” and the “Guillain-Barre Syndrome”. The reader can reuse the query in listing 7.32 by replacing the first WHERE clause with n1.id= "C0318793" AND n2.id = "C0018378", which represent the UMLS ids of “Zika Virus” and “Guillain-Barre Syndrome”, respectively

(we also removed from the query results the “Co-occurring entity id” and the “Co-occurring entity name” fields because they are not useful in this case). The cleaned result of this query, obtained by removing redundant results, is available in table 7.9.

Table 7.9 Pieces of text mentioning both “Zika Virus” and “Guillain-Barre Syndrome”

File ID Page index

Mentions context

"sample_dataset-PublicUse/ECDC Documents/ebola/communicable-disease- threats-report--20-feb-2016.pdf"

3 "Zika and potential complications published on 12 February 2016, six countries have reported an increase in the incidence of cases of microcephaly (Brazil) and Guillain- Barré syndrome" "sample_dataset-PublicUse/ECDC Documents/zika virus/zika-virus-rapid-risk- assessment-11-april-2016.docx.pdf"

2 "Zika virus epidemic, health services and practitioners should be alerted to the possible occurrence of neurological syndromes (Guillain- Barré syndrome" "sample_dataset-PublicUse/ECDC Documents/ebola/communicable-disease- threats-report-16-june-2019.pdf"

3 "Guillain-Barré is known to be triggered by bacterial infections, respiratory viruses, enteroviruses and arboviruses such as dengue and Zika"

The results available in table 7.9 clearly state the “Zika Virus” is one of the causes of the “Guillain-Barre syndrome”. For this reason, we expect that

ontologies such as SNOMED include a fact such as “the “Guillain-Barre syndrome-[:CAUSATIVE_AGENT]->Zika virus”. Let’s check it by running the query in listing 9.33 replacing the “Zika Virus Disease” (SNOMED id: 3928002) with the “Guillain-Barre syndrome” (SNOMED id: 40956001) in the WHERE clause. By running this query, the reader will notice that multiple SNOMED ontology paths connect these entities. However, most of these paths do not represent meaningful connections between these entities. Consider, for instance, a subset of these results reported in listing 7.38.

Listing 7.38 Paths connecting “Zika virus” and “Guillain-Barre syndrome”

(Zika virus)<-[:CAUSATIVE_AGENT]-(Congenital Zika virus infection

(Zika virus)<-[:CAUSATIVE_AGENT]-(Zika virus disease)-[:CAUSATIVE_

In the first entry of listing 7.38, “Zika virus” is connected to the “Guillain- Barre syndrome” (known as “Infectious neuronitis” in the SNOMED ontology) because the first one is the causative agent of the Congenital Zika virus infection, which is an infectious process. The issue is that the “Infectious process (qualifier value)” node is connected to multiple contagious processes, such as the reported “Reactivation of hepatitis B viral hepatitis (disorder)”. This disorder is associated with inflammation, which is also a morphological manifestation of “Infectious neuronitis”. Similar reasoning can be applied to the second entry reported in listing 9.38. As the reader can guess, these paths have been retrieved because they include hub nodes, such as “Infectious process (qualifier value)” and “Inflammation”, which are connected to a massive number of nodes related to infectious processes.

One solution is to filter out paths that include these hub nodes. To reach this goal, we can leverage graph data science algorithms provided by the Neo4j Graph Data Science Library (GDS), which offers a set of tools to compute the centrality of a node in the graph, leveraging features such as the node's degree. To run this algorithm, the first step is to create a graph projection that includes the relevant set of nodes and relationships to perform our analysis. In this case, we want to focus on the SNOMED ontology graph, and we can run the code available in listing 7.39.

Listing 7.39 Create a projection on the SNOMED ontology

CALL gds.graph.project(

'snomedGraph', 'SnomedEntity', 'SNOMED_RELATION' ) YIELD

graphName AS graph, relationshipProjection AS knowsProjection, nodeCount AS nodes, relationshipCount AS rels;

After generating the projection, we can run the degree algorithm in the streaming mode to retrieve the most connected nodes and use this result to filter out paths that are not relevant to our purpose. You can perform this task by running the query in listing 7.40.

Listing 7.40 Paths obtained by filtering hub nodes

CALL gds.degree.stream('snomedGraph') YIELD nodeId, score WITH gds.util.asNode(nodeId).name AS name, score AS degree ORDER BY degree DESC LIMIT 350 WITH collect(name) AS hub_nodes // #A

MATCH (s1), (s2) WHERE s1.id = "3928002" AND s2.id = "40956001" WITH s1, s2, allShortestPaths((s1)-[:SNOMED_RELATION*1..8]-(s2)) A UNWIND paths AS path WITH relationships(path) AS path_edges, nodes(path) AS path_nodes, hub_nodes WITH [n IN path_nodes | n.name] AS node_names, [r IN path_edges | COALESCE(r.type, 'IS_A')] AS rel_types, // #C [n IN path_edges | startnode(n).name] AS rel_starts, // #D hub_nodes WHERE NOT any(x IN node_names WHERE x IN hub_nodes) // #E WITH [i IN range(0, size(node_names) - 1) | CASE WHEN i = size(node_names) - 1 THEN '(' + node_names[i] + ')' // #F WHEN node_names[i] = rel_starts[i]

THEN '(' + node_names[i] + ')' + '-[:' + rel_types[i] + ']->' // ELSE '(' + node_names[i] + ')' + '<-[:' + rel_types[i] + ']-' // END ] AS string_paths RETURN DISTINCT apoc.text.join(string_paths, '') AS `Extracted pa

The query provided in listing 7.40 combines the nodes' degree computation with the detection of the relevant paths connecting two different entities. More specifically, the paths that include nodes with a higher degree are excluded from the results. Listing 7.41 reports the results after this filtering process.

Listing 7.41 Paths connecting “Zika virus disease” and “Infectious neuronitis”

(Zika virus disease)-[:IS_A]->(Viral disease)<-[:IS_A]-(Erythema

(Zika virus disease)-[:IS_A]->(Disease due to Flavivirus)<-[:IS_A

(Zika virus disease)<-[:ASSOCIATED_FINDING]-(Presumptive recent Z

(Zika virus disease)-[:IS_A]->(Viral disease)<-[:IS_A]-(Common co

(Zika virus disease)-[:IS_A]->(Viral disease)<-[:IS_A]-(VHF – Vir

(Zika virus disease)<-[:ASSOCIATED_FINDING]-(Presumptive recent Z

(Zika virus disease)-[:IS_A]->(Viral disease)<-[:TEMPORALLY_FOLLOW

(Zika virus disease)-[:IS_A]->(Viral disease)<-[:DUE_TO]-(Erythem

(Zika virus disease)-[:IS_A]->(Viral disease)<-[:AFTER]-(Transien

As the reader may observe, by filtering out the paths that include the hub nodes, we retrieved a result set of 9 entries compared to the 11,185 paths detected with the base query (updated version of listing 7.33 obtained by replacing the “Zika Virus Disease” (SNOMED id: 3928002) with the “Guillain-Barre syndrome” (SNOMED id: 40956001)). However, the results still include too general entities such as “Viral disease”, which connects the “Zika virus disease” to other random viral diseases that can be used to connect “Zika virus disease” to “Infectious neuronitis". In other words, a direct and straightforward connection between these entities is not encoded in the SNOMED ontology. Therefore, this is a typical example where the co-

occurrence of entities defined in the KG can be potentially leveraged for enriching a domain ontology in a virtuous circle.

This mechanism can be exponentially enhanced by extracting meaningful relationships from the text. By adopting this approach, the starting point is represented by the extracted relationships, which are more specific than the co-occurrence connection. Figure 7.18 shows a mental model that describes the entire process.

Figure 7.18 Mental model describing the new knowledge uncovering process. In this case, “Zika virus” and “Guillain-Barré syndrome” appear multiple times in the same sentences but the medical ontologies do not provide any relevant connection between these entities.

This example shows that the “New knowledge uncovering” use case is beneficial for discovering new information in textual content using KG technology to extend and amplify established knowledge located in biomedical ontologies.

7.7 Summary

Named Entity Disambiguation (NED) is a crucial step that allows you to connect entities mentioned in the text to a reference knowledge base. The combination of NED with KG technologies opens new opportunities to develop advanced services in critical domains, such as the management of substances of human origin. This application domain is particularly complex because it involves different types of unstructured content, such as regulation proposals, stakeholders’ positions, guidelines for SoHO management, bulletins to monitor infectious diseases. From these documents, we can extract medical entities and incorporate biomedical ontologies to enrich the extracted information and represent all this heterogenous information in a KG-based unified view. The KG construction requires multiple steps, including the definition of the schema, the ingestion of the documents, the disambiguation of medical entities from these documents, the integration of the domain ontologies and the mapping the extracted medical entities, the generation of a co-occurrence relationship between medical entities located in the same sentence. Advanced analysis can be performed on this KG to support different use cases in the application domain: conceptual search, structured knowledge-based search, KG-based interpretability and discovery, and new knowledge uncovering.

7.8 References

1. Bodenreider, O. (2004). The unified medical language system (UMLS):

integrating biomedical terminology. Nucleic acids research, 32(suppl_1), D267-D270. https://academic.oup.com/nar/article- abstract/32/suppl_1/D267/2505235 2. Cornet, R., & de Keizer, N. (2008). Forty years of SNOMED: a

literature review. BMC medical informatics and decision making, 8(1), 1-6. https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/1472 6947-8-S1-S2 3. European Centre for Disease Prevention and Control. (2016).

Communicable disease threats report. 4. Köhler, S., Gargano, M., Matentzoglu, N., Carmody, L. C., Lewis-

Smith, D., Vasilevsky, N. A., ... & Robinson, P. N. (2021). The human phenotype ontology in 2021. Nucleic acids research, 49(D1), D1207- D1217. 5. Neumann, M., King, D., Beltagy, I., & Ammar, W. (2019). ScispaCy:

fast and robust models for biomedical natural language processing. arXiv preprint arXiv:1902.07669. https://arxiv.org/abs/1902.07669 6. Shi, J., Yuan, Z., Guo, W., Ma, C., Chen, J., & Zhang, M. (2023).

Knowledge-graph-enabled biomedical entity linking: a survey. World Wide Web, 1-30. https://link.springer.com/article/10.1007/s11280-023- 01144-4 7. Widenfalk, B., Rastad, J., Wilbrand, S., & Vinnars, B. (2014). Guide to

the quality and safety of tissues and cells for human application.

[1] ChatGPT is based on a generative model that at the time of writing this book is continuously evolving. For this reason, the reader can receive different responses even among close tests and experiments.

[2] Amazon Textract is a machine learning (ML) service provided by AWS that automatically extracts text, handwriting, and data from scanned documents. Further information are available at: https://aws.amazon.com/textract/

## 8_NED_with_open_LLMs_and_domain_ontologies

8 NED with open LLMs and domain ontologies

This chapter covers

Understanding limitations of traditional Named Entity Disambiguation (NED) tools Combining general-purpose LLMs and domain ontologies for the NED task Introducing a novel, multi-step approach for disambiguation, including shortest-path detection, path-to-text translation, and textual path summarization

Chapter 7 focused on Named Entity Disambiguation (NED), highlighting the role of ScispaCy, a specialized natural language processing (NLP) tool built on the spaCy framework. This tool is designed for processing documents and publications by providing pre-trained models in the biomedical domain.

ScispaCy incorporates specific vocabularies and ontologies, such as the Unified Medical Language System (UMLS), which provides canonical entities useful for disambiguating mentions in the text.

However, this approach presents some limitations:

It is specifically designed for a particular application domain, namely the biomedical field. It presents challenges in expanding and updating the reference knowledge base to incorporate new entities and terminologies (e.g., additional aliases for existing entities). It fails to fully leverage the extensive information available within the knowledge base.

More specifically, ScispaCy does not leverage the existing relationships and paths between entities for the disambiguation task. To understand the impact

of this last point, let’s recap the example we discussed at the beginning of chapter 7:

“In the week of 13 April, Belize reported for the first time mosquito-borne Zika virus transmission. Update on the observed increase of congenital Zika syndrome and other neurological complications Microcephaly and other fetal malformations potentially associated with Zika virus infection.”

In this scenario, the contextual words surrounding the Zika term are leveraged by Scispacy to detect the correct disambiguate entities. The results are in listing 8.1:

Listing 8.1 Candidate selection and ranking with ScispaCy model

Recognized entity: Zika virus 75 85 Ranked target candidates: - C0318793 Zika Virus #A - C0276289 Zika Virus Infection - C4687930 Zika Virus Antibody Measurement

Recognized entity: congenital Zika syndrome 135 159 Ranked target candidates: - C4546023 Congenital Zika Syndrome #B

Recognized entity: Zika virus infection 268 288 Ranked target candidates: - C0276289 Zika Virus Infection #C - C0318793 Zika Virus - C4687930 Zika Virus Antibody Measurement

The term “Zika” occurs three times in this short text, and ScispaCy was able to detect the correct disambiguation. However, when we test a slightly different example and remove surrounding words such as “congenital” and “syndrome” the accuracy of ScispaCy decreases rapidly. Consider the following example:

“Zika belongs to the Flaviviridae virus family, and it is spread by Aedes mosquitoes. Individuals affected by Zika disease and other syndromes like chikungunya fever often experience symptoms like viral myalgia, infectious edema, and infective conjunctivitis. Severe outcomes of Zika are due to its

capacity to cross the placental barrier during pregnancy, causing microcephaly and congenital malformations.”

Compared to the previous example, there are no surrounding words that can support the disambiguation phase. Let’s consider the output of ScispaCy in Listing 8.2:

Listing 8.2 Candidate selection and ranking with ScispaCy model

Recognized entity: Zika 0 4 Ranked target candidates: - C0276289 Zika Virus Infection - C0318793 Zika Virus - C4687930 Zika Virus Antibody Measurement

Recognized entity: Zika disease 109 121 Ranked target candidates:

Recognized entity: Zika 278 282 Ranked target candidates: - C0276289 Zika Virus Infection - C0318793 Zika Virus - C4687930 Zika Virus Antibody Measurement

As you noticed from these results, the model disambiguated the “Zika” term as the entity “C0276289 Zika Virus Infection” in the case of the first and the third sentence, and it was not able to detect any target entity for the mention in the second sentence.

Considering these limitations, this chapter offers a novel approach using open large language models (LLMs) and domain ontologies to address them. Moreover, compared to using domain-based tools such as ScispaCy, the proposed approach can be used in other application domains in which we can employ rich ontologies.

8.1 Domain Ontology Ingestion

To drive the disambiguation process, we will use the SNOMED (Systematized Nomenclature of Medicine) ontology introduced in chapter 7 as the reference knowledge base. Let’s recap the input files that compose the

SNOMED ontology.

8.1.1 Details on SNOMED

SNOMED is one of the most comprehensive, multilingual clinical terminology repository, encompassing more than 450,000 concepts and a rich set of relationship types between these concepts.

For the application in our scenario, we will use these two files:

sct2_Description_Full-en_US1000124_20220901.txt: this file includes all the entities' names (and aliases) and relationships formed in the triplet file that define the relationship between entities. sct2_Relationship_Full_US1000124_20220901.txt: this file lists a set of triplets (plus other metadata) that define all the relationships between the SNOMED entities. Each entity and relationship is identified using a numerical code.

These files are released using a TSV format. listings 8.3 and 8.4 provide samples of the description and the relationship files, respectively.

Listing 8.3 SNOMED – sct2_Description_Full-en_US1000124_20220901.txt

130586015 20020131 1 900000000000207008 78696007 e

63434017 20020131 1 900000000000207008 38362002 en

41993017 20020131 1 900000000000207008 25064002 en

Listing 8.3 provides details of a SNOMED entry, including the code and name for “Islets of Langerhans (id: 78696007),” “Dengue Fever (id: 38362002),” and “Headache (id: 25064002)” (or “Cephalgia”). Listing 8.4 illustrates how these entities function as either source or target entities within specific semantic relationships.

Listing 8.4 SNOMED – sct2_Relationship_Full_US1000124_20220901.txt

169174023 20020131 1 900000000000207008 360555004

182243021 20020131 1 900000000000207008 20927009 3

424787021 20020131 1 900000000000207008 54012000 2

Listing 8.4 shows that “Islets of Langerhans”, “Dengue Fever”, “Cephalgia” can be target entities of a “IS_A” relationship (id: 116680003). The source entities are “Endocrine pancreas cell” (id: 360555004), “Dengue hemorrhagic fever” (id: 20927009), and “Posttraumatic headache” (id: 54012000), respectively.

8.1.2 SNOMED Loading

This section provides details on loading SNOMED in Neo4j. Listing 8.5 shows how to create nodes and relationships in Neo4j from the sct2_Relationship_Full_US1000124_20220901.txt.

Listing 8.5 SNOMED ingestion – Load relationships

[…] class SnomedRelationshipsImporter(BaseImporter): #A

def __init__(self, argv): super().__init__(file=SNOMED_RELS_FILE, argv=argv) def set_constraints(self): #B queries = [ "CREATE CONSTRAINT IF NOT EXISTS FOR (n:SnomedEntity) REQUI "CREATE INDEX snomedNodeName IF NOT EXISTS FOR (n:SnomedEnt "CREATE INDEX snomedRelationId IF NOT EXISTS FOR ()-[r:SNOM "CREATE INDEX snomedRelationType IF NOT EXISTS FOR ()-[r:SN "CREATE INDEX snomedRelationUmls IF NOT EXISTS FOR ()-[r:SN "CREATE FULLTEXT INDEX snomedEntityName IF NOT EXISTS FOR ( ]

for q in queries: self.connection.query(q, db=self.db) def import_snomed_rels(self): #C query = """ UNWIND $batch as item MERGE (e1:SnomedEntity {id: item.sourceId}) MERGE (e2:SnomedEntity {id: item.destinationId})

MERGE (e1)-[:SNOMED_RELATION {id: item.typeId}]->(e2) FOREACH(ignoreMe IN CASE WHEN item.typeId = '116680003' TH MERGE (e1)-[:SNOMED_IS_A]->(e2) // #D ) """ size = self.get_csv_size(snomedRels_file) #E self.batch_store(snomed_rels_query, self.get_rows(snomedRels_file

SNOMED includes hundreds of relationships. For this reason, to keep the graph schema as simple as possible, we decided to create a unique SNOMED_RELATION, and we store the relationship name as a type property. As you may notice, in listing 8.5 we created a specific SNOMED_IS_A relationship to define the hierarchical connections. The goal of the class in listing 8.6 is to extract information from the sct2_Description_Full- en_US1000124_20220901.txt file.

Listing 8.6 SNOMED ingestion – Load names and aliases

[…] class SnomedNamesImporter(BaseImporter):

def __init__(self, argv): super().__init__(file=SNOMED_NAMES_FILE, argv=argv) def import_snomed_names(self, snomedNames_file): snomed_names_concepts_query = """ UNWIND $batch as item MATCH (e1:SnomedEntity)-[r:SNOMED_RELATION {id: item.conc WHERE item.conceptId <> '116680003' AND r.id = item.conce SET r.type = CASE WHEN r.type IS NULL THEN item.termAsType ELSE r.type END, // #A r.aliases = CASE WHEN item.termAsType IN r.aliases THEN r.aliases ELSE coalesce(r.aliases,[]) + item.termAsType END """

snomed_names_entities_query = """ UNWIND $batch as item MATCH (e:SnomedEntity {id: item.conceptId}) SET e.name = CASE WHEN e.name IS NULL THEN item.term ELSE e.name END, // #C e.aliases = CASE

WHEN item.term in e.aliases THEN e.aliases ELSE coalesce(e.aliases, []) + item.term END // # """ size = self.get_csv_size(snomedNames_file) self.batch_store(snomed_names_concepts_query, self.get_row self.batch_store(snomed_names_entities_query, self.get_row […]

Class in listing 8.6 adds names and aliases to nodes and relationships loaded into the graph. The following step is to propagate information from the root nodes to all the sub-nodes down to the leaf nodes. The first-level nodes represent the archetypal entities of SNOMED in the medical domain, such as diseases, body structures, substances, and events. Figure 8.1 clarifies this propagation mechanism.

Figure 8.1 A sample of the hierarchical structure of the SNOMED ontology. Leveraging this hierarchical structure, nodes located on a deeper level, such as “Ecallantide” and “Retinopathy associated with AIDS”, can be categorized using the information from the first-level nodes, such as “Pharmaceutical product” and “Disease”, which represent the archetypal entities of the ontology.

Listing 8.7 shows how to retrieve all the nodes following the hierarchical structure to propagate information from the first-level nodes to the deeper nodes.

Listing 8.7 SNOMED ingestion – Label propagation from the first-level nodes

[…] class SnomedLabelPropagator(): […]

def get_rows(self): propagation_query = """ MATCH p=(n:SnomedEntity)<-[:SNOMED_IS_A]-(m:SnomedEntity) WHERE n.id= "138875005" // Root node WITH distinct m as first_node

CALL apoc.path.expandConfig(first_node, { // #A relationshipFilter: '<SNOMED_IS_A', minLevel: 1, maxLevel: -1, uniqueness: 'RELATIONSHIP_GLOBAL' }) yield path UNWIND nodes(path) as other_level // #B WITH first_node, collect(DISTINCT other_level) as uniques UNWIND uniques as unique_other_level WITH first_node,unique_other_level WHERE not first_node.name in coalesce(unique_other_l RETURN unique_other_level.id as id, first_node.name """ with self._driver.session(database=self._database) as session: result = session.run(query=propagation_query) for record in iter(result): yield dict(record) […]

The SNOMED_IS_A relationship has been leveraged to propagate the semantic types through the tree structure by leveraging the hierarchical connections between entities.

8.2 Set-Up of Ollama and Llama 3.1 8B Model

In previous chapters, we explored how to leverage OpenAI APIs to perform various Natural Language Processing (NLP) tasks. In this chapter, we extend that knowledge by deploying a Named Entity Disambiguation (NED) system locally using Ollama and Llama 3.1 8B released by Meta.

Ollama is an open-source tool that enables users to run LLMs directly on their local machines. This approach overcomes several challenges associated with cloud-based services, such as data security concerns and reliance on external infrastructure. By running models locally, users gain full control over their data while reducing latency and dependency on external providers.

Llama 3.1 8B, developed by Meta, is an open-source large language model with 8 billion parameters. This model supports a context length of up to 128,000 tokens and is optimized for multilingual information processing across multiple languages. Its design for efficient deployment on consumer- grade hardware makes it particularly accessible for various applications, including NLP tasks like NED on critical data and code generation.

To deploy the Llama 3.1 8B model on your local machine, you will first need to download and install Ollama. The tool is compatible with macOS, Linux, and Windows, and the installation files are available at https://ollama.com/. Ollama offers both command-line and graphical user interface (GUI) options to manage models. In this chapter, we will use command-line instructions to download and deploy Llama 3.1 8B for our NED system (see listing 8.8 for the detailed commands).

Listing 8.8 Ollama commands to download and serve the Llama 3.1 8B model

ollama serve ollama pull llama3.1:latest

Ollama offers built-in compatibility with the OpenAI Chat Completions API, enabling us to interact directly with our locally deployed model using Python code similar to that introduced in previous chapters. This compatibility simplifies the process of integrating the model into our Named Entity Disambiguation (NED) system. Listing 8.9 shows the Python class needed to interact with our model effectively.

Listing 8.9 Running our Llama 3.1 8B model in Python

from openai import OpenAI

class LLM_Model():

def __init__(self, url='http://localhost:11434/v1', key="defa

self.client = OpenAI( base_url= url, #A api_key = key, #B )

def generate(self, messages): response = self.client.chat.completions.create( model="llama3.1:latest", #C messages=messages, temperature=0, max_tokens=4000, top_p=1, frequency_penalty=0, presence_penalty=0, ) # It assumes as response the ChatGPT API format return response.choices[0].message.content

The Chat Completions API provided by Ollama enables you to substitute the current parameters with the OpenAI ones seamlessly. However, the results for the running example, generated using the prompts reported in the following sections, were obtained in October 2024 using the latest version of the Llama 3.1 model.

Using a general-purpose model like Llama 3.1 for NED highlights the versatility and cross-applicability of this approach across various domains. Although Llama 3.1 was not specifically trained for biomedical or highly specialized tasks, our approach wants to demonstrate the potential of large language models to perform well in niche areas when combined with domain- specific ontologies.

8.3 End-to-End NED Process

In this chapter, we decompose each process step and show how, through advanced LLM capabilities and domain-specific ontology support, the system can effectively interpret and disambiguate entities within complex biomedical texts, facilitating more precise information extraction and analysis. Figure 8.2 shows the mental model describing the entire process from the input document to the disambiguated mentions.

Figure 8.2 A comprehensive workflow for a Named Entity Disambiguation (NED) system

designed to leverage Large Language Models (LLMs) and domain-specific ontologies, such as SNOMED, for biomedical text processing. This workflow is organized into distinct stages, each involving various processes and interactions between the LLM and domain ontology to disambiguate entities within input text accurately.

As shown in figure 8.2, the process begins with an input document containing unstructured text, which is analyzed by the LLM-based Named Entity Recognition (NER) component. Here, the LLM identifies and labels relevant biomedical entities in the input text using the knowledge embedded within

the domain ontology. For instance, a term like "Zika" can be recognized as a "Disease" concept according to SNOMED. This initial step is essential for transforming raw text into structured data by annotating mentions of entities, which will then be processed in subsequent stages.

Then, the system moves into the NED Candidate Selection stage. Here, the annotated mentions in the document are used to retrieve multiple candidate entities from the domain ontology. Based on the current implementation, this stage employs a full-text search mechanism to generate a list of possible matches for each identified entity mention. For instance, the term “Zika” may correspond to several entities in SNOMED, such as “Zika Virus,” “Zika Virus Infection,” or “Congenital Zika Virus Infection.” This step is crucial in setting up a pool of potential disambiguation targets, and the system will evaluate them in the following phase to find the most accurate match.

In the final stage, the NED Candidate Disambiguation, the LLM refines its selection to determine the most precise entity match for each mention. This step involves applying a multi-step approach to evaluate each candidate based on several criteria. One key aspect is identifying the shortest path between candidates within the ontology structure. Additionally, the model leverages information from the domain ontology to translate and summarize relevant path details, thereby supporting the disambiguation process with contextual knowledge. This ensures that each entity mention in the input document is mapped accurately to its corresponding disambiguated entity within the ontology.

A notable aspect of this LLM-driven approach is the deep integration of domain-specific ontologies, like SNOMED, at each step of the disambiguation process. This integration enables the LLM to leverage specialized domain knowledge effectively, which is crucial for accurate recognition and disambiguation of biomedical entities. By incorporating the hierarchical and relational structures of the ontology, the model can make more informed decisions about entity classification and disambiguation, especially in complex cases where terms may have multiple meanings or associations.

8.4 Named Entity Recognition

The goal of Named Entity Recognition (NER) is to identify and classify named entities mentioned in unstructured text into predefined categories, such as diseases, organisms, procedures, and more. As discussed in previous chapters, one practical approach is to use prompt engineering, where the types of entities we are interested in are explicitly defined within the prompt. Often, a data scientist or data engineer can work closely with a domain expert to identify and define these entities accurately.

In our scenario, we aim to leverage the comprehensive knowledge embedded within SNOMED, enhancing the LLM with this domain-specific information. By incorporating SNOMED's structured medical knowledge, we enable the LLM to perform more precise and contextually aware entity recognition in biomedical texts. Figure 8.3 illustrates the input and output of the NER process, showcasing how the LLM processes raw text to extract and classify relevant entities.

Figure 8.3 The first stage of NED is Named Entity Recognition (NER). In our scenario, the collection of named entities is derived directly from the ontology. In SNOMED, the categories are defined by the first-level nodes of the ontology, whose information is propagated to all the other nodes.

The NER phase requires retrieving all the pre-defined categories from the SNOMED ontology. Listing 8.10 reports a query to retrieve all these categories.

Listing 8.10 Retrieving pre-defined categories from the SNOMED Ontology

MATCH (n:SnomedEntity) UNWIND n.type as named_entity WITH DISTINCT named_entity, count(named_entity) as num_of_entitie ORDER BY num_of_entities DESC RETURN collect(named_entity) as named_entities

Then, we can use the results from this query in our prompt for the NER task. Listing 8.11 shows a simplified version of the prompt messages we defined for this purpose. The full prompt is in the available code on GitHub.

Listing 8.11 Simplified prompt for NER

system = “You are an assistant capable of extracting named entiti

input = “Risk factors for rhinocerebral mucormycosis include poor

assistant = [

{ "sentence": "Risk factors for rhinocerebral mucormycosis incl "entities": [ { "id": 0, "mention": "Risk factors", "label": "Events" }, { "id": 1, "mention": "rhinocerebral mucormycosis", "label": "Disease" }, { "id": 2, "mention": "poorly controlled diabetes mellitus", "label": "Disease" }, { "id": 3, "mention": "severe immunosuppression", "label": "Qualifier value" } ] } ]

The prompt defined in Listing 8.11 is structured as follows:

System Instruction: The system message specifies to extract occurrences of named entities that belong to specific categories in the medical domain from the SNOMED ontology, avoiding irrelevant or

out-of-scope categories. Input Text: In this example, the input text discusses risk factors for a medical condition (rhinocerebral mucormycosis), listing conditions like diabetes mellitus and immunosuppression. These are medical terms of interest that need to be recognized. Assistant Output: The system responds with a structured JSON output that contains the following fields:

sentence: The system processes text sentence by sentence to ensure that it analyzes each unit of meaning individually. entities: The output includes an array of identified entities, where each entry contains an id to uniquely identify the entity in the sentence, a mention of the named entity found in the text (e.g., "rhinocerebral mucormycosis"), and a label that classifies the entity according to a SNOMED category, such as “Disease”.

An example of input text passed to the LLM model instructed with the NER prompt is reported in listing 8.12.

Listing 8.12 User message for NER phase

user = “Severe outcomes of Zika are due to its capacity to cross

The listing 8.13 reports a subset of the results generated by Llama 3.1:

Listing 8.13 NER output from the sentence reported in 8,12

{

"sentence": "Severe outcomes of Zika are due to its capacity to "entities": [ { "id": 0, "mention": "Zika", "label": "Organism", "start": 19, "end": 22 }, { "id": 1, "mention": "microcephaly", "label": "Clinical finding (finding)", "start": 105,

"end": 116 }, { "id": 2, "mention": "congenital malformations", "label": "Clinical finding (finding)", "start": 122, "end": 145 } ] }

As reported in Listing 8.13, we are able to identify and classify three distinct entities:

Zika: Identified as an "Organism," with its position in the sentence spanning from character 19 to 22. Microcephaly: Labeled as a "Clinical finding (finding)," indicating it is a medical condition. This mention appears from characters 105 to 116. Congenital Malformations: Also labeled as a "Clinical finding (finding)," representing a category of medical conditions related to congenital disorders. It is located between characters 122 and 145 in the sentence.

LLMs exhibit several limitations in accurately detecting the starting and ending characters of a mention within a sentence. For this reason, the “start” and the “end” fields were generated in post-processing by defining the Python function reported in listing 8.14.

Listing 8.14 Python function to compute starting and ending character of a mention

def find_all_mention_indices(self, string, substring):

indices = [] start_index = 0

while True: start_index = string.find(substring, start_index)

if start_index == -1: break # No more occurrences found

end_index = start_index + len(substring) - 1

indices.append((start_index, end_index))

# Move start_index forward to search for the next occurrence start_index += len(substring)

return indices

By leveraging this function, we identify in a deterministic way the position of the mention, which is typical information identified by traditional NER systems.

8.5 Candidate Selection

The second phase of the NED process is Candidate Selection (CS). CS aims to identify a set of relevant entities or concepts that could match the intended meaning of each identified named entity. Figure 8.4 illustrates the input and output of this phase, highlighting how candidate entities are selected.

Figure 8.4 The second stage of NED is the Candidate Selection. In this scenario, for each entity mention detected in the previous step, this stage retrieves a collection of potential candidates that can refer to each mention. The current implementation employs a full-text search but can be potentially extended with more advanced techniques.

The input of CS consists of the mentions annotated in the input text by the NER process, along with the domain ontology, represented by SNOMED. The output is a list of one or more candidate entities associated with each mention.

In contrast to the Named Entity Recognition and Candidate Disambiguation phases, we do not utilize the LLM in this step for several reasons. First, we aim to retrieve candidates directly from the domain ontology rather than relying on the internal knowledge embedded within the LLM. Second, the size of the ontology prevents us from loading it in its entirety within a prompt.

To perform candidate selection efficiently, we leverage Neo4j's full-text search capabilities. These enable us to identify strings in the ontology that closely match each mention. Listing 8.15 provides the code implementation for this full-text search process.

Listing 8.15 Python class for candidate selection

class CandidateSelection: […]

def full_text_query(self): query = """ CALL db.index.fulltext.queryNodes("names", $fulltextQ YIELD node WHERE node:SnomedEntity AND ANY(x IN node.type WHERE RETURN distinct node.name AS candidate_name, node.id A """

return query

def generate_full_text_query(self, input): full_text_query = "" words = [el for el in input.split() if el]

if len(words) > 1: for word in words[:-1]: full_text_query += f" {word}~0.80 AND " full_text_query += f" {words[-1]}~0.80" else: full_text_query = words[0] + "~0.80"

return full_text_query.strip() […]

Analyzing this query, we specified the “$labels” parameter to reduce the query's search space. These labels are collected from the output of the NER phase, forcing the system to identify only a subset of relevant entities that are relevant to the type of entities mentioned.

The result of the query obtained passing “Zika” as input term is provided in listing 8.16.

Listing 8.16 Example of updated NED results from the candidate selection step

{

"id": 0, "mention": "Zika", "label": "Organism", "start": 19, "end": 22, "candidates": [ {

"snomed_id": "50471002", "name": "Zika virus" }, { "snomed_id": "3928002", "name": "Zika virus disease" }, { "snomed_id": "762725007", "name": "Congenital Zika virus infection" } ] }

The JSON reported in listing 8.16 represents the output of the candidate selection phase. More specifically, the “candidates” field includes a list of potential matches or candidates the system found for each mention. Each candidate represents a possible interpretation of the mention based on SNOMED. The following fields characterize each candidate:

snomed_id: the unique identifier for the concept in SNOMED. name: the name of the medical entity associated with the snomed_id.

In this case, the candidates are:

Zika virus (50471002). Zika virus disease (3928002). Congenital Zika virus infection (762725007).

These candidates represent various possible medical meanings of "Zika" within clinical terminology, setting the stage for further refinement in the disambiguation phase. While the full-text search mechanism offers a viable solution, it can be enhanced by incorporating a vector-based search to retrieve additional candidates that may not be identified through text-matching.

8.6 Candidate Disambiguation

The final phase of the NED process is Candidate Disambiguation (CD). In this phase, we apply a strategy that leverages contextual information provided by other medical entities co-occurring with the target entities in a sentence.

By cross-referencing these entities with the structured knowledge within the domain ontology, we can verify and refine the candidate selection to determine the most accurate match.

For example, consider a sentence mentioning both "Zika" and "microcephaly." The presence of "microcephaly" alongside "Zika" provides valuable context, as it suggests a specific association with "Congenital Zika Virus Infection," given that this infection is known to cause microcephaly. The disambiguation process would use this co-occurrence to prioritize "Congenital Zika Virus Infection" over other potential meanings of "Zika" (such as a general virus or unrelated term).

Figure 8.5 illustrates the input and output of the CD phase, showing how the system uses contextual information from the SNOMED ontology to refine and finalize entity disambiguation.

Figure 8.5 The third stage of NED is the Candidate Disambiguation. The key goal is to select the best match among all the possible candidates identified in the previous step. To reach this goal, the disambiguation phase exploits an advanced approach combining graph-based algorithms (shortest path detection) and LLMs.

To generate disambiguated entities for each mention identified in the input document, we perform three distinct steps:

1. Shortest Path Detection: The objective of this step is to identify all

possible graph paths between the candidate entities associated with different mentions within a sentence. By mapping these connections, we establish potential relationships that help clarify the intended meaning of each mention.

2. Path-to-Text Translation: To leverage the LLM’s strengths in

processing textual information, we translate each graph path that connects candidate entities into natural language sentences. This transformation enables the LLM to interpret relational information within a format it processes effectively. 3. Textual Path Summarization: To provide concise and valuable

contextual information for disambiguation, we summarize all textual information derived from the translated paths into a synthetic explanation. This summary captures the essence of the relationships and supports the LLM in making more accurate disambiguation decisions.

Figure 8.6 provides an overview of these three distinct steps in the disambiguation process.

Figure 8.6 NED Candidate Disambiguation is divided into three main phases: (1) Shortest Path Detection between all the candidates related to the different entity mentions in the sentence. (2) Path-to-text Translation to transform detected paths into natural language sentences. (3) Textual paths summarization to summarize all the natural language sentences into a unique and valuable piece of text useful for the disambiguation

Figure 8.6 illustrates that LLMs empower the "Path-to-Text Translation" and "Textual Path Summarization" steps, enabling it to interpret and condense relational information effectively. The "Shortest Path Detection," discussed in the following section, utilizes Neo4j's Graph Data Science (GDS) library to efficiently identify connections between candidates.

8.6.1 Shortest Path Detection

The goal of the “Shortest Path Detection” is to detect the shortest path between all the possible candidates associated with each medical entity

mention identified during the candidate selection phase. Listing 8.17 shows the query for performing this operation.

Listing 8.17 Python class of the extracting relevant path from the SNOMED ontology

class PathExtraction():

def __init__(self, model, store, candidates, named_entities): self.model = model self.store = store self.candidates = candidates self.named_entities = named_entities […]

def get_co_occs_query(self, s1_id, s2_id): query = f""" CALL gds.degree.stream('snomedGraph') YIELD nodeId, score WITH gds.util.asNode(nodeId).name AS name, score AS degree ORDER BY degree DESC LIMIT 350 WITH collect(name) as hub_nodes MATCH (s1), (s2) WHERE s1.id="{s1_id}" AND s2.id="{s2_id}" WITH s1, s2, allShortestPaths((s1)-[:SNOMED_RELATION*1..2]- UNWIND paths AS path WITH relationships(path) AS path_edges, nodes(path) as path_ WITH [n IN path_nodes | n.name] AS node_names, [r IN path_edges | r.type] AS rel_types, [n IN path_edges | startnode(n).name] AS rel_starts, hub_nodes WHERE not any(x IN node_names WHERE x IN hub_nodes) WITH [i in range(0, size(node_names)-1) | CASE WHEN i = size(node_names)-1 THEN "(" + node_names[size(node_names)-1] + ")" WHEN node_names[i] = rel_starts[i] THEN "(" + node_names[i] + ")" + '-[:' + rel_types[i] + ']- ELSE "(" + node_names[i] + ")" + '<-[:' + rel_types[i] + '] RETURN DISTINCT apoc.text.join(string_paths, '') AS `Extrac """.format(s1_id=s1_id, s2_id=s2_id, named_entities=named_ent return query […]

The critical steps of the query reported in listing 8.17 are the following:

Degree Calculation: The query first retrieves nodes with the highest "degree" (the number of relationships they have) from the graph using CALL gds.degree.stream. These nodes represent highly connected hub nodes, which will later be excluded to focus on more meaningful, less generic connections. Shortest Path Search: It finds all shortest paths between two specific entities s1 and s2 based on their IDs, limiting the path length to 1 or 2 hops (relationships). The relationships between these nodes are filtered so that "hub" nodes are excluded to avoid generic or overly relationships. Path Transformation: Once paths are identified, the method unwinds them and collects both the nodes and relationships involved in each path. It then formats these paths into readable strings that show the direction and types of relationships (e.g., (n1)-[:REL_TYPE]->(n2)). Excluding Hub Nodes: The method excludes paths involving hub nodes (those with high degrees) to focus on more relevant or specific connections between the two entities.

Listing 8.18 shows an example of detected paths.

Listing 8.18 Results of detected paths using the GDS library of Neo4j

[

{ "id": 1, "path": "(Congenital Zika virus infection)-[:OCCURRENCE]->(Co }, { "id": 2, "path": "(Congenital Zika virus infection)-[:OCCURRENCE]->(Co }, { "id": 3, "path": "(Congenital Zika virus infection)-[:OCCURRENCE]->(Co }, { "id": 4, "path": "(Congenital Zika virus infection)-[:OCCURRENCE]->(Co }, { "id": 5, "path": "(Congenital Zika virus infection)-[:OCCURRENCE]->(Co

}, { "id": 6, "path": "(Micrencephaly)-[:OCCURRENCE]->(Congenital)<-[:OCCUR }, { "id": 7, "path": "(Micrencephaly)-[:OCCURRENCE]->(Congenital)<-[:OCCUR }, { "id": 8, "path": "(Micrencephaly)-[:OCCURRENCE]->(Congenital)<-[:OCCUR }, { "id": 9, "path": "(Acrocephaly)-[:OCCURRENCE]->(Congenital)<-[:OCCURRE }, { "id": 10, "path": "(Acrocephaly)-[:IS_A]->(Craniosynostosis syndrome)-[ }, { "id": 11, "path": "(Acrocephaly)-[:PATHOLOGICAL_PROCESS_(ATTRIBUTE)]->( }, { "id": 12, "path": "(Acrocephaly)-[:OCCURRENCE]->(Congenital)<-[:OCCURREN }, { "id": 13, "path": "(Acrocephaly)-[:OCCURRENCE]->(Congenital)<-[:OCCURRENC } ]

The JSON reported in listing 8.18 represents the output of the shortest path algorithm using Neo4j's GDS library. Each entry contains an id and a path, where each path shows a relational link between biomedical entities based on their co-occurrence within ontology paths. Here are the details:

Congenital Zika Virus Infection Paths: Many paths begin with "Congenital Zika virus infection" linked to various congenital conditions, such as "Micrencephaly," "Acrocephaly," and "Multiple congenital malformations," through the relationship type [:OCCURRENCE]. This implies that "Congenital Zika virus infection"

is associated with these conditions, possibly as a cause or occurrence. Shared Congenital Condition: "Congenital" serves as a common node, linking multiple congenital conditions like "Micrencephaly," "Acrocephaly," and other malformations. This central node indicates that these conditions share a similar occurrence attribute. Alternative Relationships: Some paths use relationships like [:IS_A] and [:PATHOLOGICAL_PROCESS_(ATTRIBUTE)], showing hierarchical or attribute-based relationships. For instance, "Acrocephaly" is classified under "Craniosynostosis syndrome," which is linked to "Congenital malformation".

8.6.2 Path-to-text Translation

The “Path-to-text Translation” aims to translate graph paths into sentences. This step allows LLMs to process complex relational data in a format they are optimized to understand natural language. This transformation makes it easier for the model to interpret connections between entities, providing context that aids in distinguishing between similar terms. Listing 8.19 shows a simplified version of the prompt for translating paths into sentences.

Listing 8.19 Simplified version of the prompt for translating paths into sentences

system = “You are an assistant capable of translating a Neo4j gra

input = {

"path": "(Hypertension)-[:RISK_FACTOR_FOR]->(Cardiovascular }

assistant = {

"sentence": "Hypertension is a risk factor for cardiovascul }

Here are the details of the prompt shown in listing 8.19:

System Instruction: The system translates Neo4j graph paths into clear, human-readable sentences. These paths represent relationships between entities, and the goal is to provide contextual sentences that help disambiguate entities in the biomedical domain. Input Graph Paths: In this case, the input consists of a graph path from

a Neo4j database, which might represent complex relationships such as interactions between diseases, drugs, genes, or other biomedical entities. Assistant Output: The system responds with a valid JSON structure showing the generated sentente.

As shown in listing 8.20, each graph path is translated into a single, clear sentence.

Listing 8.20 Results of the paths translation into natural language

[{

"sentence": "A Congenital Zika virus infection occurrence is as }, { "sentence": "A Congenital Zika virus infection occurrence is as }, […],

{ "sentence": "Micrencephaly occurs in Congenital and Multiple co }, { "sentence": "Micrencephaly occurs in Congenital and is also an }, […], { "sentence": "Acrocephaly occurs in Congenital and Other congeni }]

The JSON reported in listing 8.20 represents the output of the path-to-text translation phase within the candidate disambiguation process. Each entry in the JSON is a sentence that translates a graph path into natural language, making it accessible for an LLM to interpret and use for disambiguation.

8.6.3 Textual Paths Summarization

Before executing the final disambiguation of the selected candidates, we need to summarize the sentences representing the translated graph paths. This concise information reduces the “cognitive load” for the model (low number of tokens), making it easier to interpret entity relationships and select the most accurate candidate without being overwhelmed by excessive detail. Listing 8.21 provides a simplified version of the prompt for text

summarization.

Listing 8.21 Prompt for summarizing textual paths

system = “You are an assistant that can summarize multiple senten

input = {

"sentences": [ { "id": 1, "sentence": "Hypertension is a risk factor for cardiovascul }, { "id": 2, "sentence": "Diabetes mellitus is a complication that arise }, { "id": 3, "sentence": "Asthma is associated with respiratory disorder }, { "id": 4, "sentence": "Osteoporosis leads to bone weakness. Bone frac } ]

assistant = {

"context": "Hypertension is a risk factor for cardiovascular di }

Here are the details of the prompt:

System Instruction: The system is instructed to summarize sentences derived from ontology paths. The system must ensure that all identified entities within the sentences are retained in the summaries. The output should be formatted as a valid JSON object, with each summary provided as a string under the key "context." Input Sentences: In this example, the input consists of multiple sentences, each containing relationships between medical conditions and their effects or associations. These sentences describe complex connections, such as risk factors (e.g., hypertension for cardiovascular disease), complications (e.g., diabetes mellitus leading to diabetic retinopathy), and common links between conditions (e.g., asthma and

allergic rhinitis both associated with respiratory disorders). Assistant Output: The system responds with a single summarized sentence for each group of related entities, presented in a valid JSON format. Each summary is crafted to:

Retain the core entities mentioned in the input sentences (e.g., "Hypertension," "Myocardial infarction," "Diabetes mellitus"). Express the relationships between those entities clearly, providing a concise but comprehensive view of how the entities are connected (e.g., "Hypertension is a risk factor for cardiovascular disease, which increases the likelihood of myocardial infarction").

The output structure is a JSON that provides a string that summarizes the core relationships in the input sentences, preserving the critical entities and relationships. The result of the summarization phase is reported in listing 8.22.

Listing 8.22 Results of the summarization phase

{"context": "A Congenital Zika virus infection occurrence is asso

The result reported in listing 8.22 provides the LLM with a distilled version of complex relational information, allowing it to focus on the most relevant context for disambiguation.

8.6.4 Disambiguation

In the final stage, we combine all the ingredients for the disambiguation, including the selected candidates and the textual details provided by the summarization phase. Listing 8.23 shows the prompt for the disambiguation.

Listing 8.23 Prompt for final disambiguation

system = “You are an assistant specialized in entity disambiguati 1. Original Sentence: The sentence that contains ambiguous entiti 2. Candidate Entities: A list of potential entities extracted fro 3. Contextual Sentences: A collection of related or surrounding s Your objective is to use the entities mentioned in the contextual

input = {

"sentence": "Asthma and allergic rhinitis are commonly addresse "candidates": [ { "id": 1, "candidates": [ { "snomed_id": "233681001", "name": "Extrinsic asthma with asthma attack" }, { "snomed_id": "195967001", "name": "Asthma" }, { "snomed_id": "266361008", "name": "Intrinsic asthma" }, { "snomed_id": "266364000", "name": "Asthma attack" }, { "snomed_id": "270442000", "name": "Asthma monitored" }, { "snomed_id": "170642006", "name": "Asthma severity" }, { "snomed_id": "170643001", "name": "Occasional asthma" }, { "snomed_id": "170644007", "name": "Mild asthma" }, { "snomed_id": "170645008", "name": "Moderate asthma" } ] } ], "context":"Asthma is associated with respiratory disorders. Aller

}

assistant = {

"entities": [ { "id": 1, "disambiguation": { "snomed_id": "195967001", "name": "Asthma" } }, { "id": 2, "disambiguation": { "snomed_id": "61582004", "name": "Allergic rhinitis" } } ] }

Here are the details of the prompt:

System Instruction: The system is directed to act as an assistant specialized in entity disambiguation. Its task is to identify and accurately disambiguate ambiguous entities. The assistant must analyze each ambiguous entity in the original sentence and select the candidate that best aligns with the contextual meaning provided by the summarized contextual information. The assistant should prioritize entities that align with this contextual details. Input Structure:

Original Sentence: The primary sentence contains ambiguous entities that require disambiguation. Candidate Entities: A list of potential SNOMED entities for each mention, each with multiple possible interpretations or labels. Contextual Sentences: Additional sentences that provide context to help clarify the meaning of each ambiguous entity in the original sentence. Assistant Output:

id: A unique identifier for the entity mention. disambiguation: An object with the selected SNOMED entity, including snomed_id and name, that best matches the contextual

information.

The result of the LLM model leveraging this prompt is available in listing 8.24:

Listing 8.24 Results of the disambiguation process

{

"entities": [ { "id": 0, "disambiguation": { "snomed_id": "762725007", "name": "Congenital Zika virus infection" } }, { "id": 1, "disambiguation": { "snomed_id": "204030002", "name": "Micrencephaly" } }, { "id": 2, "disambiguation": { "snomed_id": "116022009", "name": "Multiple congenital malformations" } } ] }

The result shown in listing 8.24 is achieved by passing as input the following sentence: “Severe outcomes of Zika are due to its capacity to cross the placental barrier during pregnancy, causing microcephaly and congenital malformations”. Therefore, each entity is matched to the most relevant concept in the SNOMED ontology, allowing for precise identification and classification based on contextual information.

8.7 Conclusions

The chapter provided an in-depth exploration of a Named Entity Disambiguation (NED) system that leverages open Large Language Models (LLMs) and domain-specific ontologies.

By integrating domain ontologies like SNOMED with open and general- purpose LLMs such as Llama 3.1 8B, we addressed some of the limitations of traditional NLP tools in the biomedical domain, such as ScispaCy. We showcased a more flexible approach that can adapt across different application domains. Using Neo4j's Graph Data Science (GDS) library for path detection and full-text search, combined with the disambiguation power of LLMs, enabled a robust system for identifying and accurately disambiguating entities in complex texts.

The chapter also emphasized the importance of using contextual information in the disambiguation phase. Through techniques like Path-to-text translation and Textual path summarization, we improved the LLM’s ability to process relational data in a natural language format, enhancing its capacity to distinguish between similar entities.

Overall, this chapter shows a promising approach to developing a comprehensive end-to-end system for entity disambiguation, enabling more precise results in specialized domains. This framework lays the groundwork for future applications of LLMs in domain-specific NED tasks, opening the door to more accurate and efficient text-processing solutions in diverse fields.

8.8 Summary

Named Entity Disambiguation (NED) is essential for accurately identifying and distinguishing entities in complex domains, particularly in biomedical text. Traditional Natural Language Processing (NLP) tools such as ScispaCy have some limitations:

they can not be used in diverse domains, they can not leverage the relationships between entities, and their reference knowledge can not be extended and updated. The combination of general-purpose Large Language Models (LLMs) and domain ontologies allows us to address these issues: LLMs can be

driven by the continuously updated knowledge incorporated by the ontology and leverage its relational structure. To reach this goal, we can deploy a flexible end-to-end process for NED, including multiple phases involving LLMs and domain ontologies, such as Named Entity Recognition (NER), NED Candidate Selection, and NED Candidate Disambiguation. To fully leverage the capabilities of LLMs combined with the graph dimension of domain ontologies, the disambiguation phase is divided into three different stages:

1. Shortest-path detection. 2. Path-to-text translation. 3. Textual path summarization. Future NED applications can leverage this framework and adapt to different domains, which are characterized by rich ontologies describing the relational nature of their specific entities.

## Part_IV:_Machine_learning_on_knowledge_graphs

The application of machine learning to knowledge graphs represents a crucial advancement in our ability to extract meaningful insights from complex, interconnected data structures. This part explores how representation learning and graph neural networks can transform the static knowledge contained in graphs into dynamic, learnable features that power sophisticated downstream tasks. While previous parts focused on constructing and enriching knowledge graphs, these chapters demonstrate how to leverage these structures for predictive modeling and pattern recognition.

The synergy between knowledge graphs and machine learning creates powerful capabilities that enhance our broader goal of building hybrid intelligent systems. This combination enables:

Neural network-based representations that capture the complexity of graph structures and their contained entities Parallel processing approaches that mirror how LLMs handle relationships between words and concepts Flexible feature representations that support various downstream tasks from classification to link prediction Automated knowledge extraction through the interpretation of learned embeddings Verifiable results that demonstrate how structured information can be effectively encoded in vector spaces

Chapter 9 introduces the fundamental concepts and motivations for applying machine learning to knowledge graphs, establishing why graph-based approaches better reflect real-world dependencies and how they can be applied to a core set of computational tasks. It sets the theoretical foundation for understanding how knowledge graphs can be transformed into learnable representations.

Chapter 10 illustrates the critical role of feature engineering in graph-based machine learning, exploring both manual and semi-automated approaches. Through practical examples in fraud detection and drug repurposing, it demonstrates how different graph metrics and structural patterns can be captured and utilized effectively.

Chapter 11 advances into the realm of graph neural networks, showing how these specialized architectures can automatically learn optimal representations from graph structures. It bridges the gap between traditional feature engineering and modern deep learning approaches.

Chapter 12 demonstrates these concepts in action through two compelling real-world applications: anti-money laundering through node classification and movie recommendations through link prediction. These implementations showcase how graph neural networks can effectively tackle complex business challenges while maintaining interpretability.

Throughout this part, the connection to Large Language Models remains evident, as many of the representation learning techniques mirror approaches used in modern language models. This parallel highlights the broader theme of our book: the powerful combination of structured knowledge representation with advanced machine learning techniques.

## 9_Machine_learning_on_knowledge_graphs:_a_primer_approach

This chapter covers

What is Machine Learning on a Knowledge Graph, and why is it critical? Explore the most common machine learning tasks performed on graphs and the methods to execute them. Understanding the role of node and relationship representations.

Building knowledge graphs is a crucial step in developing intelligent systems. It allows us to acquire holistic knowledge – often from multiple and different data sources – and represent it properly so that it can be used for exploration and navigation purposes as well as for more advanced analytics. Along the second and the third part of the book, after the creation of the graph, even leveraging LLMs, we have seen how to query it and extract relevant information, how to navigate through the different nodes and relationship types, and even how to extract relevant statistical information to validate the import process and to evaluate the “quality” of the knowledge stored in it. All these have been important and critical steps in the process of building sophisticated applications as intelligent advisory systems (IAS), which is the main purpose of this book. With that goal in mind, having a knowledge graph, performing effective exploration, and leveraging query functionalities is not enough and, even worse, will not exploit the full power of the knowledge graphs we just built. Each graph contains way more implicit information than what you can discover by looking at it just with your eyes. Moreover, the dimension of these graphs is not trivial, which means that “a human brain” is not capable of scanning a significant part of them and extracting all the relevant information necessary to accomplish complex tasks.

In an IAS, the critical aspect of “advising” involves providing insights that the users cannot extract on their own. How can a researcher efficiently

navigate through a vast knowledge graph containing diseases, proteins, genes, and compounds to identify potential drug repurposing opportunities? How can a clinician combine patients' symptoms and DNA sequences with existing literature, gene-protein-compound relationships, clinical trials, and standard protocols to develop highly personalized treatment plans? Similarly, in law enforcement, how can investigators analyze complex networks of communications, financial transactions, and location data to reveal life patterns - such as a suspect's regular movements between safe houses, frequent contacts during specific time windows, or recurring financial patterns that might indicate coordinated activities?

The list of all these scenarios is very long. The support the knowledge graphs can provide in various and complex domains is endless. In most cases – if not all – these suggestions require leveraging machine learning algorithms that use the knowledge in the graphs as input. This chapter, along with the entire fourth section of the book, delves into the realm of machine learning on knowledge graphs. The reasons why we decided to focus on machine learning on (knowledge) graphs are that first, we would like to extend the previous book with new algorithms and theories, trying not to overlap with it, secondly, in the last few years, a lot of research and work has been done in this field. Since the publication of Graph-Powered Machine Learning [1] different algorithms have been improved – in particular, Graph Neural Networks (GNNs)– new ones have been introduced and tested on graphs – like Large Language Models –, and way more libraries are now available. These libraries are stable and performant enough to be considered in a production environment. For these reasons, we can consider machine learning on graphs a hot topic worth an entire book on.

Furthermore, when applied to knowledge graphs, which possess a rich array of information, machine learning delivers exceptional value. It can strengthen a multitude of applications and facilitate numerous use cases. Within this context, it evolves into a profoundly meaningful tool. Let's commence our in- depth exploration from this point, addressing the reasons why you should consume this third part with eager anticipation.

9.1 Machine Learning on graphs: why?

Before embarking on this exciting yet tough journey, we want to describe the primary reasons that make this topic particularly relevant for practitioners. We hope to instill in our readers a clear understanding of why executing machine learning on (knowledge) graphs can be valuable, with and without LLMS, occasionally proving to be the optimal, or perhaps even the only feasible, choice.

The key reasons can be summarized as follows – some of them have been already demonstrated during the book others are introduced here for the first time:

Data representation: Data used or produced by real-world applications has very diverse forms, from matrix and tensor to sequence and time series [2]. Representing data as graphs provides a universal representation of it. Data from many systems across various areas can be explicitly denoted as graphs such as social networks, transportation networks, protein-protein interaction networks, brain networks, and knowledge graphs. Furthermore, numerous other types of data can be transformed into graphs, due to such model flexibility. Problem modeling: A huge number of problems can be addressed as a small set of computational tasks on graphs. For example, detecting anomalous nodes (e.g., spammers or terrorists), identifying genes relevant to diseases, and suggesting medications for patients can be summarized as problems of node classification, while recommendations, polypharmacy side effect prediction, drug-target interaction identification, and knowledge graph completion are, essentially, problems of relationship prediction. Data item dependency: Traditional machine learning algorithms assume that data items are independent and identically distributed. In practice, in many real-world use cases data items – like calls among people, transactions, product orders, proteins, etc. – are not only intrinsically connected but ignoring such relationships can generate incomplete or wrong results. Hence, storing data as graphs, which naturally stores relationships, and applying computational tasks on graphs could provide better results in these cases [3].

In addition to those points, for our book, machine learning techniques on

graphs like GNNs together with LLMs create powerful Intelligent Advisory Systems by combining the GNN's ability to detect complex structural patterns with the LLM's natural language understanding and generation capabilities. In drug discovery, GNNs can analyze molecular structures and protein-drug interactions across the knowledge graph, while LLMs interpret research papers and clinical reports to provide evidence-based recommendations for drug repurposing. For personalized medicine, GNNs process patient similarity networks and biological pathway graphs, while LLMs synthesize medical literature and clinical guidelines to explain treatment rationales in clear medical terminology. In law enforcement, GNNs excel at identifying suspicious patterns in communication networks and financial transaction graphs, while LLMs can transform these technical insights into actionable intelligence reports - for example, explaining how a suspect's regular meetings at specific locations correlate with unusual financial transfers and encrypted communications. This GNN-LLM integration enables the system to not only identify complex patterns but also explain them in context- appropriate language, making the insights more accessible and actionable for end users.

We should also consider that machine learning is inherently a problem-driven discipline, so the final goal we would like to achieve and the business problem we would like to solve, ultimately, represent the real reasons why we should use the best tool possible. As we will see on these pages, for many of the scenarios considered machine learning on graphs will represent the best arrow in your bow.

9.2 Machine learning on graphs: What?

Classical machine learning algorithms can be categorized in different ways. The most common one differentiates them according to the type of data and the task they solve. This categorization split them mainly into supervised and unsupervised.

In the supervised algorithm the data are partially labeled, i.e., the output is known for certain data items, and the goal is to predict the labels for those for which we don’t know yet. A typical example of this learning process is the spam filter. The learner requires labels, such as “spam”

and “not spam” (the significant information), in the training dataset for each data item (emails). It learns from these labels how to classify an email. In unsupervised algorithms, the data is fully unlabeled, and the goal is to extract insights and patterns from data, like, for example, clusters given a set of data points or communities given a graph.

Machine learning on graphs is no different, but the usual categories of supervised and unsupervised are not necessarily the most informative or useful when it comes to graphs [4] even though they remain valid: for example, node classification can be considered a supervised task (even though as we will see in few paragraphs this is not completely true) while community detection is unsupervised.

Here we will go through the most important and well-studied machine- learning tasks on graph data. Supervised problems are popular with graph data, but machine learning problems on graphs often blur the boundaries between the traditional machine learning categories. The different tasks on graphs can be mainly divided into two categories:

node-focused tasks where the entire data set is represented as one graph with nodes and relationships as the data points. graph-focused tasks, where data consists of a set of graphs and each data point is an entire graph.

Figure 9.1 describes the concepts above visually.

Figure 9.1 Node-focused and graph-focused tasks are represented in terms of expected input and output during training and prediction.

The forthcoming subsections outline the primary tasks within each category. This list omits some tasks from each group. However, we've chosen to focus on the tasks in this book that, in our estimation, are the most beneficial and common for the reader.

9.2.1 Node classification

Suppose you manage a large social network sprawling with millions of users, like Facebook, Twitter, and LinkedIn, to mention a few well-known platforms. Among these users, a substantial number are bots. Detecting these bots can be crucial for a multitude of reasons: they could potentially violate the network's terms of service, spread or perpetuate fake news, or simply be irrelevant targets for marketing campaigns. Manually identifying them is either infeasible or extremely time-consuming. Ideally, you would require a model that can differentiate users as either bots or legitimate users, given a reasonably small set of manually labeled examples. Figure 9.2 delineates the principal components and phases of node classification.

Figure 9.2 Node classification typical flow. As with many supervised machine learning tasks, it has two phases. The first is the training and the second is the prediction that, in this case, translates into classifying unclassified nodes.

This is a classic example of a node classification task: more formally, the goal is to predict for each unlabeled node u in the graph V the label yu, when we are only given the known labels on a training set of nodes Vtrain that is a small subset of V.

Node classification is perhaps the most popular machine learning task on graph data, especially in recent years. Other examples of node classification beyond social networks include classifying the function of proteins in the interactome [4] and classifying the topic of documents based on hyperlinks or citation graphs [5]. Hence, node classification is a powerful tool during the implementation of an intelligent advisor system. It helps with complex tasks of identification and decision-making.

At first glance, node classification seems to be a straightforward variant of standard supervised classification, yet distinct differences exist. Notably, the nodes in a graph are not independent and identically distributed (i.i.d.). Typically, in constructing supervised machine learning models, we assume that each data point is statistically independent of all other data points. Should this not be the case, we might need to articulate the dependencies among all our input points. Similarly, we assume that the data points are identically distributed to ensure our model can effectively generalize to unseen data points. However, node classification entirely shatters this i.i.d. assumption. Instead of modeling a set of i.i.d. data points, we're modeling an interconnected network of nodes.

Node classification in graph-based machine learning fundamentally differs from classical ML approaches by leveraging the interconnected nature of data rather than assuming independent and identically distributed (i.i.d.) samples. In social networks, homophily illustrates this interconnectedness - nodes influence each other through their relationships, exhibiting shared interests, attributes, and behaviors with their neighbors [6]. This violation of the i.i.d. assumption means that effective models must consider both node features and their network relationships when making predictions [7].

Different graph types exhibit various relationship patterns that challenge the i.i.d. assumption. In protein interaction networks, structural equivalence

becomes crucial - nodes with similar neighborhood structures tend to share functional properties [8]. Heterophily presents another pattern where nodes preferentially connect to those with different characteristics, further demonstrating how real-world data often violates i.i.d. assumptions.

These principles underscore why treating nodes as independent data points fails to capture the rich relational information encoded in graphs. Successful node classification requires modeling both node attributes and their complex interdependencies - a fundamental departure from classical ML's i.i.d. assumption.

Coming back to the distinction between supervised or unsupervised in classical machine learning algorithms, is node classification supervised or unsupervised? Many researchers agree that it is semi-supervised [9] because when we are training node classification models, we usually have access to the full graph, including all the unlabeled (e.g., test) nodes. The only thing we are missing is the labels of the test nodes. However, we can still use information about the test nodes (e.g., knowledge of their neighborhood in the graph) to improve our model during training. This is different from the usual supervised setting, in which unlabeled data points are completely unobserved during training. The general term used for models that combine labeled and unlabeled data during training is semi-supervised learning, so, understandably, this term is often used for node classification tasks. It is important to note, however, that standard formulations of semi-supervised learning still require the i.i.d. assumption, which does not hold for node classification. We still struggle with this definition. This shows why machine learning tasks on graphs do not easily fit classical categories!

Node classification can be expanded to assign multiple labels to a single node. For instance, Flickr[1], an image-hosting platform, utilizes graphs and multilabel node classification to address users' interests. In addition to hosting photos, Flickr also serves as an online social community where users can follow each other, thus forming a network graph through these user connections. Moreover, Flickr users can subscribe to interest groups, such as "Black and White," "The Fog and The Rain," and "Dog World," among others. These memberships signify user interests and can subsequently serve as user labels. Since users can subscribe to numerous groups, each user may

be associated with multiple labels.

A multilabel node classification problem on graphs can help predict potential groups that users might be interested in but haven't yet subscribed to. Tang and Liu (2009) [10] provide datasets related to such Flickr user behaviors.

9.2.2 Link Prediction (aka Relationship Prediction)

Link prediction is a fundamental task in graph-based machine learning that aims to identify potential future connections between nodes. Consider a comprehensive database of research papers from domains such as healthcare (like PubMed[2] or MedRxiv[3]), COVID-19 research (CORD-19[4]), broad scientific research (like Web Of Science[5]), or computer science (like DBLP[6]). From these sources, we can construct a co-authorship graph where authors represent nodes, and edges represent their collaboration on at least one paper. The link prediction task then involves forecasting likely future collaborations between authors who haven't yet worked together.

This predictive approach extends naturally to other critical domains. In law enforcement and security applications, when analyzing graphs of known associations between persons of interest (such as fraudsters, criminal networks, or terrorist cells), link prediction algorithms can identify potential future connections or uncover existing but hidden relationships between subjects. Figure 9.3 illustrates the two primary phases of a typical link prediction task.

Figure 9.3 Link prediction typical flow. Link prediction has two phases. During the first one, the training, a graph is taken and the model is trained to recognize whether a link exists or not. During the prediction phase, the model provides an output for each pair of nodes representing the target relationship to predict. For each, it will provide an existence probability.

As clear from the scenario above, in many real-world applications, graphs are not complete because of missing edges. The incompleteness happens mostly due to two factors:

Some of the connections exist but they are not observed or recorded, which leads to missing edges in the observed graphs. In certain cases, they are kept hidden on purpose by the key actors in the network. Many graphs are naturally evolving. On Facebook, users can always become friends with other users they weren’t before. In the academic collaboration graphs, a given author can always build new collaboration relations with other authors writing a new article.

Inferring or predicting these missing edges can benefit many Intelligent Advisory Systems such as friend recommendations [11, 12], product recommendation, knowledge graph completion [13], predicting drug side- effects [14], inferring new facts in a relational database [15], protein-protein interaction discovery [16], and criminal intelligence analysis [17]. The list is endless.

This task is known by various names — link prediction, graph completion, and relational inference — to name a few, depending on the specific application domain. We acknowledge the challenge of carving out a concise definition that could effectively differentiate between them. For the remainder of this book, these terms will be utilized interchangeably. However, it's worth noting that link prediction typically pertains to the prediction of the mere existence of a connection between two nodes, regardless of the relationship type (which is consistently the same and predetermined). Conversely, relationship prediction will be invoked when we endeavor to pinpoint not just the existence, but also the type of relationships.

Like node classification, link prediction is typically approached as a semi- supervised learning task. While specific links between node pairs may be missing, we can leverage both existing examples of connections and rich node-level information to predict potential relationships. This semi- supervised nature allows us to utilize known patterns in the graph structure along with node attributes to infer likely missing or future connections between nodes.

9.2.3 Clustering and community detection

Imagine, again, that you obtain a comprehensive catalog of research papers from any of the sources discussed, and from this, a collaborative graph is generated, connecting two researchers who have co-authored a paper. Upon inspection of this network, it's unlikely we'd observe a dense “hairball” where collaborations are equally probable among everyone. Instead, the graph would likely be partitioned into distinct clusters of nodes — grouped by such elements as research area, institution, or geographic factors. The propensity for two researchers from the same university to collaborate frequently is higher than that of colleagues situated remotely. Similarly, two researchers within the same specific field are more likely to reference other researchers within that sphere rather than in an entirely unrelated domain.

Like many real-world networks, research collaboration graphs exhibit distinct community structures. Within these communities, nodes are more likely to form connections with other members of their community than with nodes outside it. This natural clustering behavior forms the theoretical foundation for community detection algorithms, which aim to identify these inherent groupings in the network structure, as illustrated in Figure 9.4.

Figure 9.4 Community Detection flow. The input is a graph, and the output is a mapping between nodes and groups. In the image, the groups are represented as colors and different circles.

Community detection aims to uncover latent group structures within a graph using only the network's topology and relationships. This fundamental task has diverse real-world applications, from identifying functional modules in genetic interaction networks [18] to detecting fraudulent user groups in financial transaction networks [19], to mention just a few examples.

Throughout this book, we've repeatedly encountered graph clustering, highlighting both its ubiquity and versatility. One of its most powerful applications is in graph description and summarization. By identifying densely connected regions - whether they represent researchers from the same geographic area or proteins sharing similar functions - clustering provides a higher-level view of the network's organization. This capability is particularly

valuable when dealing with large-scale graphs that defy direct visualization or comprehensive manual analysis, effectively providing a structural summary of complex network relationships.

While the terms "community detection" and "graph clustering" are often used interchangeably in network analysis, they differ fundamentally from traditional clustering algorithms like K-means[7] or DBScan[8]. The key distinction lies in the data structure: traditional clustering algorithms work with independent data points in vector space, while graph clustering specifically handles interconnected data where relationships between nodes are crucial to determining group membership. We'll use these terms interchangeably throughout our discussion, as they refer to the same task when applied to graphs.

While node classification and relationship prediction are typically semi- supervised tasks that leverage existing labeled data, graph clustering is generally unsupervised, requiring no pre-labeled information to identify community structures. However, some clustering approaches, such as Label Propagation [20], can incorporate existing labels to guide community assignment, bridging the gap between supervised and unsupervised learning in graph analysis.

The previous three algorithms belong to category of the node-focused tasks, the next one instead belongs to the graph-focused tasks.

9.2.4 Graph classification

Consider the task of predicting both the toxicity and solubility of chemical molecules. These properties emerge not just from the individual atoms present but critically from how these atoms are connected to form the molecule. We can naturally represent each molecule as a graph, where atoms serve as nodes and chemical bonds as edges [21]. This graph representation allows us to apply graph classification techniques to predict multiple molecular properties simultaneously. As illustrated in Figure 9.5, graph classification can systematically analyze both the atomic composition and structural patterns to categorize previously unlabeled molecules based on their properties like solubility and toxicity.

Figure 9.5 Graph classification flow. Like many supervised tasks, it has two phases. During the training, it takes different graphs with the related classes and trains a model to recognize the class of each. During the prediction, the model takes a graph and predicts its class.

While node classification focuses on predicting labels for individual nodes within a single graph, graph classification deals with datasets containing multiple independent graphs, each representing a complete sample (such as a molecule). In graph classification, each graph serves as an independent and identically distributed (i.i.d.) data point with its label - for instance, whether a molecule is toxic or not. This marks a key difference from node classification, where predictions rely on the interconnected nature of nodes within a single graph.

The goal of graph classification is to learn a mapping from entire graphs to their associated labels using a training set of labeled graphs. Similarly, graph clustering at the graph level extends traditional unsupervised clustering to categorize entire graphs rather than nodes. The primary challenge in these graph-level tasks lies in developing features that effectively capture both the internal structure of each graph and the properties of its components.

Graph clustering has many real-world applications as Intelligent Advisor Systems. Apart from the molecule example, the same approach can be used for similar tasks. For example, classifying protein structures into enzymes or non-enzymes. Enzymes are a type of protein that catalyzes biochemical reactions. Proteins can be represented as graphs, where amino acids are nodes, and edges between two nodes can be created if they are less than a certain distance. After the training, given a protein, the graph classification algorithm can predict if it is an enzyme or not. Another completely unrelated, example is the malware classifier. The task in this case is to build a classification model to detect if a computer program is malicious by analyzing a graph-based representation of its syntax and data flow [22].

9.3 Machine learning on graphs: How?

At this point, we understand why machine learning on graphs has emerged as a distinct branch of research and the complexities these algorithms can address. It is now appropriate to explore the various implementation options we have. Generally, solutions can be developed primarily in two directions, as summarized in Figure 9.6.

Figure 9.6 A graph-based classification approach (collective) versus traditional algorithms for classification.

There are two fundamental approaches to handling graph-based tasks like

node classification, as illustrated in Figure 9.6. The first approach employs algorithms specifically designed for graphs, such as collective classification [23]. These specialized algorithms directly process the graph structure, simultaneously considering both node features and neighborhood relationships to make predictions.

The second approach transforms the graph problem into a traditional machine learning task by first converting graph structures into feature vectors. This feature engineering step allows us to leverage the entire ecosystem of existing ML algorithms, including modern deep learning techniques. The main challenge then shifts to defining appropriate features that capture the essential characteristics of nodes, edges, and, in the case of graph classification, the entire graph structure.

In this part of the book, we focus specifically on feature engineering for graphs. We begin by exploring manual feature extraction methods, demonstrating their thoroughness but also their tedious nature. We then progress to semi-automated approaches before diving into Graph Neural Networks (GNNs) as a powerful solution for automated feature learning. GNNs excel at capturing both structural patterns and node properties, making them particularly valuable for incomplete knowledge graphs. We'll examine how GNNs create meaningful vector embeddings that encode graph knowledge in ways that directly benefit downstream machine learning tasks such as classification (for node classification for example).

Using the ideas just presented in the node classification let’s try to abstract them and see how they can be applied to the broader range of algorithms.

9.3.1 Node Classification and Link Prediction

Figure 9.7 describes the high-level training process for node classification and link prediction It shows the key elements in more detail.

Figure 9.7 Training flow for node classification and link prediction. A critical step is the “featurization” of nodes and relationships. Once these steps have been done, the vectors can be passed to a classic algorithm. Both node classification and link prediction can be seen as classification tasks.

As a result of the training process, a prediction model has been created. This represents the output of this phase and the input of the next phase. Figure 9.8

shows the steps for the prediction process that takes the existing nodes and relationships and predicts classes and missing links.

Figure 9.8 Prediction flow for node classification and link prediction. The featurization process should be the same as the training. The classifier model built previously is used in this phase for making the predictions.

It's important to note that the featurization process used during training must be the same as the one used for making predictions, otherwise, the prediction

phase will not function correctly.

Let’s put these principles into practice with a quick and simple example. Suppose that you would like to classify nodes in a network. For learning purposes, we will use a small graph: the famous Zachary Karate Club [24]. The graph documents the relationships among 34 members of a karate club, tracking interactions between pairs of members outside the club. A dispute between the administrator, “John A”, and the instructor, “Mr. Hi” (pseudonyms), caused the club to divide into two factions.

After the club's split, each member (represented as a node) became affiliated with either the instructor's new club (Mr. Hi) or remained with the administrator's original club (John A). This real-world outcome provides us with ground truth labels - each node is labeled according to which club the member ultimately joined. Our node classification task aims to predict these club affiliations using only the friendship network structure that existed before the split, demonstrating how network patterns can predict social behavior. This makes this network perfect for our example. The purpose is to explain, using a simple code, the entire flow described in Figures 9.7 and 9.8.

Let’s inspect first the network and then analyze it. For this example, we are not going to store anything in Neo4j. The network is super small, and it is available out-of-the-box, in many network analysis tools, also in networkx[9] that is the tool we will use for the analysis we need. Listing 9.1 shows how to import the necessary packages and load the Karate Club graph.

Listing 9.1 Code for creating a Karate Club network and draw it

import networkx as nx import matplotlib.pyplot as plt

G = nx.karate_club_graph() #A draw_and_save_graph_picture(G)

def draw_and_save_graph_picture(G): #B

set_club_colors(G) layout_position = nx.spring_layout(G, k=8 / math.sqrt(G.order colors = [n[1]['color'] for n in G.nodes(data=True)] nx.draw_networkx(G, pos=layout_position, node_color=colors)

plt.axis('off') plt.savefig("Karate_Graph.svg", format="SVG", dpi=1000) plt.savefig("Karate_Graph.png", format="PNG", dpi=1000) plt.show()

def set_club_colors(G): #C

for node in G.nodes(data=True): # Mr. Hi = 'purple', John A = 'blue' color = '#00fff9' if node[1]['club'] == 'Mr. Hi': color = '#e6e6fa' node[1]['color'] = color

The resulting network is visualized in Figure 9.9, where nodes are colored in two different shades representing the club each member ultimately joined after the split.

Figure 9.9 Karate Club graph as produced by the code in listing 9.1, the colors of the nodes represent the club the member ultimately joined.

As described in Figures 9.7 and 9.8, the first step is creating a "vector representation" of each node that will serve as input for classification algorithms during both training and prediction. While chapters 10 and 11 will explore sophisticated embedding techniques in detail, let's start with a simple but illustrative approach: representing each node by its degree (the number of connections it has to other nodes). This basic feature captures a node's connectivity in the network. In Listing 9.2 we demonstrate how to compute the degree of each node in the graph, providing a basic but informative feature for our classification task.

Listing 9.2 A simple representation technique that uses the degree.

def compute_degree_embeddings(G):

embeddings = np.array(list(dict(G.degree()).values())) #A embeddings = [[i] for i in embeddings] #B return embeddings

While this technique is foundational, the degree of a node may not offer substantial help in classifying nodes for our goal. As an exercise, we recommend also using two other metrics: the "degree of Mr. Hi's neighbors" and the "degree of John A's neighbors". These metrics could provide more insight for the algorithm to discern the group to which a node belongs. Feel free to try it yourself; there's a solution available later in this chapter. Chapter 10 introduces numerous techniques for the manual computation of vector representations for both nodes and relationships.

Before the advent of Graph Neural Networks, presented in Chapter 11, Node2Vec [25] was a prominent technique for autonomous representation learning that computed node embeddings based purely on network structure. Listing 9.3 shows how to generate these structure-aware node embeddings using the Node2Vec algorithm.

Listing 9.3 Using Node2Vec as embedding technique

def compute_complex_embeddings(G):

node2vec = Node2Vec( G, dimensions=64,

walk_length=30, num_walks=200, workers=4, seed=0) #A

model = node2vec.fit( window=10, min_count=1, batch_words=4, seed=0) #B

embeddings = [model.wv.get_vector(i) for i in G.nodes] return embeddings

The code in Listing 9.3 uses Node2Vec to create vector representations (embeddings) of nodes in a graph. The process occurs in three main steps:

1. It initializes Node2Vec by specifying that a 64-dimensional vector

should represent each node. The algorithm then performs 200 random walks through the graph, each visiting 30 nodes. For efficiency, it uses 4 parallel processes for efficiency, and a random seed ensures reproducible results. 2. The model is then trained using parameters inspired by Word2Vec: it

considers 10 nodes before and after in each walk sequence, includes all nodes in the training (even those visited once), and processes words in small batches. 3. Finally, it extracts the learned embeddings for each node in the graph,

returning a list of vector representations where each vector captures the node's structural role in the network. These embeddings can be used as input features for downstream machine learning tasks like node classification or link prediction.

Chapter 11 presents additional methods for automatic node embedding. At this point, you should comprehensively understand the overall process. The upcoming chapters will delve into each step in more detail.

It is worth noticing that in Listing 9.2 and 9.3, we computed the embedding for the full graph so we don’t have to compute first for the training set and then for the prediction.

Listing 9.4 Training function

def train(self, train_dataset):

node_embeddings = train_dataset.embeddings.values.tolist() node_labels = train_dataset.label.values.tolist() #B

self.scaler = StandardScaler().fit(node_embeddings) #C scaled_embeddings = self.scaler.transform(node_embeddings)

clf = LogisticRegressionCV( random_state=0, solver='liblinear', multi_class='ovr', max_iter=1000) #D self.model = clf.fit(scaled_embeddings, node_labels) #E

The training process in Listing 9.4 introduces two crucial machine learning concepts that deserve explanation: feature standardization and logistic regression.

Feature standardization is essential when working with multiple node features that operate on different scales. Consider our example: node degrees might range from single digits to thousands, while other metrics like betweenness centrality or PageRank operate on entirely different scales. By using StandardScaler to transform all features to have zero mean and unit variance, we ensure each feature contributes proportionally to the model's decisions, regardless of its original scale.

Why Feature Scaling Matters

Machine learning algorithms often rely on Euclidean distance calculations between data points. Without proper scaling, features with larger numerical ranges will dominate these calculations, regardless of their actual importance. For example, if node degree ranges from 1-1000 while centrality measures range from 0-1, the degree will overwhelm the influence of centrality in distance-based calculations. This can lead to biased predictions and reduced model accuracy, as the model becomes overly sensitive to features simply because of their scale rather than their predictive power.

Logistic regression[10], despite its name, is a classification algorithm that excels at binary prediction tasks. Here it is estimating the probability of a

node belonging to a particular class. The algorithm transforms linear combinations of features into probabilities between 0 and 1 using the logistic function, making it ideal for our node classification task. For instance, in the karate club example, it predicts the probability of each member joining either the instructor's or administrator's club.

This implementation demonstrates a key advantage of our approach: by converting graph data into feature vectors through embeddings and proper scaling, we can leverage well-established machine learning algorithms. The subsequent evaluation phase (Listing 9.5) tests this model's accuracy by comparing predicted labels with actual outcomes for a held-out set of test nodes.

Listing 9.5 Evaluating function which make prediction and compare with actual value

def evaluate(self, test_dataset):

test_embeddings = test_dataset.embeddings.values.tolist() true_labels = test_dataset.label.values.tolist() #B

scaled_test_embeddings = self.scaler.transform(test_embedd

predicted_labels = self.model.predict(scaled_test_embeddin

print("True labels:\t\t", true_labels) print("Predicted labels:\t", list(predicted_labels))

# Calculate performance metrics metrics = precision_recall_fscore_support(true_labels, pre print('Precision:', metrics[0], 'Recall:', metrics[1], 'f- conf_matrix = confusion_matrix(true_labels, predicted_labe print("Confusion Matrix:\t", conf_matrix)

The function described in Listing 9.5 is useful in making a proper evaluation of the quality of the predictive model. In our toy example, we have a very small network and just two labels. The code also works for bigger graphs and for multiple labels.

At this point, we have all the ingredients to perform the training and the evaluation of our node classification task. Listing 9.6 uses all the components

(functions) of previous listings and prints the results. Switching from one embedding to the other requires just commenting and uncommenting the desired embedding function.

Listing 9.6 The full process

G = nx.karate_club_graph() draw_and_save_graph_picture(G)

labels = np.asarray([G.nodes[i]['club'] != 'Mr. Hi' for i in G.no

.astype(np.int64) #A

#embeddings = compute_degree_embeddings(G) #B embeddings = compute_complex_embeddings(G)

df = pd.DataFrame({

'nodeId': G.nodes, 'embeddings': embeddings, 'label': labels }) #C

train, test = train_test_split(df, test_size=0.4, random_state=0)

classifier = EvaluateEmbedding() classifier.train(train) #E classifier.evaluate(test) #E

After running the code with the two different embedding techniques we can see the different results in the following listing.

Listing 9.7 Results for the two embeddings

RESULTS WITH MORE SIMPLE EMBEDDINGS USING DEGREE

Gold: [0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0] Predicted: [1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0]

Precision: 0.44642857142857145 Recall: 0.42857142857142855 f-score: 0.42857142857142855

Confusion Matrix: [[3 3]

[5 3]] #A

RESULTS WITH MORE COMPLEX EMBEDDINGS USING NODE2VEC Gold: [0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0] Predicted: [0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0]

Precision: 0.6530612244897959 Recall: 0.6428571428571429 f-score: 0.6446886446886447

Confusion Matrix: [[4 2]

[3 5]] #B

Let's examine these confusion matrices in detail to understand model performance. In the first run, the model's predictions broke down as follows:

Top-left (3): True Negatives (0→0) - Correctly predicted class 0 Top-right (3): False Negatives (1→0) - Incorrectly predicted class 0 when class 1 Bottom-left (5): False Positives (0→1) - Incorrectly predicted class 1 when class 0 Bottom-right (3): True Positives (1→1) - Correctly predicted class 1

The second run (B) showed different results:

Top-left (4): True Negatives - Correctly predicted class 0 Top-right (2): False Negatives - Incorrectly predicted class 0 when actually class 1 Bottom-left (3): False Positives - Incorrectly predicted class 1 when actually class 0 Bottom-right (5): True Positives - Correctly predicted class 1

The results from both runs demonstrate relatively poor and inconsistent performance. Multiple executions of these scenarios show significant volatility in the outcomes, with f-scores fluctuating widely between 30% and 70%. Given that logistic regression is a well-established and proven algorithm, this instability suggests that the limitation lies not in the classification method itself but in our feature engineering approach. The high variance in results indicates that our simple node representation, based solely on degree, fails to capture the complex network patterns necessary for reliable node classification. The same happens with Node2Vec.

Let’s improve our feature engineering by leveraging homophily. Instead of using only node degree, which simply counts total connections, we can create a richer node representation by considering the social context of these connections. Specifically, for each node, we'll create a three-element vector containing:

1. the total degree (total connections), 2. the "Mr. Hi degree" (connections to the instructor's group), and 3. the "Officer degree" (connections to the administrator's group).

This approach capitalizes on homophily by assuming that a node's group membership is likely influenced by the group affiliations of its neighbors. Listing 9.8 implements this enhanced feature engineering approach.

Listing 9.8 Compute featurization based on three types of degrees

def compute_specific_degree_embeddings(G):

clubs = nx.get_node_attributes(G, "club") mr_hi_degree = #A [[clubs[c] for c in G.neighbors(i)].count('Mr. Hi') for i officer_degree = #B [[clubs[c] for c in G.neighbors(i)].count('Officer') for degree = list(dict(G.degree()).values()) #C embeddings = #D [[degree[i], mr_hi_degree[i], officer_degree[i]] for i in return embeddings

Now that we have a new function, we need just to change the embedding function in the code of Listing 9.6 to point to the new one. The results are presented in Listing 9.9.

Listing 9.9 Results obtained using the new vectors made by using three degrees

Gold: [0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0] Predicted: [0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1]

Precision: 0.9365079365079365 Recall: 0.9285714285714286 f-score: 0.9274255156608098

Confusion Matrix: [[5 1]

[0 8]]

As you can immediately notice the results are astonishingly better than before. You can run it multiple times and the results will be very stable and always close to 100%. Even the confusion matrix shows how much accurate it is.

This experiment with the karate club graph, though simple, illuminates several crucial principles for building effective Intelligent Advisory Systems using machine learning on graphs:

Feature Engineering is Critical: The way we represent nodes and relationships fundamentally impacts the success of graph-based machine learning tasks, with domain-informed features often outperforming simple metrics like degree centrality. Autonomous Embeddings Require Careful Tuning: Methods like Node2Vec, while powerful, don't automatically guarantee optimal results - parameters must be thoughtfully configured to avoid generating overly homogeneous node representations. As an exercise, execute the experiment using lower values of walk_length and num_walks. Domain Understanding Matters: Knowledge of underlying graph dynamics and network properties like homophily can guide better feature engineering strategies, often proving more effective than generic approaches.

These insights will serve as foundations as we explore more sophisticated approaches to graph representation learning in subsequent chapters.

9.3.2 Graph Classification

In the case of graph classification, the approach is very similar but in most of the algorithms what happens is that instead of extracting features for the nodes a set of features for the entire graph is computed or extracted, and, of course, we have as input multiple graphs. In Figure 9.10, the high-level process for graph classification is presented.

Figure 9.10 High-level training process for the graph classification task. The output is a classifier model that has been trained on the known classes of different graphs.

Once the model has been trained it is used as one of the inputs of the prediction phase together with the unclassified graphs. Figure 9.11 shows the key steps and actors of the prediction.

Figure 9.11 High-level prediction process for the graph classification task. The output in this case is the classes assigned to the previous unlabeled graphs.

Other tasks follow the same approach depicted in Figure 9.11, but in this book, we are considering only graph classification as an example of graph- level machine learning tasks.

From the above discussion, and throughout the rest of this book, it becomes evident that feature extraction for nodes, relationships, and graphs is a vital aspect of the process since this form the inputs for training and prediction. This is particularly important given the unique nature of graph-based machine learning as compared to other processes, which often leverage algorithms that have been battle-tested over four decades or more (consider, for example, Bayesian inference[11] or Logistic Regression and their longevity). Similarly, the relatively newer - but valuable - deep learning algorithms have proven their worth. As we will explore further in this book, the final quality of the output, entailing accuracy, and overall performance, largely depends on the quality of these features and the precise tuning of downstream algorithm parameters. That's why the subsequent chapter, and a significant portion of the rest of this book, focuses on this exact task and how to effectively utilize the results of this process in accomplishing specific tasks.

9.3.3 Graph Clustering

It is worth mentioning that not all machine learning tasks on graphs require the same steps as above. In the case of graph clustering, for example, the input is the whole graph (or a subgraph) and the algorithms use nodes and relationships to extract communities.

Figure 9.12 High-level graph clustering task process. This is an example of a pure graph algorithm approach that doesn’t require an intermediate transformation in a vector representation or other formats.

For this task, there is no feature extraction since graph algorithms leverage the node relationships, their directions, and their weights to split the graph.

Certain algorithms like WCC[12], which we have seen in the first part of the book, use a fixed mechanism and consider only completely independent subgraphs. Other methods, such as the Louvain and Label Propagation algorithms, aim to optimize certain outcomes, such as modularity. Louvain has been discussed previously in the book, so let us introduce quickly Label Propagation.

The Label Propagation Algorithm (LPA) [20] is a fast method for detecting communities within a network. It operates by propagating labels throughout the network, and at the end, nodes with the same labels are considered as belonging to the same community. It's important to note that LPA doesn't guarantee a consistent output due to its randomness. Therefore, running it multiple times on the same network may produce slightly different communities.

These algorithms have been designed to work on graphs, so they don’t need any featurization process, they have already all they need in the graph structure.

To show how the process works, let’s use the same toy graph from Karate Club and run Louvain on it. Listing 9.10 contains all the code necessary to accomplish this task.

Listing 9.10 Running Louvain on Karate Club’s network

import math import time import networkx as nx import matplotlib.pyplot as plt

def set_club_colors(G):

for node in G.nodes(data=True): # Mr. Hi = 'purple', Officier = 'blue' color = '#00fff9' if node[1]['club'] == 'Mr. Hi': color = '#e6e6fa' node[1]['color'] = color

def draw_and_save_graph_picture(G, i=0):

set_club_colors(G) layout_position = nx.spring_layout(G, k=8 / math.sqrt(G.order

colors = [n[1]['color'] for n in G.nodes(data=True)] nx.draw_networkx(G, pos=layout_position, node_color=colors) plt.axis('off') plt.savefig("Karate_Graph_" + str(i) + ".svg", format="SVG", plt.savefig("Karate_Graph_" + str(i) + ".png", format="PNG", plt.show()

if __name__ == '__main__':

start = time.time() G = nx.karate_club_graph() draw_and_save_graph_picture(G) communities = nx.community.louvain_communities(G, seed=123) i = 1 for community in communities: #B subGraph = G.subgraph(community) draw_and_save_graph_picture(subGraph, i) i += 1

end = time.time() - start print("Time to complete:", end)

The code above identifies 4 communities, each of which is represented in Figure 9.13.

Figure 9.13 The result of the community detection algorithm applied to the Karate Club network. The algorithm has identified 4 different communities. All but one contain members of the same groups. Only in one case, there is a spurious element, but it is very well connected to other people belonging to a different group.

We can immediately recognize that the algorithm did a great job identifying groups of nodes that are well connected with other nodes in the groups and loosely connected with nodes outside. This is easy to notice since each group is pretty much homogeneous with nodes belonging to the same subgroup (the different grey scales of the nodes help us to notice that). In only one case we have a spurious node (the number 8) which is in a group with all nodes of the other group. Maybe he is in the wrong group!

It's crucial to highlight two aspects that distinguish this task from the ones we've covered prior:

There's no requirement for representation transformation. The entire graph serves as the input, and the algorithm directly interacts with the nodes and relationships. There's no training phase. This task follows an unsupervised approach. It takes the graph and generates the output without employing any examples of labels, etc.

In some use cases, we leveraged the DBSCANN[13] algorithm to perform graph clustering. In that case, we had, or we created a feature vector for each node and then we applied the classical clustering algorithm to extract groups of nodes. Our experiments showed that the results were very poor compared with the results obtained by specific graph algorithms. This is easy to understand since DBSCANN doesn’t leverage the relationship information unless we use it to generate the features. There are scenarios in which this approach can make sense, for example when you have features representing certain types of textual attributes, but since it doesn’t leverage the information power of the graph it will not be considered in this book.

9.4 Summary

Machine Learning on graphs emerges as a crucial approach because it naturally handles interconnected data, provides universal data representation, and enables the modeling of complex problems through a limited set of computational tasks. Unlike traditional machine learning that assumes independent and

identically distributed (i.i.d.) data points, graph-based approaches explicitly leverage connections and dependencies between nodes, better reflecting real-world relationships and patterns. Machine learning tasks on graphs fall into two main categories: node- focused (like node classification and link prediction) where tasks are performed on a single graph, and graph-focused (like graph classification) where each graph is a separate data point. Node classification and link prediction are typically semi-supervised tasks, utilizing both labeled and unlabeled data, along with the graph's structural information. Community detection, in contrast, is usually unsupervised and operates directly on graph structure. Feature engineering plays a critical role these tasks. Simple approaches (like using node degree) often underperform compared to more sophisticated features that capture domain knowledge (like homophily- based metrics) or structural patterns. Node embeddings can be generated through various approaches, from manual feature engineering to automated techniques like Node2Vec. However, autonomous embeddings require careful tuning to avoid generating overly homogeneous representations. The success of graph-based machine learning often depends on finding the right balance between automated feature learning, domain knowledge incorporation, and appropriate algorithm selection for the specific task at hand.

9.5 References

1. A. Negro, Graph-Powered Machine Learning. Shelter Island, NY, USA:

Manning Publications, 2021 2. A. Mukkamala, P. Szolovits, and J. Lin, "Representation Learning: From

Diagnoses to Healthcare Analytics," Data Science, vol. 4, no. 2, pp. 125- 146, Sep. 2021. Accessed: May 15, 2024 [Online]. Available: https://content.iospress.com/articles/data-science/ds007 3. Y. Ma and J. Tang, Deep Learning on Graphs. Cambridge, UK:

Cambridge University Press, 2021 4. W. L. Hamilton, Graph Representation Learning. San Rafael, CA, USA:

Morgan & Claypool Publishers, 2020 5. T. N. Kipf and M. Welling, "Semi-Supervised Classification with Graph

Convolutional Networks," in International Conference on Learning Representations (ICLR 2016), San Juan, Puerto Rico, 2016. Accessed: May 15, 2024 [Online]. Available: https://arxiv.org/abs/1609.02907 6. M. McPherson, L. Smith-Lovin, and J. Cook, "Birds of a Feather:

Homophily in Social Networks," Annual Review of Sociology, vol. 27, no. 1, pp. 415-444, 2001 7. D. Zhou, O. Bousquet, T. Lal, J. Weston, and B. Scholkopf, "Learning

with Local and Global Consistency," in Proceedings of the 17th International Conference on Neural Information Processing Systems (NeurIPS 2004), Vancouver, BC, Canada, 2004. Accessed: May 15, 2024 [Online]. Available: https://proceedings.neurips.cc/paper/2003/hash/87682805257e619d49b8e Abstract.html 8. C. Donnat, M. Zitnik, D. Hallac, and J. Leskovec, "Graph Wavelets for

Structural Role Similarity in Complex Networks," in Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD 2018), London, UK, 2018. Accessed: May 15, 2024 [Online]. Available: https://dl.acm.org/doi/10.1145/3219819.3220025 9. Z. Yang, W. Cohen, and R. Salakhutdinov, "Revisiting Semi-Supervised

Learning with Graph Embeddings," in Proceedings of the 33rd International Conference on Machine Learning (ICML 2016), New York, NY, USA, 2016. Accessed: May 15, 2024 [Online]. Available: https://proceedings.mlr.press/v48/yanga16.html 10. L. Tang and H. Liu, "Relational Learning via Latent Social

Dimensions," in Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD 2009), Paris, France, pp. 817-826, 2009. Accessed: May 15, 2024 [Online]. Available: https://dl.acm.org/doi/10.1145/1557019.1557109 11. L. A. Adamic and E. Adar, "Friends and Neighbors on the Web," Social

Networks, vol. 25, no. 3, pp. 211-230, 2003 12. R. Ying, R. He, K. Chen, P. Eksombatchai, W. L. Hamilton, and J.

Leskovec, "Graph Convolutional Neural Networks for Web-Scale Recommender Systems," in Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD 2018), London, UK, pp. 974-983, 2018. Accessed: May 15, 2024 [Online]. Available: https://dl.acm.org/doi/10.1145/3219819.3219890

13. M. Nickel, K. Murphy, V. Tresp, and E. Gabrilovich, "A Review of

Relational Machine Learning for Knowledge Graphs," Proceedings of the IEEE, vol. 104, no. 1, pp. 11-33, 2015. Accessed: May 15, 2024 [Online]. Available: https://ieeexplore.ieee.org/document/7358050 14. M. Zitnik, M. Agrawal, and J. Leskovec, "Modeling polypharmacy side

effects with graph convolutional networks," Bioinformatics, vol. 34, no. 13, pp. i457–i466, 2018. 15. A. Bordes, N. Usunier, A. Garcia-Duran, J. Weston, and O. Yakhnenko,

"Translating Embeddings for Modeling Multi-relational Data," in Proceedings of the 26th International Conference on Neural Information Processing Systems (NeurIPS 2013), Lake Tahoe, NV, USA, 2013. Accessed: May 15, 2024 [Online]. Available: https://papers.nips.cc/paper_files/paper/2013/hash/1cecc7a77928ca8133f Abstract.html 16. Wang, H. Huang, C. Ding, and F. Nie, "Predicting Protein–Protein

Interactions from Multimodal Biological Data Sources via Nonnegative Matrix Tri-Factorization," Journal of Computational Biology, vol. 20, no. 4, pp. 344-358, 2013. Accessed: May 15, 2024 [Online]. Available: https://doi.org/10.1089/cmb.2012.0273 17. G. Berlusconi, F. Calderoni, N. Parolini, M. Verani, and C. Piccardi,

"Link Prediction in Criminal Networks: A Tool for Criminal Intelligence Analysis," PLOS ONE, vol. 11, no. 4, e0154244, 2016. Accessed: May 15, 2024 [Online]. Available: https://doi.org/10.1371/journal.pone.0154244 18. M. Agrawal, M. Zitnik, J. Leskovec, "Large-scale analysis of disease

pathways in the human interactome," in Proc. Pacific Symposium on Biocomputing (PSB), 2018, pp. 111-122. Accessed: Nov. 18, 2024 [Online]. Available: https://pmc.ncbi.nlm.nih.gov/articles/PMC5731453/ 19. S. Pandit, D. Chau, S. Wang, and C. Faloutsos, "NetProbe: A fast and

scalable system for fraud detection in online auction networks," in Proc. 16th International World Wide Web Conference (WWW), 2007, pp. 201-210. Accessed: Nov. 18, 2024 [Online]. Available: https://www.cs.cmu.edu/~christos/PUBLICATIONS/netprobe- www07.pdf 20. U. N. Raghavan, R. Albert, and S. Kumara, "Near linear time algorithm

to detect community structures in large-scale networks," Physical Review E, vol. 76, no. 3, 036106, 2007. Accessed: Nov. 18, 2024

[Online]. Available: https://arxiv.org/pdf/0709.2938 21. J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl,

"Neural message passing for quantum chemistry," in Proc. 34th International Conference on Machine Learning (ICML), 2017, pp. 1263- 1272. Accessed: Nov. 18, 2024 [Online]. Available: https://proceedings.mlr.press/v70/gilmer17a.html 22. Y. Li, C. Gu, T. Dullien, O. Vinyals, and P. Kohli, "Graph matching

networks for learning the similarity of graph structured objects," in Proc. 36th International Conference on Machine Learning (ICML), 2019, pp. 3835-3845. Accessed: Nov. 18, 2024 [Online]. Available: https://proceedings.mlr.press/v97/li19d.html 23. P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. Eliassi-

Rad, "Collective classification in network data," AI Magazine, vol. 29, no. 3, pp. 93-106, 2008. Accessed: Nov. 18, 2024 [Online]. Available: https://doi.org/10.1609/aimag.v29i3.2157 24. 24. W. W. Zachary, "An information flow model for conflict and fission

in small groups," Journal of Anthropological Research, vol. 33, no. 4, pp. 452-473, 1977. Accessed: Nov. 18, 2024 [Online]. Available: https://www.jstor.org/stable/3629752 25. A. Grover and J. Leskovec, "node2vec: Scalable Feature Learning for

Networks," in Proc. 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), 2016, pp. 855-864. Accessed: Nov. 18, 2024 [Online]. Available: https://arxiv.org/abs/1607.00653

[1] https://www.flickr.com/

[2] https://pubmed.ncbi.nlm.nih.gov/

[3] https://www.medrxiv.org/

[4] https://www.kaggle.com/datasets/allen-institute-for-ai/CORD-19- research-challenge

[5] https://clarivate.com/academia-government/scientific-and-academic- research/research-discovery-and-referencing/web-of-science/

[6] https://dblp.org/

[7] K-means partitions n data points into k clusters by iteratively assigning each point to the nearest cluster center (centroid) and updating these centers. The 'k' number of clusters must be specified in advance.

[8] DBSCAN (Density-Based Spatial Clustering of Applications with Noise) groups together points that are closely packed (points with many nearby neighbors) while marking points in low-density regions as outliers. It doesn't require specifying the number of clusters beforehand.

[9] https://networkx.org/

[10] Logistic regression is a fundamental classification algorithm that estimates the probability of a binary outcome (like node class membership) by transforming a linear combination of input features using the logistic (sigmoid) function. Unlike linear regression which outputs continuous values, logistic regression maps its predictions to probabilities between 0 and 1. It does this by applying the logistic function f(z) = 1/(1 + e^(-z)) to the weighted sum of input features. The model learns the optimal weights during training to maximize the likelihood of the observed class labels given the feature values [Hosmer & Lemeshow, 2000 – REFERENCE 25 - Hosmer Jr, D. W., & Lemeshow, S. (2000). "Applied Logistic Regression" (2nd ed.). John Wiley & Sons. ISBN: 0-471-35632-8].

[11] Bayesian inference is a statistical approach that updates the probability of a hypothesis as more evidence becomes available, combining prior knowledge with observed data. It's based on Bayes' theorem: P(H|D) = P(D|H)P(H)/P(D), where P(H|D) is the posterior probability of the hypothesis given the data, P(D|H) is the likelihood of the data given the hypothesis, P(H) is the prior probability of the hypothesis, and P(D) is the evidence. Unlike frequentist approaches, Bayesian methods explicitly incorporate prior beliefs and update them with new evidence, providing a natural framework for sequential learning and uncertainty quantification [Gelman et al., 2013 – REFERENCE 26 - Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., & Rubin, D. B. (2013). "Bayesian Data Analysis" (3rd ed.). Chapman and Hall/CRC. ISBN: 978-1439840955].

[12] WCC has been introduced and used extensively in chapter 4. As a reminder, a Weakly Connected Component (WCC) in a graph is a subgraph

where any node can reach any other node when treating all edges as undirected, regardless of their original direction.

[13] Density-Based Spatial Clustering of Applications with Noise (DBSCAN) is a popular density-based clustering algorithm often used in data mining and machine learning. It works by grouping points in the dataset that are closely packed together (points with many nearby neighbors), marking points that lie alone in low-density regions as noise or outliers, and associating points that are far away from each other into separate clusters. It's especially noted for its ability to discover clusters of arbitrary shape and effectively separate noise from clusters.

## 10_Graph_feature_engineering:_manual_and_semi-automated_approaches

This chapter covers

Manual feature engineering techniques for nodes and relationships in graphs Combining domain expertise with semi-automated extraction in graph representation Real-world applications of feature engineering in fraud and drug discovery

The success of machine learning on graphs depends on a fundamental challenge: how to effectively represent graph elements (nodes, relationships, and entire graphs) as vectors that machine learning algorithms can process. This representation step, often called vectorization or featurization, determines how well our models can learn and make predictions.

While modern machine learning algorithms ‚Äì from traditional approaches like logistic regression and random forests to sophisticated deep learning models ‚Äì are well-established, they can't directly process graph structures. Instead, they require numerical input vectors. The quality of these vectors directly impacts the performance of any downstream task, whether it's classifying nodes, predicting relationships, or analyzing entire graphs.

This chapter explores the art and science of creating these vector representations, progressing from manual to automated approaches. We start with manual feature engineering, where we carefully craft interpretable features based on domain knowledge and graph properties. This hands-on approach, while time-consuming, provides deep insights into what makes representations effective and helps us understand why certain features work better than others.

As we advance through the chapter, we'll gradually introduce more automated techniques. However, an important trade-off emerges: the more automated our feature extraction becomes, the less interpretable our features tend to be. This creates a spectrum:

Manual features: Highly interpretable but labor-intensive to create (in this chapter) Semi-automated features: Balance between interpretability and efficiency (in this chapter) Fully automated features: Efficient to generate but harder to interpret (in chapters 11 and 12)

To ground these concepts, let's start with a simple classification example outside of graphs. Consider weather prediction: before we tackle complex graph representations, understanding how we represent traditional tabular data for classification will provide valuable context. The following dataset from Kaggle[1] illustrates basic feature engineering:

Table 10.1 An example of a dataset for a weather forecast.

Date Precipitation Max Min Wind Weather 2012-01-01 0.0 12.8 5.0 4.7 Drizzle 2012-01-02 10.9 10.6 2.8 4.5 Rain 2012-01-03 0.8 11.7 7.2 2.3 Rain 2012-01-04 20.3 12.2 5.6 4.7 Rain 2012-01-05 1.3 8.9 2.8 6.1 Rain 2012-01-06 2.5 4.4 2.2 2.2 Rain 2012-01-07 0.0 7.2 2.0 2.3 Rain 2012-01-08 0.0 10.0 2.8 2.0 Sun

Let's examine this weather prediction dataset in detail. Each row represents a daily weather record from Seattle, containing measurable attributes like precipitation, temperature range, and wind speed. The "Weather" column serves as our prediction target (or label). During model training, we use rows where we know both the features (measured attributes) and the label (actual weather). For prediction, we apply our trained model to new data where we only have the features and need to determine the weather label.

This straightforward example highlights key concepts that are similar to node classification in graphs but with an important distinction: in traditional datasets like weather prediction, features are direct measurements from the real world. In graph-based machine learning, we must construct our features from the graph structure itself. Instead of physical measurements, we need to capture meaningful properties of nodes, relationships, or entire graphs as numerical values.

There are two fundamental approaches to creating these graph-based features:

Feature Engineering relies on manually designed features based on graph properties and domain knowledge. These features are highly interpretable but time-consuming to create, and they may not capture all relevant patterns for complex tasks. Common examples include node degree, clustering coefficients, and various centrality measures. Representation Learning, in contrast, automatically learns feature representations from the graph structure. This approach requires minimal human intervention and can adapt to specific tasks through training. While it often captures complex patterns more effectively than manual engineering, it typically produces features that are harder to interpret.

While this chapter focuses on feature engineering, understanding its challenges and limitations helps us appreciate why representation learning (covered in subsequent chapters) has become increasingly important. Manual feature engineering remains valuable for two key reasons. First, it produces interpretable features that humans can understand and validate. Second, it provides insights into what makes graph representations effective, informing the design of automated approaches.

Another significant advantage of manually extracted features is their compatibility with Large Language Models (LLMs) for autonomous reasoning about graphs. Because these features are based on well-understood graph algorithms and properties, LLMs can effectively interpret and reason about them. For instance, when analyzing a social network, an LLM can understand that high betweenness centrality might indicate key intermediaries in criminal networks, or that specific clustering patterns could reveal hidden organizational structures. This interpretability enables LLMs to perform

sophisticated analysis tasks like identifying potential money laundering patterns in financial networks, detecting influential spreaders in disease transmission networks, or recognizing bottleneck points in supply chain graphs. The human-understandable nature of these features thus extends beyond direct human interpretation to enable automated reasoning systems to make meaningful inferences about graph structures.

We'll explore three practical approaches to feature engineering, each drawn from real-world consulting projects. These examples demonstrate both the power and limitations of manual feature extraction in graph-based machine learning.

10.1 Manual node features

As usual, let‚Äôs describe a realistic scenario where these manual features can make sense. Suppose that you have a network of people, i.e. a graph where you have nodes representing people and relationships represent any connection among people. Yes, you could call it a social network since that's what it is. The reason why we preferred a more generic network of people is because it can represent any sort of connection among people like frequent calls, family or co-working connection, etcetera. We are considering then a more generic set of connections than what you can get just because people are ‚Äúfriends‚Äù on a social media platform like Facebook. Moreover, this is not a simple set of people, among them there are some recognized fraudsters. The frauds can be of any type, tax fraud, financial fraud, or credit card fraud. It doesn't matter.

In our network, we don‚Äôt have the full list of fraudsters, so our work is to ‚Äúclassify‚Äù nodes to recognize the remaining unrecognized fraudsters or determine the risk of people being victims of fraudulent activities. The sample network is represented in Figure 10.1.

Figure 10.1 An example of a fraudulent social network. The white nodes represent legit people or people for whom we don‚Äôt know if they are fraudsters or not. The black nodes represent fraudsters.

As you can see from Figure 10.1, there are two types of nodes, the black ones representing recognized fraudsters and the white nodes representing the rest. At this stage, we can‚Äôt say if the white ones are licit people or not, since it could be that they are, but they haven‚Äôt been recognized as such.

This network serves as our running example throughout the chapter to demonstrate manual feature computation and effectiveness. To help you follow along and experiment with the code, Listing 10.1 shows how to create this network using Python and NetworkX:

Listing 10.1 Creating the fraud detection network example

import networkx as nx import matplotlib.pyplot as plt

def create_fraud_network():

G = nx.Graph() #A fraudsters = ['D', 'E', 'F', 'I'] #B # Add all nodes first nodes = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T'] # G.add_nodes_from(nodes) for node in G.nodes(): G.nodes[node]['is_fraudster'] = node in fraudsters #D edges = [ ('A', 'B'), ('A', 'G'), ('A', 'H'), ('A', 'I'), ('A', ('A', 'T'), ('B', 'D'), ('B', 'C'), ('D', 'E'), ('D', ('D', 'G'), ('E', 'F'), ('F', 'G'), ('G', 'I'), ('H', ('I', 'K'), ('I', 'N'), ('K', 'J'), ('L', 'M'), ('L', ('N', 'M'), ('O', 'P'), ('O', 'Q'), ('Q', 'R'), ('Q', ] #E G.add_edges_from(edges) return G #F

This code recreates the exact network shown in Figure 10.1, where:

Black nodes represent known fraudsters (nodes D, E, F, and I) White nodes represent legitimate users or unidentified nodes All connections between nodes are represented as undirected edges

This network is our running example for demonstrating various measures and network metrics throughout the chapter. You can use this network for all the subsequent analyses we perform in the chapter by simply calling:

Listing 10.2 Example usage

G = create_fraud_network()

# Use this graph object with any of our metric calculations degree_metrics = compute_degree_metrics(G) #defined later triangle_metrics = compute_triangle_metrics(G) #defined later # ... and so on

Our goal is to implement a classifier that given a node will be able to say if it is a fraudster or not. We introduced the classification task earlier in this chapter, as one of the most common tasks in the field of machine learning on graphs. Generally, the task is accomplished by leveraging well-known classical classification algorithms, like Logistic Regression, Bayesian classifier, decision tree, random forest, and so on. They are all supervised algorithms, so during the training, the input is a set of features associated with each data point ‚Äì the nodes in our cases ‚Äì and a label, in our case fraudster/not fraudster. After the training builds a machine learning model, given a set of features representing an ‚Äúunseen‚Äù data point (i.e. an item not used during the training since we don‚Äôt know the label) the classifier will be able to assign labels with a certain likelihood.

In order to use such algorithms, hence, we have to ‚Äúrepresent‚Äù each node as a set of features that reasonably can be used by the algorithms during the training and the prediction as good indicators.

Figure 10.2 Node feature extraction: using metrics and graph algorithms to transform nodes into numerical feature vectors that capture their key characteristics.

There are many of interesting information we can extract for each node. We will distinguish among two major types:

The local features are those we can extract considering the node's neighborhood or the node‚Äôs egonet. An ego-centered network or egonet represents the one-hop neighborhood of the node of interest. In other words, an egonet consists of a particular node and its immediate neighbors. The center of the egonet is the ego, and the surrounding nodes are the alters. These local features can also consider a n-order neighborhood around a node, that consists of the nodes that are n hops apart from that node. The global features measure the role each node has in the entire network or in a great portion of it (not in the egonet or in a n-order neighborhood). In this category fall the centrality metrics, like Betweenness centrality, Closeness centrality, PageRank, and Eigenvector centrality[2]. These measures on one side can capture the influence a node can have in the network and on the other how a node can be influenced by others.

We use the metrics described to identify features representing each node. It‚Äôs worth noting here that even though the metrics used are very common in social networks we customize some of them with the purpose of improving the final quality of the classification and to show how the featurization process can be customized according to the needs.

Let's explore how we extract meaningful features from the nodes in our fraud detection network (Figure 10.1). Our approach progresses from local to global metrics, starting with features based on immediate neighbors before examining network-wide patterns. For each metric, we'll:

Define the metric and its significance Present code for automated extraction Display results in a table

We'll build these features incrementally, showing how each new metric adds another dimension to our node representations. This systematic approach allows us to capture increasingly complex patterns of network structure and

node behavior.

10.1.1 Degree

The degree of a node represents how many neighbors the node has. If applied to a specific case this measure can be further refined. In the fraud case considered here, it is often useful to distinguish between the number of fraudulent and legitimate direct neighbors. These are called fraudulent and legitimate degrees (in the table below they are shortened as fraud and legit degrees). These two measures, together with the global one, provide a better representation of the node‚Äôs direct connections. For example, if I have 10 direct neighbors and they are all fraudulent the chances that I am a fraudster are higher than the opposite case. The following Listing 10.3 contains the code that computes these values in a generic graph.

Listing 10.3 Computing different types of node degrees in a fraud detection network

def compute_degree_metrics(G):

degree_metrics = {} #A for node in G.nodes(): neighbors = list(G.neighbors(node)) total_degree = len(neighbors) #B fraud_degree = sum(1 for neighbor in neighbors if G.nodes[neighbor].get('is_fraudster' legit_degree = total_degree - fraud_degree #D degree_metrics[node] = { 'total_degree': total_degree, 'fraud_degree': fraud_degree, 'legit_degree': legit_degree } return degree_metrics #E

def get_node_degrees(G, node):

metrics = compute_degree_metrics(G) return metrics.get(node, { 'total_degree': 0, 'fraud_degree': 0, 'legit_degree': 0

}) #F

This code provides a complete implementation for calculating the degree- based metrics. It assumes the graph has nodes marked with an 'is_fraudster' boolean attribute to identify fraudulent nodes. For the toy fraud graph of Figure 10.1 these values can be computed manually. The next table contains the related values. For example, node D has a total of 4 direct neighbors, of which 2 are fraudulent and 2 are legitimate.

Exercise

As an exercise, please compute the values by yourself and check them through the table to verify that the concepts are clear. These are very simple measures to compute, later the others will be required to run the code.

Table 10.2 Global degree, fraudulent degree, and legitimate degree of the sample fraud graph in Figure 10.1.

Node A B C D E F G H I J Total Degree 6 3 1 4 2 3 4 2 4 1 Fraud Degree 1 1 0 2 2 2 3 0 0 0 Legit Degree 5 2 1 2 0 1 1 2 4 1 Node K L M N 0 P Q R S T Total Degree 3 2 2 3 3 1 3 1 1 1 Fraud Degree 1 0 0 1 0 0 0 0 0 0 Legit Degree 2 2 2 2 3 1 3 1 1 1

10.1.2 Triangles

In graph theory, a triangle is a subgraph that consists of three nodes that are all connected to each other. So, if we have three nodes A, B, and C they form a triangle if there exist relationships between the couples A-B, A-C, and B-C.

Figure 10.3 An example of three connected nodes forming a triangle (or not). If each node is connected to the other two, they constitute a triangle.

The presence of triangles in a node‚Äôs egonet is an indication that the target node has strong connections with its neighbors. Think about your close group

of people, your friends are also friends with each other. That‚Äôs why triangles reveal the influential effect of a closely connected group of individuals. As in the case of degree, it is possible to extend the concept to consider the specific domain and the goals we have. Consider a node and the two alters forming a triangle. If the two alters are both fraudulent, we can say that the triangle is fraudulent. Vice versa, if both are legitimate the triangle is legitimate. If only one is fraudulent (remember we are ignoring our target node), the triangle is called semi-fraudulent (here many of you could be optimistic people and see the glass half full and say that it is semi-legitimate, but in this case, it is better to be pessimistic).

The following code computes the total number of triangles, the fraudulent, legitimate, and semi-fraudulent triangles in a graph.

Listing 10.4 Computing triangle metrics in a fraud detection network

import networkx as nx

def compute_triangle_metrics(G):

triangle_metrics = {} #A for node in G.nodes(): triangles = [] neighbors = list(G.neighbors(node)) for i in range(len(neighbors)): for j in range(i + 1, len(neighbors)): if G.has_edge(neighbors[i], neighbors[j]): #B triangles.append((neighbors[i], neighbors[j]) total_triangles = len(triangles) #I fraud_triangles = 0 #I legit_triangles = 0 #I semi_fraud_triangles = 0 #I for n1, n2 in triangles: #C n1_fraud = G.nodes[n1].get('is_fraudster', False) n2_fraud = G.nodes[n2].get('is_fraudster', False) if n1_fraud and n2_fraud: #D fraud_triangles += 1 elif not n1_fraud and not n2_fraud: #E legit_triangles += 1

else: #F semi_fraud_triangles += 1 triangle_metrics[node] = { 'total_triangles': total_triangles, 'fraud_triangles': fraud_triangles, 'legit_triangles': legit_triangles, 'semi_fraud_triangles': semi_fraud_triangles } return triangle_metrics #G

def get_node_triangles(G, node):

metrics = compute_triangle_metrics(G) return metrics.get(node, { 'total_triangles': 0, 'fraud_triangles': 0, 'legit_triangles': 0, 'semi_fraud_triangles': 0 }) #H

Table 10.3 contains the values for our simple fraudulent graph. Even in this case, they can be computed easily. This could be another good exercise to reinforce the concepts related to triangles and their variations.

Table 10.3 Triangle measures of the sample fraud graph in Figure 10.1.

Node A B C D E F G H I J Total Triangles 1 0 0 2 1 2 2 0 1 0 Fraud Triangles 0 0 0 1 1 1 1 0 0 0 Legit Triangles 0 0 0 0 0 0 0 0 1 0 Semi-Fraud Triangles 1 0 0 1 0 1 1 0 0 0 Node K L M N 0 P Q R S T Total Triangles 0 1 1 1 0 0 0 0 0 0 Fraud Triangles 0 0 0 0 0 0 0 0 0 0 Legit Triangles 0 1 1 1 0 0 0 0 0 0 Semi-Fraud Triangles 0 0 0 0 0 0 0 0 0 0

10.1.3 Density

Another measure indicating how strictly connected are nodes and, hence,

how they can influence each other is the density. The density measures the extent to which nodes in a graph are connected. Given a graph of N nodes a fully connected graph has several edges computed by the following formula:

In this case, each node is connected to every other node in the network. The density measures the portion of these possible edges that are observed in the actual graph. So, supposing that M is the number of edges in the graph, density can be computed with the following formula:

The formula above computes the density of the entire network. It is possible to compute the density for each node considering the density of its egonet. For example, if we take the node A as the target node. Its egonet has 7 which means that the total possible edges in the egonet is 21, obtained as 7(7 ‚Äì 1)/2. The observed edges are 7. So, 7 divided by 21 is ~0.33. In this example it was relatively simple, the Listing 10.5 contains the code when things get harder. This measure for its nature is not extended with specific values related to the fraud domain we are considering.

Listing 10.5 Computing egonet density in a fraud detection network

import networkx as nx

def compute_density_metrics(G):

density_metrics = {} #A for node in G.nodes(): neighbors = list(G.neighbors(node)) egonet_nodes = neighbors + [node] #B N = len(egonet_nodes) #I

if N < 2: # Handle special case where egonet is too smal density_metrics[node] = 0.0 continue M = 0 for i in range(len(egonet_nodes)): for j in range(i + 1, len(egonet_nodes)): if G.has_edge(egonet_nodes[i], egonet_nodes[j]): M += 1 max_possible_edges = (N * (N - 1)) / 2 #D density = M / max_possible_edges #E density_metrics[node] = round(density, 2) #F return density_metrics #G

def get_node_density(G, node):

metrics = compute_density_metrics(G) return metrics.get(node, 0.0) #H

Running the previous listing on the sample graph we are considering the values will look like those presented in Table 10.4.

Table 10.4 Density measures of the sample fraud graph in Figure 10.1.

Node A B C D E F G H I J Density 0.33 0.5 1 0.6 1 0.83 0.6 0.66 0.5 1 Node K L M N 0 P Q R S T Density 0.5 1 1 0.66 0.5 1 0.5 1 1 1

Differently from the measures we have seen so far, that use the egonet of the node to compute their value, from now on the measures consider the entire network during the computation.

10.1.4 Geodesic (or shorter) path

The geodesic path or shortest path represents the minimum distance between two nodes. It is possible to use this concept and combine it with the specific needs of the domain and the purpose we must identify features that could be

the input for downstream algorithms. A key question we could ask is: how far is the closest fraudulent node to our target node? The basic idea is that if fraudulent nodes are closer, fraud might impact more intensively and easily contaminate the target of interest.

Calculating geodesic paths is computationally expensive. There are different algorithms available for computing the shortest paths. Considering that we need to compute the shortest path between a target node and all the others, one of the best algorithms to do so is Dijkstra[3]. This algorithm is the one used in Listing 10.6.

In the fraud case, not all the paths are the same. It is interesting to know whether there are paths between fraudulent nodes and legitimate nodes, how long they are, and how many of these the network contains. If more paths exist between two nodes and these are short, there is a higher chance that fraudulent behavior will affect the target node.

Based on these considerations, for our features, we decided to take the shortest path to a fraudulent node (geodesic path in the table). Even though this is a piece of relevant information, we need to know also how many fraudulent nodes are surrounding a node at a certain distance, so we also took the number of paths with 1, 2, and 3 hops to any fraudulent node.

This time before showing the code let‚Äôs start with some concrete examples and the table with the values that can be computed manually. In this way, it should be easier to understand the concepts. The code will come later.

The node A is connected to node I via a single hop (direct connection). The geodesic distance is, hence, 1. There are no other direct connections to other fraudulent nodes: the number of 1-hop paths is 1. Regarding the number of 2- hop paths, they are 4: A-G-I, A-B-D, A-G-D, A-G-F. And so on for the 3-hop and then for all the other nodes. The result of this process is in Table 10.5.

Table 10.5 Geodesic paths of the sample fraud graph in Figure 10.1.

Node A B C D E F G H I J Geodesic path 1 1 2 0 0 0 1 2 0 2 #1-hop paths 1 1 0 2 2 2 3 0 0 0

#2-hop paths 4 3 1 8 4 7 5 2 6 1 #3-hop paths 18 13 3 19 15 17 25 4 9 0 Node K L M N 0 P Q R S T Geodesic path 1 2 2 1 2 3 3 4 4 2 #1-hop paths 1 0 0 1 0 0 0 0 0 0 #2-hop paths 0 1 1 0 1 0 0 0 0 1 #3-hop paths 9 1 1 8 4 1 1 0 0 4

Our network is straightforward and computing even the 3 hop paths is not complicated and recommended as exercise. Nevertheless, for real networks, manual computation is unfeasible and not practical. Listing 10.6 contains the code for extracting the features related to the geodesic distance automatically. It uses the library networkx, which has an implementation of the Dijkstra algorithm from a single source. Even Neo4j, in the current GDS library[4], has a couple of Dijkstra implementations, one of which computes the shortest paths between a source node and all nodes reachable from that node.

Listing 10.6 Computing geodesic path metrics in a fraud detection network

import networkx as nx from collections import defaultdict

def compute_geodesic_metrics(G, max_hops=3):

path_metrics = {} #A fraudster_nodes = [n for n, attr in G.nodes(data=True) if attr.get('is_fraudster', False)] #B for node in G.nodes(): if G.nodes[node].get('is_fraudster', False): geodesic_path = 0 #J hop_counts = defaultdict(int) else: paths_to_fraudsters = [] #C hop_counts = defaultdict(int) for fraudster in fraudster_nodes: try: path = nx.shortest_path(G, node, fraudster) path_length = len(path) - 1 # Convert to num paths_to_fraudsters.append(path_length)

if path_length <= max_hops: hop_counts[path_length] += 1 #E except nx.NetworkXNoPath: continue geodesic_path = min(paths_to_fraudsters) if paths_to_ path_metrics[node] = { 'geodesic_path': geodesic_path, '#1-hop_paths': hop_counts[1], '#2-hop_paths': hop_counts[2], '#3-hop_paths': hop_counts[3] } #G return path_metrics #H

def get_node_paths(G, node):

metrics = compute_geodesic_metrics(G) return metrics.get(node, { 'geodesic_path': float('inf'), '#1-hop_paths': 0, '#2-hop_paths': 0, '#3-hop_paths': 0 }) #I

We are not going to describe the algorithm since it is out of the scope of the book, but we recommend [3] where the reader can find a detailed explanation. What is relevant from the previous implementation is that it computes the geodesic distance from the starting node to the fraudster nodes only and up to a predefined distance.

10.1.5 Closeness

Closeness centrality represents how a node is ‚Äúclose‚Äù to all the other nodes in the network. More precisely, it measures the mean distance from a node to all other nodes in the network, where the distance between nodes is computed as geodesic or shortest path, described in the previous section, between them. Given a network with N nodes, the mean geodesic distance or ‚Äúfarness‚Äù from node i to the other nodes is computed as follows:

This formula calculates the average shortest path length from a node to all other nodes in the network. Let's examine each part:

The numerator Œ£

## 11_Graph_representation_learning_and_graph_neural_network

This chapter covers

Graph representation learning and its role in scaling machine learning on graphs How deep learning automates the feature engineering process The fundamentals of graph embeddings and their applications Introducing Graph Neural Networks (GNNs)

In chapters 9 and 10, we explored the fundamental concepts of machine learning on graphs, demonstrating how these techniques can solve complex tasks like node classification, link prediction, and community detection. We showed how manual feature engineering can effectively capture graph properties and relationships to power downstream machine learning tasks through carefully crafted examples and hands-on implementations. These approaches provided valuable insights into what makes graph-based machine learning work, offering complete transparency into how our models make decisions.

However, as our Karate Club experiments revealed, even simple classification tasks required significant effort to design and implement effective features. While manual approaches excel at interpretability and help build intuition, they face significant challenges when scaled to real-world knowledge graphs containing millions of nodes and relationships. Consider a drug discovery knowledge graph, like the Hetionet database we have seen in the previous chapter, integrating data from multiple biomedical databases ‚Äì manually crafting features to capture all relevant patterns between compounds, proteins, and diseases becomes prohibitively complex. Similarly, in fraud detection networks where new patterns emerge constantly, static hand-crafted features may fail to adapt quickly enough.

Graph Representation Learning (GRL) offers a powerful solution to these

scalability challenges. Rather than relying on human expertise to design features, GRL leverages deep learning techniques to automatically learn optimal representations ‚Äì called embeddings ‚Äì directly from the graph structure and node attributes. These learned representations capture complex patterns and similarities that would be difficult or impossible to specify manually. The embedding process transforms nodes, edges, and subgraphs into dense vector representations that preserve the graph's structural and semantic properties while being readily consumable by standard machine learning algorithms.

Graph Neural Networks (GNNs) have emerged as a particularly effective framework for learning these representations. By iteratively aggregating and transforming information from neighboring nodes, GNNs can automatically discover relevant features for downstream tasks while handling the unique challenges of graph-structured data. This chapter introduces the key concepts behind graph representation learning and GNNs, showing how they enable machine learning on graphs to scale to real-world applications.

While we'll explore the technical details of these approaches, our focus remains practical ‚Äì understanding when and how to effectively apply these tools to build better intelligent advisory systems. By combining the intuitions developed through manual feature engineering with the power of automated representation learning, we can build graph-powered machine learning systems that are both scalable and interpretable.

11.1 Embeddings in graph representation learning

When we look at the history of graph representation learning, we can see a fascinating evolution that mirrors the broader development of machine learning itself. This field, which focuses on teaching computers to understand and work with graph-structured data, has grown through three distinct generations, each building upon the insights and achievements of its predecessors [1].

1. The first generation emerged from traditional mathematics and

computer science, focusing on what we now call traditional graph embedding. Imagine trying to take a complex three-dimensional object

and create a faithful two-dimensional drawing of it ‚Äì this was essentially the challenge these early researchers faced. They approached graphs through the lens of classical dimensionality reduction, trying to find ways to represent complex graph structures in simpler, more manageable forms while preserving their essential properties. 2. The second generation marked a revolutionary shift, sparked by

breakthroughs in natural language processing. Just as word2vec showed us how to capture the meaning of words in numerical vectors, Node2Vec demonstrated that we could do the same with nodes in a graph. This wasn't just a technical advance ‚Äì it represented a fundamental shift in how we thought about graph representation. Rather than just reducing complexity, we were now capturing meaningful relationships and patterns in our numerical representations. 3. The third and current generation has embraced the power of deep

learning, particularly through GNNs. Think of how deep learning transformed image recognition by automatically learning to identify features from raw pixels ‚Äì GNNs do something similar for graphs, automatically learning to understand complex patterns of relationships and interactions. This has opened entirely new possibilities, from improving recommendation systems to advancing drug discovery.

To truly harness the power of graph embeddings in real-world applications, we need to understand their fundamental characteristics and how these properties influence their performance in different scenarios. In the following sections, we'll explore the key dimensions that define graph embeddings - from the geometric spaces they occupy to how they capture local and global graph properties. While this exploration might seem theoretical at first, it provides essential insights for practitioners. Understanding these characteristics helps us make informed decisions about which embedding approach best suits our specific needs, whether we're building a recommendation system that needs to capture subtle user preferences or a drug discovery platform that must understand complex molecular interactions. Just as a carpenter needs to understand the properties of different woods to choose the right material for a project, we need to understand the properties of different embedding approaches to select the right tool for our graph-based applications. This knowledge becomes particularly valuable when we face challenging decisions, such as whether to prioritize capturing

local patterns or global structure, or whether to use standard Euclidean space or explore alternative geometric representations.

11.1.1 Understanding graph embeddings: From discrete to continuous

To truly understand graph embeddings, let's start with a simple analogy. Imagine you're trying to describe the layout of a city to someone who's never been there. You could list every street and intersection (like a discrete graph representation made by nodes and relationships), or you could draw a map with coordinates for each location (like a continuous embedding made by numbers). The map, while not capturing every detail, makes it much easier to understand relationships between locations and do useful calculations about the distance between points [2]. Figure 11.1 depicts the node embedding problem.

Figure 11.1 The image depicts the node embedding problem. The goal of the task is to learn an encoding function, encode(x), which maps nodes to multi-dimensional embedding space. These embeddings are optimized so that distances in the embedding space reflect some relevant aspects of the original graph, like the relative positions of the nodes.

This is exactly what graph embeddings do ‚Äì transform discrete graph structures into continuous vector representations. But why is this transformation so valuable? Let's explore this through the different aspects of

embedding approaches.

The geometry of embeddings: Beyond flat spaces

To understand why geometric spaces matter in graph embeddings, we need first to recognize a fundamental challenge in machine learning: different types of data have different inherent structures. When we work with images or text, we deal with data with a regular, predictable structure. Every pixel in an image has eight neighbors, and every word in a text has exactly one word before and after it. This regularity allows us to use well-established techniques like convolutional neural networks for images or recurrent neural networks for text [3]. Figure 11.2 shows the difference between an Euclidean and an non-euclidean space.

Figure 11.2 An illustration of Euclidean vs. non-Euclidean space [3]. Pixels in an image are all equally distant from each other and have all eight neighbors (except those on the border). In a graph, the number of neighbors can vary enormously from one node to the other.

However, graphs present a unique challenge. In a graph, each node might have a completely different number of connections and neighborhood structure than its neighbors. This irregularity means we can't directly apply the same techniques we use for regular structured data. We need approaches that can handle this variability, which brings us to the question of geometric spaces.

Most machine learning models, including many graph embedding approaches, operate in Euclidean space. This is the familiar geometric space we learn about in school, where distances are measured with straight lines and the Pythagorean theorem holds. Euclidean space is particularly effective for data where relationships are uniform and don't change based on position or scale. When we embed something in Euclidean space, we represent it as a vector of coordinates, and the distance between two points is calculated using

the standard Euclidean distance formula: the square root of the sum of squared differences between coordinates.

Researchers have discovered that some types of graph structures, particularly hierarchical ones, can be represented more effectively in non-Euclidean spaces, especially hyperbolic spaces. The key difference lies in how space itself behaves. Figure 11.3 shows how Euclidean space grows polynomially as we increase the number of dimensions.

Figure 11.3 Euclidean space grows polynomially as we increase the number of dimensions. Moving from 2D to 3D to 4D and beyond, the available space increases according to a polynomial relationship with the dimension. For example, a unit square in 2D has an area of 1, a unit cube in 3D has a volume of 1, and a unit hypercube in 4D has a hypervolume of 1. Still, the total space available along each axis increases polynomially rather than exponentially.

In Euclidean space, the amount of space available grows polynomially as you move outward from a center point ‚Äì imagine drawing larger and larger circles on a flat piece of paper. In hyperbolic space, the available space grows exponentially as you move outward ‚Äì like a tree that branches out more and more at each level.

Consider these practical implications for embeddings. In Euclidean space:

Vectors are straightforward coordinate points, like (0.5, 0.3)

Distances grow linearly with coordinate differences The space between points remains constant regardless of their position Most machine learning models assume this space, making it the default choice for many applications

In hyperbolic space:

Vectors are still represented as coordinates, but they behave differently Distances grow exponentially as points approach the boundary of the space The same coordinate difference represents larger distances near the boundary Special distance formulas must be used to calculate similarities between points

The choice of geometric space becomes particularly important when we know our graph has certain structural properties. For instance, many real- world graphs exhibit hierarchical structure ‚Äì think of corporate organizations, biological taxonomies, or internet network topology. In these cases, hyperbolic embeddings might preserve the graph's structure more faithfully than Euclidean embeddings.

It's crucial to understand that the Euclidean space works well for many applications. We should consider non-Euclidean embeddings only when we have strong evidence that our graph structure would benefit from their properties. When we do use non-Euclidean embeddings, we need to be mindful of how this affects our entire pipeline ‚Äì from how distances are calculated to how we visualize and interpret the results. This geometric perspective is part of a broader field called Geometric Deep Learning[1] (GDL), which studies how to apply deep learning techniques to data with non-standard geometric structures. Understanding these geometric spaces helps us choose the right tools for our specific graph problems and interpret the results appropriately.

Local vs Global: Understanding positional and structural embeddings

The distinction between positional and structural embeddings reflects a

fundamental question in graph analysis: what aspects of the graph structure are most important to preserve? Let's explore this through an example of a social network [3].

Positional embeddings are like preserving the "absolute" positions of people in a social network. If we think of a friend group, positional embeddings would maintain information about who is central to the entire network, who bridges different communities, and how many steps it takes to get from one person to another. These embeddings use techniques like matrix factorization and random walks to capture these global properties. Structural embeddings, on the other hand, focus on the "relative" positions or local patterns. Two people might be far apart in the network, but if they have similar friendship patterns (like both being the organizers of their respective friend groups), structural embeddings would represent them similarly. This is where GNNs excel, as they can learn to recognize and preserve these local patterns of connectivity.

Historically, these two approaches to graph embeddings have served different purposes, reflecting their inherent strengths. Positional embeddings, with their focus on preserving global graph structure, have proven particularly valuable for unsupervised tasks where the overall network topology matters most. Classic examples include link prediction (where understanding the global connection patterns helps predict missing edges) and clustering (where knowing a node's position in the broader network helps identify communities).

On the other hand, structural embeddings, particularly those generated by GNNs, have excelled in supervised tasks. Their ability to capture local neighborhood patterns makes them especially effective for node classification (where similar local structures often indicate similar labels) and whole graph classification (where local patterns can define graph-level properties).

Recent advances in the field have begun to blur these traditional boundaries. Researchers have developed sophisticated approaches that bridge the gap between positional and structural representations. New architectures like positional GNNs have emerged, incorporating global positional information into the typically local-focused GNN framework. Furthermore, theoretical

work has revealed deeper connections between these two perspectives, suggesting that positional and structural embeddings might be two sides of the same coin, capturing complementary aspects of graph structure. This convergence suggests a promising direction for future research: embedding methods that can adaptively combine both global and local information based on the specific requirements of the task at hand.

Learning strategies: Transductive vs Inductive approaches

The choice between transductive and inductive learning approaches reflects a crucial question in machine learning: how should our models handle new, unseen data? This distinction is particularly important in graph learning, where new nodes and edges frequently appear [3].

Transductive learning is like learning to solve a specific puzzle ‚Äì you can get very good at it, but your knowledge might not transfer to a different puzzle. In graph terms, transductive approaches learn embeddings for a fixed set of nodes, optimizing them directly for the known graph structure. This works well for static graphs where you don't expect new nodes to appear. Transductive methods allow us to infer new information between nodes analyzed during training. For example, when working with partially labeled nodes, we can classify unlabeled nodes based on the patterns learned from the known structure. Similarly, we can predict new edges between graph nodes that were observed during the training process. Inductive learning, by contrast, is like learning the general strategies for solving puzzles. Instead of memorizing specific patterns, inductive approaches learn rules that can be applied to new situations. This makes them crucial for dynamic graphs, such as social networks where new users join regularly or protein interaction networks where new proteins are discovered. Node features play a crucial role in inductive graph representation learning methods. Unlike transductive approaches, these features are employed to learn embeddings with parametric mappings. The learning goal is achieved by optimizing such parametric mappings instead of directly optimizing the embeddings. This means the learning mappings can be applied to any node, even those that were not seen during the training process.

To better understand the difference, let‚Äôs consider a recommendation system for an e-commerce platform. A transductive approach works well for recommending products to existing users based on their past interactions. However, when a new user joins the platform, you need an inductive approach that can generate meaningful embeddings for this new user based on their initial interactions and profile information.

The role of supervision in learning embeddings

The distinction between supervised and unsupervised learning in graph embeddings reflects different levels of available information and different goals for the embedding process.

Unsupervised learning in graph embeddings is like trying to understand the social dynamics of a group just by watching who talks to whom, without knowing anything about the content of their conversations. The goal is to discover natural patterns and structures in the graph data itself. This approach relies on the assumption that the graph structure alone contains valuable information ‚Äì which is often true! For example, in a citation network, papers that frequently cite the same sources are likely related, even if we don't know their content. Supervised learning, on the other hand, is like having additional context about the interactions we observe. Maybe we know that certain connections represent successful business deals while others represent failed negotiations. This additional information helps guide the learning process toward specific goals. In a protein interaction network, this might mean using known protein functions to help predict the functions of other proteins based on their interaction patterns.

11.1.2 Real-World applications and examples

Let's explore how these different aspects of graph embeddings come together in real-world applications:

Social Network Analysis: Consider a large social network like LinkedIn. Here, structural embeddings help identify users with similar professional roles across different industries, while positional embeddings help

understand who the key influencers and bridge-builders are in the network. The platform needs inductive learning capabilities to handle new users and might use supervised learning with job titles and skills to improve recommendation accuracy. Biological Networks: In protein interaction networks, the choice of geometry becomes crucial. The complex hierarchical organization of biological systems often benefits from hyperbolic embeddings. The need to constantly incorporate new discoveries makes inductive learning essential, while the availability of experimental data enables supervised learning approaches to improve prediction accuracy. Medical Knowledge Graphs: Consider a knowledge graph representing medical knowledge. The hierarchical nature of medical terminology (from broad categories to specific conditions) makes hyperbolic embeddings particularly appropriate. Structural embeddings help identify similar concepts across different branches of medicine, while supervised learning with expert-labeled data helps ensure accurate relationships between concepts.

Through these examples, we can see how different embedding approaches combine to solve real-world problems. The choice of embedding strategy isn't just a technical decision ‚Äì it reflects our understanding of the problem structure and our goals for the solution.

11.2 The encoder-decoder model

The encoder-decoder model provides a powerful and unified way to understand how graph embedding methods work. At its core, this model breaks down the complex task of learning graph representations into two key operations: encoding graph information into compact representations and decoding those representations to reconstruct important graph properties [2]. Figure 11.4 presents an overview of the encode/decode model for nodes.

Figure 11.4 Overview of the encoder-decoder approach. The encoder maps the node c to a low- dimensional embedding zc. The decoder then uses zc to reconstruct c's local neighborhood information.

Think of the encoder-decoder model like a translation system ‚Äì first, it converts (encodes) information from one form into a more compact, standardized format, and then it translates (decodes) that information back to reconstruct meaningful patterns. In the context of graphs, this means transforming the discrete graph structure into continuous vector representations that preserve the important characteristics of the original graph.

11.2.1 The Encoder: Converting graph structure to vectors

The encoder takes the raw graph data - its structure and any additional

features - and converts it into dense vector representations (embeddings) for each node. More formally, the encoder takes two key inputs:

The graph structure represented as an adjacency matrix that shows how nodes are connected Node features, which provide additional information about each node's characteristics

What makes the encoder interesting is its flexibility - different methods can encode different aspects of the graph. Some focus on preserving the global structure and relative positions of nodes, while others prioritize capturing local neighborhood patterns. The encoder can be as simple as a lookup table that assigns each node a vector, or as sophisticated as a neural network that processes both structure and features.

11.2.2 The Decoder: Reconstructing graph properties

The decoder takes the embeddings created by the encoder and attempts to reconstruct important properties of the original graph. This reconstruction provides a way to ensure the embeddings have captured meaningful information. The decoder's task varies depending on what aspects of the graph we want to preserve:

For methods focused on graph structure, the decoder might try to predict whether nodes are connected in the original graph For capturing node similarity, it might reconstruct measures of neighborhood overlap between nodes In cases with additional supervision, the decoder could predict node labels or other properties

This reconstruction task serves as a training signal - by trying to minimize the difference between the decoder's predictions and the actual graph properties, we can optimize the encoder to learn better representations.

11.2.3 The power of the framework

What makes the encoder-decoder framework particularly valuable is its

ability to unify many different approaches to graph representation learning. Whether we're looking at matrix factorization methods, random walk-based approaches, or Graph Neural Networks, they can all be understood as different ways of implementing this basic encode-decode pattern. The framework also helps us understand the tradeoffs involved in different methods. For instance, simpler encoders might be more computationally efficient but capture less complex patterns, while more sophisticated neural network-based encoders can learn richer representations but require more data and computation to train effectively.

By breaking down graph embedding methods into these encoding and decoding components, we gain:

A clearer understanding of how different methods relate to each other Better insights into what types of graph properties each method can capture A structured way to think about designing new embedding approaches

This encoding-decoding perspective ultimately provides a lens through which we can analyze, compare, and improve methods for learning representations of graph-structured data.

11.2.4 Node2vec: an example of encoder-decoder framework

Consider the famous Karate Club network, where nodes represent club members and edges represent friendships between them. The club eventually split into two groups following a dispute between the instructor (node 1) and the club president (node 34). This makes it a perfect example to understand how node2vec [4] works within the encoder-decoder framework.

The encoding process in node2vec

First, node2vec's encoder needs to transform each club member (node) into a numerical vector that captures their position and role in the friendship network. Here's how it does this:

Random Walk Generation: The encoder starts by taking "walks" through

the friendship network. Let's say we start at the instructor (node 1). A walk might look like: 1 ‚Üí 2 ‚Üí 3 ‚Üí 4, representing a path through friends of friends. node2vec is clever about these walks - it can adjust how they explore the network by balancing between:

Going broad: Exploring different social circles in the club Going deep: Staying within tight-knit groups of friends Vector Creation: Based on these walks, the encoder creates a vector for each member that captures their social context. Members who frequently appear near each other in these walks (close friends or in the same social circles) will get similar vector representations.

The decoding process

The decoder's job is to take these vectors and reconstruct information about the friendship network. For any two members i and j, the decoder tries to predict how likely they are to be friends based on their vector representations. It does this using what's called the softmax function, which essentially asks: "Given member i's vector, how likely are we to see member j nearby in our random walks?"

Real-World Insights:

This encoding-decoding process reveals interesting patterns in the Karate Club network:

1. Community Structure:

Members who ended up in the same faction after the split (either with the instructor or the president) tend to get similar vector representations The vectors capture not just direct friendships but also broader community affiliations 2. Bridge Members:

Members who had friends in both factions (like node 3) get vector representations that reflect their intermediate position These vectors help identify potential mediators in the conflict 3. Leadership Roles:

The instructor (node 1) and president (node 34) get distinct vector

representations that reflect their central but opposing positions in the network Their vectors capture their roles as community leaders through their connection patterns

The Power of the Framework

The encoder-decoder framework helps us understand why node2vec works:

The encoder's random walks capture both local friendships and broader social structures The decoder's prediction task ensures the vectors preserve meaningful social relationships Together, they create representations that can predict not just friendships but also eventual faction membership

This example shows how the encoder-decoder framework isn't just theoretical - it helps us understand and improve real-world network analysis. In the Karate Club case, it transforms a complex social network into vector representations that capture subtle social dynamics, from direct friendships to community structures to leadership roles. Listing 11.1 shows how to apply node2vec embedding to the Karate Club‚Äôs Network.

Listing 11.1 Applying node2vec embedding to Karate Club‚Äôs Network

import networkx as nx import numpy as np from node2vec import Node2Vec import matplotlib.pyplot as plt from sklearn.manifold import TSNE

G = nx.karate_club_graph() #A

faction_labels = {

0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 1, 10: 1, 11: 0, 12: 0, 13: 0, 14: 1, 15: 1, 16: 0, 17: 0, 18: 1, 19: 0, 20: 1, 21: 0, 22: 1, 23: 1, 24: 1, 25: 1, 26: 1, 27: 1, 28: 1, 29: 1, 30: 1, 31: 1, 32: 1, 33: 1 } #B nx.set_node_attributes(G, faction_labels, 'faction') #C

node2vec = Node2Vec(

G, dimensions=16, walk_length=10, num_walks=20, p=1, q=1, workers=4 ) #D

model = node2vec.fit(window=5) #E

def decode_similarity(model, node1, node2):

return model.wv.similarity(str(node1), str(node2)) #F

def visualize_graph_and_embeddings(model, G):

embeddings = np.zeros((len(G.nodes()), model.vector_size)) for i, node in enumerate(G.nodes()): embeddings[i] = model.wv[str(node)]

tsne = TSNE(n_components=2, random_state=42) node_pos_2d = tsne.fit_transform(embeddings)

fig, axes = plt.subplots(1, 2, figsize=(18, 8))

# Left plot: original graph pos = nx.spring_layout(G, seed=42) colors = ['red' if G.nodes[node]['faction'] == 0 else 'blue' for node in G.nodes()] nx.draw( G, pos, ax=axes[0], with_labels=True, node_color=colors, node_size=500, font_size=10 ) axes[0].set_title("Original Karate Club Graph")

# Right plot: node embeddings axes[1].scatter(node_pos_2d[:, 0], node_pos_2d[:, 1], c=color for i, node in enumerate(G.nodes()): axes[1].annotate(str(node), (node_pos_2d[i, 0], node_pos_ axes[1].set_title("Node2Vec Embeddings (t-SNE)")

plt.tight_layout() plt.show() #G

print("Similarity between instructor (0) and their close ally (1)

decode_similarity(model, 0, 1)) print("Similarity between instructor (0) and president (33):",

decode_similarity(model, 0, 33)) #H

visualize_graph_and_embeddings(model, G) #I

def analyze_community_structure(model, G):

instructor_allies = [] president_allies = [] for node in G.nodes(): if node not in [0, 33]: sim_to_instructor = decode_similarity(model, node, 0) sim_to_president = decode_similarity(model, node, 33) if sim_to_instructor > sim_to_president: instructor_allies.append(node) else: president_allies.append(node) return instructor_allies, president_allies #J

instructor_group, president_group = analyze_community_structure(m print("\nPredicted instructor's faction:", sorted(instructor_grou print("Predicted president's faction:", sorted(president_group))

This implementation demonstrates several key aspects of the encoder-decoder framework we discussed:

The Encoder:

Implements random walks through the Karate Club network using node2vec Configures walk parameters (p and q) to balance between local and global exploration Transforms the discrete graph structure into continuous vector embeddings The Decoder:

Provides functions to decode the embeddings back into meaningful graph properties

Can predict node similarities using the learned vector representations Visualizes the embeddings to show how they capture the network's community structure Analysis:

Shows how the embeddings capture both local friendship patterns and the broader faction split Demonstrates that nodes in the same faction end up with similar vector representations Reveals how central figures (instructor and president) get distinct embeddings that reflect their roles

The visualization will show how the encoded vectors naturally cluster nodes by faction, while the similarity calculations demonstrate how the decoded embeddings can predict relationships in the original graph. You can see how members close to the instructor (node 0) have different embedding patterns than those close to the president (node 33), reflecting the real social dynamics that led to the club's split.

11.3 Shallow embeddings: a first approach to graph representation

The simplest way to understand graph embeddings is through what we call "shallow embeddings." These represent the most straightforward implementation of the encoder-decoder framework, but their simplicity comes with important limitations that have driven the development of more sophisticated approaches [2].

11.3.1 Understanding shallow embeddings

In shallow embedding methods, the encoder performs a remarkably simple operation: it acts as a lookup table, directly mapping each node to its vector representation. Think of it like assigning each person in a social network their own unique "coordinate" in a high-dimensional space. This is similar to how a dictionary works - each word (node) has its own dedicated entry (vector).

Within the encoder-decoder framework, this approach can be understood as follows:

The encoder simply maintains a matrix where each row corresponds to a node's embedding vector. When asked to encode a node, it merely returns the corresponding row from this matrix. This is why we call these embeddings "shallow" - there's no deep processing or transformation of the input, just a direct lookup operation. The decoder then takes these embedding vectors and tries to reconstruct important properties of the graph, such as whether two nodes are connected or how similar their neighborhoods are. This reconstruction process helps optimize the embedding vectors during training to capture meaningful patterns in the graph structure.

Let's consider a concrete example: In the Karate Club network we discussed earlier, a shallow embedding approach would create a separate vector for each club member. These vectors would be adjusted during training so that members who interact frequently end up with similar vectors, while those who rarely interact end up with dissimilar vectors.

11.3.2 Limitations of shallow embeddings

While shallow embeddings have achieved considerable success in many applications, they face several important limitations:

Parameter Inefficiency: The most obvious limitation is that shallow embeddings require a separate vector for each node in the graph. This means the number of parameters grows linearly with the number of nodes, making them impractical for very large graphs. For instance, in a social network with millions of users, we would need millions of embedding vectors. No Parameter Sharing: Shallow embeddings don't share any parameters between nodes. This means they can't leverage common patterns or structures that might appear in different parts of the graph. In our Karate Club example, if two members play similar roles in different social circles, the model has to learn these patterns independently for each member rather than recognizing and reusing the pattern.

Feature Blindness: These methods typically don't take advantage of node features or attributes. If we knew additional information about each club member (like age, interests, or role), shallow embeddings wouldn't have a natural way to incorporate this information into the representations. Transductive Nature: Perhaps the most significant limitation is that shallow embeddings are inherently transductive - they can only generate embeddings for nodes that were present during training. If a new member joins the club, we can't automatically generate an embedding for them because they weren't part of the original lookup table. We would need to retrain the entire model to incorporate new nodes.

These limitations have motivated the development of more sophisticated approaches, particularly Graph Neural Networks, which we'll explore in section 11.6. These advanced methods address many of these limitations by learning to generate embeddings based on both graph structure and node features, enabling inductive learning and better parameter sharing.

Despite their limitations, shallow embeddings remain important both historically and practically. They help us understand the fundamental challenges of graph representation learning and provide a baseline against which we can compare more sophisticated approaches. Their simplicity also makes them useful in situations where the graph is relatively small and static, and when computational resources are limited.

11.4 Embeddings in knowledge graphs

In our previous sections, we explored how to learn embeddings for simple graphs where edges represent a single type of relationship between nodes. However, real-world graphs often have much richer structures, in particular, those we are addressing in this book for implementing an Intelligent Advisor System. Think of a biomedical knowledge graph where nodes might represent drugs, diseases, genes, and proteins, with dozens of different types of relationships connecting them - "treats", "causes", "interacts_with", and so on. These multi-relational graphs require more sophisticated approaches to embedding.

In this section, we'll extend our discussion of shallow embeddings to handle these complex knowledge graphs. While we're still working with the basic concept of learning a unique embedding vector for each node, we'll need to introduce new techniques to handle the multiple types of relationships. This is particularly important for tasks like knowledge graph completion, where we aim to predict missing relationships between entities.

Let's understand the challenge with a concrete example. Consider a biomedical knowledge graph where we have:

Entities: Aspirin, Inflammation, Headache, COX-2 enzyme Relationships: TREATS, INHIBITS, CAUSES

In a simple graph, we might just care about whether entities are connected. However, in a knowledge graph, the type of connection matters significantly. We need to be able to capture that:

(Aspirin, TREATS, Headache) (Aspirin, INHIBITS, COX-2) (Inflammation, CAUSES, Headache)

Our embedding approach needs to encode not just whether entities are related, but how they are related.

When working with graph embeddings, particularly in knowledge graphs, we need two key components to effectively learn meaningful representations: a way to measure how well our embeddings capture the graph structure (the loss function) and a way to handle different types of relationships between nodes (the multi-relational decoder). Let's explore how these components work together to create effective graph representations.

11.4.1 Loss function

Think of a loss function as a teacher grading a student's work. Just as a teacher needs clear criteria to evaluate performance, we need well-defined ways to measure how well our embeddings capture the graph's structure. However, designing an effective loss function for graph embeddings presents some unique challenges.

The simplest approach might seem to be directly comparing our predicted connections with the actual graph structure using something like mean squared error. However, this approach faces two major practical limitations:

Computational Efficiency: In a graph with millions of nodes, comparing every possible pair of nodes becomes computationally impossible. For example, in a social network with 1 million users, we'd need to evaluate nearly a trillion potential connections. Sparsity: Most real-world graphs are sparse, meaning most nodes aren't connected to each other. A social media user might have hundreds of friends, but this is tiny compared to the millions of total users. Our loss function needs to handle this imbalance effectively.

To address these challenges, modern approaches typically use what's called "negative sampling" with cross-entropy loss. Let's break this down with an example:

Imagine you're building a book recommendation system. For each book a user has read (a positive example), you sample a few books they haven't read (negative examples). Instead of evaluating all possible books, you only need to look at a small, manageable set of comparisons. This approach is both computationally efficient and helps handle the sparsity problem.

Let's explore how this works with one of the most common approaches: the cross-entropy loss with negative sampling.

Here's a concrete example of how this loss function works:

Let's understand each element:

Basic Components:

‚Ñí: The total loss we're trying to minimize Œ£: Sum over all existing edges in our knowledge graph

## 12_Node_classification_and_link_prediction_with_GNNs

This chapter covers

Understanding how to use Graph Neural Networks (GNNs) in real scenarios. Building a node classification system based on GNNs for anti-money laundering. Building a link prediction system based on GNNs for movie recommendation and rating.

This chapter explores the application of Graph Neural Networks (GNNs) in two key tasks: node classification and link prediction. These tasks represent fundamental challenges in graph-based machine learning and are central to many real-world applications.

The first part of this chapter explores the application of GNNs for node classification, with a focus on anti-money laundering (AML) applications. By representing financial transactions as a graph, GNNs can be employed to identify suspicious patterns, classify nodes as licit or illicit, and aid in combating financial fraud.

The second part focuses on link prediction in recommendation systems. In this scenario, we will use a GNN-based approach to predict potential user- movie interactions based on the ratings. By learning embeddings for users and movies and leveraging the links between them, we aim to recommend movies to users based on their preferences.

Despite the differences in tasks and application domains, both scenarios can be addressed by adopting the same end-to-end framework, illustrated in the mental model shown in figure 12.1. This framework comprises multiple steps, from processing the input data as a graph to producing a trained model for the inference phase.

Figure 12.1 End-to-end framework to develop a GNN-based system that can be adopted for multiple tasks and application domains. The starting point is represented by the input data, which is usually a collection of semi-structured sources describing different objects and the interactions between these objects. The goal is to process such data to produce a graph that becomes the input for an encoder/decoder architecture. In our scenarios, the encoder employs a GNN model, and the decoder is based on a function that allows us to optimize the model for a specific downstream task. The output of this framework is a trained model that can be adopted for the inference phase.

Throughout the chapter, we compare the performance of different GNN architectures: Graph Convolutional Networks (GCN) [1], Graph Attention Networks (GAT) [3], and GraphSAGE (SAGE) [2]. These models are developed using the Pytorch Geometric (PyG) library and are evaluated using

metrics such as precision, recall, and F1-score to highlight their strengths and weaknesses in each task. Insights from confusion matrices and visualizations of metric trends further illustrate the trade-offs and capabilities of these approaches in real-world scenarios.

By the end of this chapter, you will have a comprehensive understanding of how to apply GNNs to solve node classification and link prediction tasks, equipping you with the tools to leverage graph-based machine learning for diverse applications.

12.1 Node Classification for Anti-money Laundering

Node classification is a critical task in graph-based machine learning, which is suitable for different application domains, including combating financial crimes. In the context of anti-money laundering (AML), financial transactions can be modeled as graphs, where nodes represent accounts or entities and edges represent transaction relationships.

This section will focus on using GNNs to detect licit and illicit nodes within financial transaction networks provided by the Elliptic dataset. Figure 12.2 shows the end-to-end model for applying GNNs in node classification, where each block is characterized by a specific implementation to satisfy the requirements of our task.

Figure 12.2 This diagram illustrates the end-to-end framework tailored for node classification in the context of AML. Transaction data and node labels are transformed into a homogeneous graph structure. The encoder, a homogeneous GNN, learns node embeddings by capturing its local graph structure. The decoder classifies nodes as licit or illicit by leveraging a log softmax function and cross-entropy loss, producing a trained model for detecting suspicious activities.

The following subsections explore each component of this end-to-end model, from data understanding to generating a helpful model for AML.

12.1.1 Input Data

We will leverage the Elliptic dataset to explore the potential of GNNs in detecting licit and illicit nodes within financial transaction networks.

The Elliptic dataset is a time-series graph comprising over 200,000 Bitcoin transactions (nodes), 234,000 directed payment flows (edges), and 166 node features derived from anonymized data.

The dataset is currently hosted on Kaggle, and it is provided across three distinct files. As of January 2025, these files provide the following information:

elliptic_txs_features.csv: A 167-column file with 203,769 rows. The first column lists all node IDs, while the remaining columns represent anonymized features associated with each node. Data from this file is stored in the features variable. elliptic_txs_edgelist.csv: A 2-column file containing 234,355 rows corresponding to the number of edges in the network. The first column lists node IDs representing transaction sources, while the second column lists node IDs representing transaction targets. Data from this file is stored in the edges variable. elliptic_txs_classes.csv: A 2-column file containing 203,769 rows corresponding to the number of nodes in the network. The first column lists all node IDs, while the second column specifies the label associated with each node: 1, 2, or unknown. Data of this file is stored in the classes variable.

In this dataset, the classes defining licit, illicit, and unknown nodes are distributed as reported in table 12.1:

Table 12.1 Distribution of licit, illicit, and unknown classes associated with nodes.

Class Labels Counts Percentage unknown unknown 157205 77.15% Licit 2 42019 20.62% Illicit 1 4545 2.23%

The next step is to pre-process these CSV files to assign a more compact numerical representation to our data and then create different structures to make our financial transactions graph suitable for the GNN model.

12.1.2 Graph Processor – Data Preparation

The preprocessing phase is crucial to achieving a compact numerical representation of the original data and preparing our financial transaction

graph structure for the GNN learning phase. Let’s start with processing information in edges. A sample of the original data is available in table 12.2:

Table 12.2 Sample of data stored in edges variable showing transactions between the source nodes (txId1 column) and the target nodes (txId2 column)

txId1 txId2 230425980 5530458 232022460 232438397 230460314 230459870 230333930 230595899 232013274 232029206

The txId1 column in table 12.2 lists the IDs of the source nodes, while txId2 lists the IDs of the target nodes. The code in listing 12.1 assigns each node a new incremental ID and creates an updated version of the edge data using these IDs.

Listing 12.1 Assign a new incremental ID to the nodes and update the edges data

tx_id_mapping = {tx_id: idx for idx, tx_id in enumerate(features[ edges_with_features = edges[edges['txId1'].isin(list(tx_id_mappin

& edges['txId2'].isin(list(tx_id_mapping.keys()))] #B edges_with_features['Id1'] = edges_with_features['txId1'].map(tx_ edges_with_features['Id2'] = edges_with_features['txId2'].map(tx_

A sample of the output of this process stored in the edges_with_features variables is available in table 12.3:

Table 12.3 Edge data with the new incremental IDs associated with each node involved in a transaction

txId1 txId2 Id1 Id2 230425980 5530458 0 1 232022460 232438397 2 3 230460314 230459870 4 5 230333930 230595899 6 7 232013274 232029206 8 9

Starting from the new IDs, we can create an edge_index, a tensor describing the edges within the graph. Listing 12.2 shows how to create this structure.

Listing 12.2 Create an edge_index tensor using the incremental node IDs

edge_index = torch.tensor(edges_with_features[['Id1', 'Id2']].val

A sample of the edge_index tensor is available in listing 12.3.

Listing 12.3 Sample of the edge_index data

tensor([[ 0, 2, 4, ..., 201921, 201480, 201954], [ 1, 3, 5, ...,

Then, let’s process the original node features data structure by creating a new tensor, where each row corresponds to the incremental ID generated in the previous step. This step allows us to prepare the node features for the learning phase by dropping the original node IDs. Listing 12.4 shows how to perform this operation.

Listing 12.4 Create a node feature tensor by dropping the original node ID

node_features = torch.tensor(features.drop(columns=['txId']).valu

The output is stored in the node_features tensor with size [203769, 166]. The first dimension of this tensor corresponds to the number of nodes, and the second dimension corresponds to the number of features.

As we mentioned, the index of each row of this matrix corresponds to the incremental IDs created in the previous step. In other words, the 0th row of the node_features tensor corresponds to the set of features associated with node 0, whose original ID is 230425980.

The final step is to transform the original labels in the dataset into a numerical representation. To obtain this result, we will use the LabelEncoder class of the scikit-learn library, as reported in Listing 12.5.

Listing 12.5 Transform original node classes into a numerical representation

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder() #A class_labels = le.fit_transform(classes['class']) #B original_labels = le.inverse_transform(class_labels) #C node_labels = torch.tensor(class_labels, dtype=torch.long) #D

The output is a new node_labels tensor with size 203769, where label 0 corresponds to the licit nodes (”1” in the original datasets), label 1 corresponds to the illicit nodes (”2” in the original dataset), and label 2 corresponds to unknown nodes (”unknown” in the original dataset). The following subsection describes how to build our graph data as input for GNN models using PyTorch Geometric (PyG).

12.1.3 Graph Processor – Homogenous PyG Graph

We can now build our graph data structure using the PyG features. The first step is to leverage the result of the preprocessing phase (node_features, edge_index, node_labels) to create a PyG Data object, as shown in listing 12.6.

Listing 12.6 Create a PyG Data object from the result of the data preparation phase

from torch_geometric.data import Data data = Data(x=node_features,

edge_index=edge_index, y=node_labels)

The PyG Data object incorporates the features of the nodes, the information on the edges, and the label associated with each node. The next step is to apply a mask to this object, which specifies the nodes to be used during the training and evaluation phases.

Considering that our GNN models need to learn which nodes are licit and which are illicit, we need to “make visible” only the nodes with known labels to our model. To reach this goal, we can run the code available in listing 12.7.

Listing 12.7 Prepare mask filter to retrieve only known node labels

known_mask = (data.y == 0) | (data.y == 1) #A unknown_mask = data.y == 2 #B

The tensor of valid nodes (known_mask) is used to define the dimensions of the training, validation, and testing sets. Such dimension is specified using the code in listing 12.8:

Listing 12.8 Define the dimension of the training, validation, and testing datasets

import numpy as np num_known_nodes = known_mask.sum().item() #A permutations = torch.randperm(num_known_nodes) #B train_size = int(0.8 * num_known_nodes) #C val_size = int(0.1 * num_known_nodes) #D test_size = num_known_nodes - train_size - val_size #E total = np.sum([train_size, val_size, test_size]) #F

Using the dimension of the results stored in the total variable, we determine the number of observations—that is, the number of labeled nodes to be included in the training, validation, or testing datasets. The results are summarized in table 12.4.

Table 12.4 Dimension of the training, validation, and testing datasets based on the number of labeled nodes

Dataset Number of Nodes Percentage Training 37,251 80.00% Validation 4,656 10.00% Testing 4,657 10.00%

By leveraging the code shown in listing 12.9, we create the training, validation, and testing index masks on the PyG Data object based on the dimensions established in the previous step.

Listing 12.9 Training, validation, and testing masks on the PyG Data object

data.train_mask = torch.zeros(data.num_nodes, dtype=torch.bool) #A data.val_mask = torch.zeros(data.num_nodes, dtype=torch.bool) #B data.test_mask = torch.zeros(data.num_nodes, dtype=torch.bool) #C train_indices = known_mask.nonzero(as_tuple=True)[0][permutations val_indices = known_mask.nonzero(as_tuple=True)[0][permutations[t test_indices = known_mask.nonzero(as_tuple=True)[0][permutations[ data.train_mask[train_indices] = True #G data.val_mask[val_indices] = True #H

data.test_mask[test_indices] = True #I

Table 12.5 shows the dataset's statistics to clarify how the data has been split for training and evaluation. In this scenario, we can directly check the balance of licit and illicit information within each dataset.

Table 12.5 Training, validation, and testing dataset size and class distribution for each dataset.

Dataset Total Count Licit Licit (%) Illicit Illicit (%) Training 37,251 33,645 90.32 3,606 9.78 Validation 4,656 4,193 90.06 463 9.88 Testing 4,657 4,181 89.78 476 9.45

The splitting process for the classification task was conducted manually by defining multiple scripts collected in this section. In the section dedicated to graph processing in the context of link prediction (12.2.3), we will see that the PyG library provides some features to help us split the dataset. Let’s now discuss the end-to-end architecture to build our node classification system.

Figure 12.3 provides an overview of the steps applied by the Graph Processor, which involves a preparation phase, the construction of a PyTorch Geometric Data object, and the creation of training, validation, and testing datasets.

Figure 12.3 Overview of data processing applied to the Elliptic dataset. Source data are preprocessed to create node features, edge indices, and encoded labels. These tensors are incorporated into a PyTorch Geometric Data object, where a node masking approach partitions the nodes into training (80%), validation (10%), and testing (10%) sets.

12.1.4 Encoder-Decoder Architecture

Chapter 11 introduced the encoder-decoder architecture for training a GNN model on a downstream task. Listing 12.10 shows the implementation of a Python class to define an encoder-decoder for node classification.

Listing 12.10 Encoder-decoder implementation for node classification

import torch import torch.nn.functional as F

class NodeClassifier(torch.nn.Module):

def __init__(self, gnn_model): super().__init__() self.gnn = gnn_model

def forward(self, x, edge_index): x = self.gnn(x, edge_index) return F.log_softmax(x, dim=1)

The forward method of the NodeClassifier class includes an encoding phase performed by a GNN to update the node representation with information from its neighbors and a decoding phase executed by the log_softmax function provided by PyTorch, which produces a probability value associated with each node. Such probability value will be used to establish if a node is licit or illicit. Let’s see the implementation of these components in detail.

The Encoder

The encoder component employs a homogeneous GNN to perform the message-passing process (see chapter 11). This means that the encoder is designed to handle homogeneous graphs containing a single type of node and edge, such as those provided in financial transaction networks.

To compare the behavior of different GNN encoders, we define a base graph model (see listing 12.11) that specifies the shared characteristics of these encoders.

Listing 12.11 Base graph model to define a general GNN encoder

import torch import torch.nn.functional as F

class BaseGraphModel(torch.nn.Module):

def __init__(self, input_dim, hidden_dim,

out_dim, conv_layer, **conv_kwargs): #A

super(BaseGraphModel, self).__init__() #B self.conv1 = conv_layer(input_dim, hidden_dim, **conv_kwa self.conv2 = conv_layer(hidden_dim, out_dim, **conv_kwarg

def forward(self, x, edge_index): #E x = self.conv1(x, edge_index) #F x = F.relu(x) #G x = self.conv2(x, edge_index) #H return x #I

The base class in listing 12.11 shows a two-layer architecture in which any GNN implementation provided by PyG can be used for neighborhood aggregation and node update operations (convolution).

In this chapter, we will compare the performance of Graph Convolutional Networks (GCN), Graph Attention Networks (GAT), and GraphSAGE (SAGE) for node classification and link prediction tasks. Details about the implementation of these GNN models are covered in chapter 11. The **conv_kwargs argument defined in our base class allows us to pass extra parameters that may be required for specific GNN implementations. To grasp this idea, consider the implementation of our SAGE model provided in listing 12.12.

Listing 12.12 SAGE model implemented from the base class

from torch_geometric.nn import SAGEConv

class SAGE(BaseGraphModel):

def __init__(self, input_dim, hidden_dim, out_dim): super(SAGE, self).__init__( input_dim=input_dim, hidden_dim=hidden_dim, out_dim=out_dim, conv_layer=SAGEConv )

The SAGEConv layer from PyG does not require additional parameters, so initializing our SAGE model is straightforward. More specifically, the code

in listing 12.12 creates a GNN model with two SAGEConv layers.

Other GNN models require an extension of our base class. Consider the implementation of the GAT model shown in Listing 12.13.

Listing 12.13 GAT model implemented by extending the base class

from torch_geometric.nn import GATConv

class GAT(BaseGraphModel):

def __init__(self, input_dim, hidden_dim, out_dim, num_heads= concat_hidden_dim = hidden_dim * num_heads #B

conv1 = GATConv( #C in_channels=input_dim, out_channels=hidden_dim, heads=num_heads, add_self_loops=add_self_loops )

conv2 = GATConv( #D in_channels=concat_hidden_dim, out_channels=out_dim, heads=1, #E add_self_loops=add_self_loops, concat=False #F )

super(BaseGraphModel, self).__init__() #G self.conv1 = conv1 #H self.conv2 = conv2 #I

In this case, we must re-implement the convolutional layers to include the additional parameters required by the GATConv layer implemented in PyG.

The Decoder

The decoder component utilizes PyTorch's log_softmax function to transform the neural network's output—specifically, the output of the GNN encoder—into probability values. Unlike the standard softmax function, log_softmax improves the stability of the learning process by mitigating the effects of extremely small or large numbers during the probability

computation.

In this context, the log_softmax function works in tandem with PyTorch's CrossEntropyLoss function. This function combines the logarithm of softmax probabilities with the negative log-likelihood loss, providing a efficient approach to classification tasks. For example, in an anti-money laundering (AML) system, the GNN encoder might analyze transaction data to detect suspicious behavior, and each node could be assigned a probability for different categories, such as licit or illicit.

The log_softmax function ensures these probabilities are numerically stable and interpretable, while the CrossEntropyLoss function measures how well the model’s predictions match the actual labels. If the model predicts a high probability for illicit but the node is labeled as licit, the loss function identifies the error and optimize the model to improve accuracy. This robust combination is critical in scenarios like AML, where accurate classification has significant financial and legal implications.

Figure 12.4 provides an overview of the different steps within the encoder- decoder architecture.

Figure 12.4 Overview of the main steps of the encoder-decoder architecture. Homogeneous graph data, consisting of transactions (nodes) and Bitcoin flows (edges), is processed using three distinct GNN encoders: Graph Convolutional Network (GCN), Graph Attention Network (GAT), and GraphSAGE (SAGE). Each encoder generates unique node representations by aggregating neighbor features. Then, the Log Softmax function transforms these representations into probability values, which are used to classify transactions as licit or illicit.

12.1.5 Evaluation and Analysis

In our analysis, we compare three distinct end-to-end models for node classification. Each model employs a different GNN encoder: Graph Convolutional Networks (GCN), Graph Attention Networks (GAT), and GraphSAGE (SAGE). The results presented in this section were achieved after 400 epochs on a T4 Colab machine.

Table 12.6 shows the number of parameters and the total training time in seconds for each end-to-end model.

Table 12.6 Number of parameters and total training time for each encoder.

Encoder Parameters Train Time (s) GCN 2723 19.02 GAT 22025 43.45 SAGE 5427 36.71

The results presented in table 12.6 indicate that the GCN model is the most efficient, the GAT model is the least efficient, and the SAGE model falls between the two. As you can guess, the training time is directly connected to the number of parameters. In section 12.1.4, we clarified that the GAT model incorporates an attention mechanism that introduces learnable coefficients for the neighborhood edges that increase the number of learning parameters.

To explore the generalization capabilities of these models, we can evaluate their performance on precision, recall, and f1-score metrics. In the context of node classification for anti-money laundering, these metrics allow us to answer the following questions:

Precision: When the model says a node is licit or illicit, how often is it right? Recall: Out of all the existing licit and illicit nodes, how many did the model correctly find? F1-Score: How well does the model balance being accurate when it makes a prediction and finding as many correct nodes as possible?

Figure 12.5 illustrates a plot of GCN, GAT, and SAGE's performance across these metrics on the validation dataset during the training phase. The vertical axis represents the metric values, while the horizontal axis indicates the number of epochs.

Figure 12.5 Performance of GCN, GAT, and SAGE across precision, recall, and f1-score on the validation dataset during the training phase.

The results shown in Figure 12.5 were obtained using the built-in scoring functions provided by the sklearn library for precision, recall, and f1-score. These scoring functions provide the average parameter to manage imbalanced datasets, such as ours, where licit nodes are much more numerous than illicit ones. We decided to use the weighted average because it provides a performance metric that reflects the data distribution and is suitable for general evaluation.

The performance of the GCN, GAT, and SAGE models across precision, recall, and f1-score metrics reveals specific trends during training. The SAGE model consistently outperforms the others in terms of precision and F1-score, indicating its strength in balancing the accurate prediction of licit and illicit nodes while maintaining a high degree of correctness. While performing comparably in recall, the GCN model struggles with precision, particularly in the earlier stages of training, which negatively impacts its overall balance. The GAT model performs moderately well, showing improvements in precision over GCN but not matching SAGE's overall effectiveness.

Comparing the results achieved by each model according to such metrics, let’s now answer the questions we have previously defined in the context of AML scenario for detecting licit and illicit nodes:

Precision: When the models predict whether a node is licit or illicit, the SAGE model is the most accurate, maintaining consistently high precision throughout the training phase. This indicates that SAGE minimizes false positives and makes more reliable predictions about licit and illicit nodes. The GAT model also performs well in this regard, but its predictions are less reliable than SAGE's, particularly in the later epochs where its precision fluctuates. On the other hand, the GCN model demonstrates the lowest precision, often misclassifying licit and illicit nodes, especially in earlier epochs. Recall: The models perform similarly in identifying all actual licit and illicit nodes, with all three achieving close recall values as training progresses. This means that GCN, GAT, and SAGE can effectively detect almost all licit and illicit nodes. However, recall alone does not account for false positives, so it must be combined with precision to understand the overall model performance.

F1-Score: This score reflects how well the models balance precision and recall in detecting licit and illicit nodes. The SAGE model achieves the highest value, proving it finds the right balance. This indicates that SAGE identifies most licit and illicit nodes and minimizes incorrect predictions. The GAT model performs moderately well, with better balance than GCN, but not as robust as SAGE. Due to its lower precision, the GCN model struggles to achieve high scores despite its high recall, indicating weaker overall performance in balancing these metrics.

Beyond the overall behavior, we can conduct a more specific evaluation to distinguish the performance of these models on licit (e.g., legitimate) nodes from the performance of these models on illicit (e.g., fraudulent and malicious) nodes. With this purpose in mind, we can leverage a confusion matrix that breaks down the classification performance compared to the previously defined metrics by showing the counts of correct and incorrect predictions for each class (licit or illicit). Figure 12.6 shows the confusion matrices of the GCN, GAT, and SAGE models on the testing dataset.

Figure 12.6 Confusion matrices to understand the performance of GCN, GAT, and SAGE models on each class (licit or illicit).

The confusion matrices provide an in-depth comparison of how well the GCN, GAT, and SAGE models distinguish between licit and illicit nodes. Overall, the SAGE performs best, correctly classifying most licit and illicit nodes with minimal misclassifications. The GAT model also performs well but with slightly higher errors for the illicit class. The GCN model exhibits the weakest performance, with lower precision and recall for illicit nodes, indicating challenges in distinguishing these nodes compared to the other models. Let’s analyze the behavior of each model in detail:

GCN: this model shows moderate performance in classifying licit and illicit nodes. It correctly classifies 64.1% of illicit nodes (291 out of 454) and 97.9% of licit nodes (4113 out of 4203). However, 35.9% of illicit nodes are misclassified as licit (163 instances), highlighting its struggle to identify illicit nodes correctly. On the licit side, only 2.1% of licit nodes are incorrectly predicted as illicit (90 instances), showing better accuracy in predicting licit nodes. This imbalance indicates that while GCN effectively identifies licit nodes, its precision for illicit nodes requires improvement. GAT: this model outperforms GCN by correctly classifying 70.0% of illicit nodes (318 out of 454) and 99.5% of licit nodes (4182 out of 4203). It reduces misclassification for the illicit class compared to GCN, with 30.0% of illicit nodes misclassified as licit (136 instances). For the licit class, only 0.5% of licit nodes are incorrectly identified as illicit (21 instances), showcasing its strong capability to predict licit nodes accurately. This improvement demonstrates that GAT provides a better balance between precision and recall than GCN, especially for the illicit class. SAGE: this model achieves the highest performance among the three. It correctly classifies 80.6% of illicit nodes (366 out of 454) and 99.1% of licit nodes (4165 out of 4203). Its misclassification rates are the lowest: only 19.4% of illicit nodes are misclassified as licit (88 instances), and 0.9% of licit nodes are incorrectly classified as illicit (38 instances). These results highlight SAGE’s superior ability to accurately identify both licit and illicit nodes, making it the most reliable model for this task.

Based on the overall analysis, the SAGE model’s superior balance, higher accuracy for both licit and illicit nodes, and minimal misclassifications make it the most effective and reliable choice. It is particularly well-suited for applications where detecting illicit nodes is critical, such as anti-money laundering, while maintaining high accuracy for licit nodes.

12.2 Link Prediction for Movie Recommendation

Link prediction is pivotal in graph-based machine learning and is particularly relevant for applications like recommendation systems. By leveraging graph structures, we can model interactions and preferences as links between entities—in this case, representing the relationships between users and movies.

This section explores the adoption of Graph Neural Networks (GNNs) to predict user-movie links using the MovieLens dataset as a data source. The goal is to evaluate the ability of GNNs to suggest relevant movies while avoiding irrelevant recommendations, thereby enhancing the recommendation process.

Figure 12.7 illustrates the end-to-end framework for applying GNNs to the link prediction task. Each component is designed and implemented to meet the specific requirements of this domain, ensuring a robust pipeline for learning and prediction.

Figure 12.7 This diagram illustrates our end-to-end framework designed for link prediction in the context of recommendation systems. Interactions/preferences data are transformed into a heterogenous graph structure that includes two types of nodes: users and movies. The encoder, a heterogenous GNN, learns node embeddings by capturing its local graph structure. The decoder predicts the existence of links between users and movies by leveraging a dot product operation combined with a binary cross-entropy loss to produce a trained model for suggesting relevant movies to users.

The following subsections explore each component of this end-to-end framework, from data understanding to generating a helpful model for recommendation.

12.2.1 Input Data

We will use the small version of the MovieLens dataset to explore the capabilities of GNNs in performing link prediction for recommendation purposes. This version of the MovieLens dataset includes 100,000 ratings and 3,600 tag applications applied to 9,000 movies by 600 users. Raw data is currently available on the Grouplens website:

https://files.grouplens.org/datasets/movielens/ml-latest-small.zip

The version available in January 2025 provides the following files:

movies.csv: a CSV file with 9742 rows and 3 columns. The columns define the ID of the movie, the title of the movie, and the genres, respectively. ratings.csv: a CSV file with 100836 rows and 4 columns. The columns define the ID of the user, the ID of the movie, the rating, and a timestamp.

We will focus on a subset of the available columns for the link precition task. In the case of movies.csv, we will use the movieId and the genres column. A sample of this file with this subset of columns is available in table 12.7.

Table 12.7 Sample of movieId and genre columns from the movies.csv file

movieId genres 1 Adventure|Animation|Children|Comedy|Fantasy 2 Adventure|Children|Fantasy 3 Comedy|Romance 4 Comedy|Drama|Romance 5 Comedy

The movie is identified by an incremental ID starting from 1, while its genre is provided as a collection of categorical strings separated by the column character (|).

In the ratings.csv file, we will consider only the userId and movieId columns. Table 12.8 provides a sample file with this subset of columns.

Table 12.8 Sample of userId and movieId columns from the ratings.csv file

userId movieId 1 1 1 3 1 6

1 47 1 50

Each row of Table 12.8 defines the rating of a specific movie (identified by the same ID used in the movies.csv file) from a particular user identified by an incremental ID.

12.2.1 Graph Processor – Data Preparation

As we did in the context of the node classification task, we have to prepare data to achieve a compact numerical representation of the original data and a rating graph structure for the GNN learning phase.

Let’s start by processing the movies.csv file. The main goal is transforming the genre information into something numerically processable (vector of features). The code to perform this transformation is listed in 12.14.

Listing 12.14 Transform the genre information into a vector of features

movies_df = pd.read_csv(movies_path, index_col='movieId') #A genres = movies_df['genres'].str.get_dummies('|') #B movie_feat = torch.from_numpy(genres.values).to(torch.float) #C assert movie_feat.size() == (9742, 20) #D

A sample of the output stored in the movie_feat variable is reported in table 12.9.

Table 12.9 Vector of features for the movie genres

movieId Action Adventure Drama Horror 1 0 1 0 0 2 0 1 0 0 3 0 0 0 0 4 0 0 1 0 5 0 0 0 0

In this representation, each movie is associated with a feature vector containing 0s and 1s, where a value of 1 in a specific column indicates that

the movie belongs to the corresponding genre.

In our example, the movie with ID 1 can be categorized as an Adventure movie, while a movie with ID 4 is categorized as a Drama movie. Multiple genre values can be associated with the same movie.

As performed in the context of the node classification task, we need to create an edge_index tensor that describes the connections between the users and movies. The first step is to map the original IDs of movies and users to incremental IDs starting from 0. Then, we can create the edge_index using the new IDs. Listing 12.15 creates the new IDs, maps such IDs to the old ones, and generates the tensor.

Listing 12.15 Generate the edge_index tensor from the new user and movi IDs

ratings_df = pd.read_csv(ratings_path) #A

unique_user_id = ratings_df['userId'].unique() unique_user_id = pd.DataFrame(data={

'userId': unique_user_id, 'mappedID': pd.RangeIndex(len(unique_user_id)), }) #B

unique_movie_id = pd.DataFrame(data={

'movieId': movies_df.index, 'mappedID': pd.RangeIndex(len(movies_df)), }) #C

# Map user IDs in ratings to their new incremental IDs ratings_user_id = pd.merge(

ratings_df['userId'], unique_user_id, left_on='userId', right_on='userId', how='left' ) ratings_user_id = torch.from_numpy(ratings_user_id['mappedID'].va

ratings_movie_id = pd.merge(

ratings_df['movieId'], unique_movie_id, left_on='movieId', right_on='movieId', how='left'

) ratings_movie_id = torch.from_numpy(ratings_movie_id['mappedID'].

edge_index_user_to_movie = torch.stack([ratings_user_id, ratings_

The output of ID mappings for users and movies and the edge_index tensor is reported in listing 12.16.

Listing 12.16 Output of the ID mapping process and sample of the edge_index tensor

Mapping of user IDs to consecutive values: ==========================================

userId mappedID 0 1 0 1 2 1 2 3 2 3 4 3 4 5 4

Mapping of movie IDs to consecutive values: ===========================================

movieId mappedID 0 1 0 1 2 1 2 3 2 3 4 3 4 5 4

Final edge indices pointing from users to movies: ================================================= tensor([[ 0, 0, 0, ..., 609, 609, 609],

[ 0, 2, 5, ..., 9462, 9463, 9503]])

The size of the edge index tensor is [2, 100836], and the second element corresponds to the number of ratings in the dataset. After the preprocessing phase, we can finally create our Pytorch Geometric (PyG) graph structure to represent such ratings as interactions between users and movies.

12.2.1 Graph Processor – Heterogeneous PyG Graph

In our scenario, we have two types of nodes: users and movies. We construct our graph using PyG's HeteroData to represent information about these nodes. Unlike the Data class used in homogeneous graphs, which assumes a

single node type and a single edge type, HeteroData allows us to differentiate features for each node type and associate the edge_index with a specific relationship—in this case, the 'rates' relationship.

Listing 12.17 shows how to construct the PyG HeteroData, leveraging the result of the preprocessing phase, which consists of the movie_feat generated with the related genres and the edge_index.

Listing 12.17 Build the PyG HeteroData object

from torch_geometric.data import HeteroData data = HeteroData() data["user"].node_id = torch.arange(len(unique_user_id)) data["movie"].node_id = torch.arange(len(movies_df)) data["movie"].x = movie_feat data["user", "rates", "movie"].edge_index = edge_index_user_to_mo data = T.ToUndirected()(data)

In the case of PyG HeteroData, we have to add reverse edges to make explicit the GNN message passing from users to movies and vice versa.

After generating the graph, we can split the rating edges into training, validation, and testing datasets. Our main goal is to avoid overlapping between these datasets in terms of links. In the context of node classification, we performed this spitting operation manually to separete the nodes, but in this case, we leverage the PyG built-in functions for the link splitting.

Therefore, to accomplish this task, we can employ the transforms.RandomLinkSplit from PyG and use it as reported in Listing 12.18.

Listing 12.18 Create the training, validation, and testing datasets with RandomLinkSplit

import torch_geometric.transforms as T

transform = T.RandomLinkSplit(

num_val=0.1, #A num_test=0.1, #B disjoint_train_ratio=0.3, #C neg_sampling_ratio=2, #D add_negative_train_samples=False, #E

edge_types=("user", "rates", "movie"), #F rev_edge_types=("movie", "rev_rates", "user"), #G )

The transforms.RandomLinkSplit randomly partitions the edges within the ("user", "rates", "movie") relation into training, validation, and test edges. Compared to traditional dataset splitting, in the case of GNNs we have to consider other factors. For example, the disjoint_train_ratio parameter plays a vital role because it further subdivides the training edges into two distinct groups:

edges used for message passing stored in the edge_index variable; edges used for supervision stored in the edge_label_index variable.

The difference between these two types of edges is reflected in the training data structure reported in listing 12.19.

Listing 12.19 Details on training data

Training data: ============== HeteroData(

user={ node_id=[610] }, movie={ node_id=[9742], x=[9742, 20] }, (user, rates, movie)={ edge_index=[2, 56469], edge_label=[24201], edge_label_index=[2, 24201] }, (movie, rev_rates, user)={ edge_index=[2, 56469] } )

The size of the training set is 80669, which corresponds to 80% of 100836 (the total number of edges). However, the training HeteroData reports a value corresponding to 56469 (70% of 80669) for edge_index and a value of

24201 (30% of 80669) for edge_label_index. It is important to remember that these edge sets are disjointed to avoid an overlap between the edges used for the message passing and those used for the supervision.

Moreover, in the context of a heterogenous graph, the reverse edges are specified with the rev_edge_types parameter. Reverse edges are used for the message passing but not for training the link prediction model. Listing 12.20 shows the details of the validation and testing dataset for completeness.

Listing 12.20 Details on validation and testing dataset

Validation data: ================ HeteroData(

user={ node_id=[610] }, movie={ node_id=[9742], x=[9742, 20] }, (user, rates, movie)={ edge_index=[2, 80670], edge_label=[30249], edge_label_index=[2, 30249] }, (movie, rev_rates, user)={ edge_index=[2, 80670] } )

Test data: ================ HeteroData(

user={ node_id=[610] }, movie={ node_id=[9742], x=[9742, 20] }, (user, rates, movie)={ edge_index=[2, 90753], edge_label=[30249], edge_label_index=[2, 30249]

}, (movie, rev_rates, user)={ edge_index=[2, 90753] } )

The dimensions reported in listing 12.20 allow us to understand which edges are used for message-passing and which are employed for evaluating the model's goodness on the validation and testing dataset.

For the validation dataset, the edge_index size is 80670, equal to the number of edges in the training dataset. The edge_label_index size is 30249, equal to 10083 (10% of 100836), summed to 20166 negative edges generated by the RandomLinkSplit.

In this context, the 80670 training edges are used for message passing, while the 30249 edges are used to evaluate the model for the link prediction task on the validation dataset. An analogous principle is used for the testing set. In this case, the edges used for message passing (90753) include the training (80670) and validation edges (10083).

After splitting the dataset, the next step is to define a mini-batch loader capable of producing subgraphs suitable for input into our GNN. Although this step may not be essential for small-scale graphs, it is crucial to employ GNNs on larger graphs that exceed CPU or GPU memory capacity.

For this purpose, we employ the loader.LinkNeighborLoader component of PyG selects a sample of edges from the set of input edges. Then, it constructs a subgraph from all the nodes in this list by sampling the number of neighbors in each iteration. Listing 12.21 shows how to use the loader.LinkNeighborLoader.

Listing 12.21 Sampling the number of neighbors in each iteration

from torch_geometric.loader import LinkNeighborLoader

edge_label_index = train_data["user", "rates", "movie"].edge_labe edge_label = train_data["user", "rates", "movie"].edge_label

train_loader = LinkNeighborLoader(

data=data, num_neighbors=[20, 10], #A neg_sampling_ratio=2, #B edge_label_index=(("user", "rates", "movie"), edge_label_inde edge_label=edge_label, batch_size=128, #C shuffle=True #D )

It is essential to clarify that this process is also performed on the validation and testing datasets. After describing the data preparation phase, we can understand the architecture for addressing the link prediction task.

Figure 12.8 provides an overview of the steps applied by the Graph Processor, which involves a preparation phase, the construction of a PyTorch Geometric HeteroData object, the creation of training, validation, and testing datasets, and the preparation of the mini-batches.

Figure 12.8 This diagram illustrates the data processing pipeline for the MovieLens dataset, including incremental ID mapping, genre encoding, and edge creation. The heterogeneous graph data comprises two node types (users and movies) and one edge type (user-rates-movie). Edges are split into training (80%), validation (10%), and testing (10%) sets, with negative examples generated for validation and testing. A mini-batch loader then prepares subgraphs for GNN input, ensuring scalability for large graphs that exceed memory capacity.

12.2.2 Encoder-Decoder Architecture

The link prediction system is also based on an encoder-decoder architecture. However, the architecture discussed in this section comprises specific components based on the characteristics of the input data (a heterogeneous graph structure) and the downstream task to be performed (link prediction for

recommendation). Listing 12.22 shows the implementation of the encoder- decoder architecture in this scenario.

Listing 12.22 Encoder-decoder architecture for link prediction

class MovieLensLinkPredictor(torch.nn.Module):

def __init__(self, gnn_model, data, hidden_channels): super().__init__() self.embedding = MovieLensEmbedding( data["user"].num_nodes, data["movie"].num_nodes, hidden_channels ) #A

self.gnn = gnn_model( data.metadata(), hidden_channels, hidden_channels, hidden_channels ) #B

self.classifier = DotProduct() #C

def forward(self, data): #D x_dict = self.embedding(data) x_dict = self.gnn(x_dict, data.edge_index_dict) pred = self.classifier( x_dict["user"], x_dict["movie"], data["user", "rates", "movie"].edge_label_index ) return pred

The forward method of the MovieLensLinkPredictor shows the propagation of the data into the encoding-decoding process. The encoding phase combines two distinct steps to enhance user and movie data representation:

Embedding generation: This step generates embeddings for both users and movies to improve the expressiveness of their features. Since users are not associated with intrinsic features, their embeddings are learned from the model. For movies, a feature vector encoding their genres is used as input to the embedding process, providing a meaningful starting

representation for each movie node. Heterogeneous GNN model: The embeddings are updated using a Heterogeneous Graph Neural Network (GNN) model. This model leverages the graph structure to refine node representations by aggregating information from neighboring nodes. Specifically, user embeddings are updated with information from the movies they interact with, and movie embeddings are updated with information from the users who have interacted with them. This bidirectional information exchange ensures that the embeddings effectively capture the relational structure of the graph.

The decoding phase leverages the learned embeddings to make the prediction. This is implemented using a dot product operation, where the embeddings of user and movie nodes are combined to compute a similarity score. This score represents the likelihood of a link (rating) between a user and a movie. The dot product enables the model to predict user-movie relationships effectively based on the learned embedding. Let’s now analyze the encoder and decoder implementations within our link prediction system.

The Encoder

As introduced at the beginning of this section, the encoder component of our link prediction system for recommendations has two main steps. The first step generates the embedding, while the second applies the heterogeneous GNN model.

The implementation of the embedding generation step is available in listing 12.23.

Listing 12.23 Class for the embedding generation

import torch

class MovieLensEmbedding(torch.nn.Module):

def __init__(self, user_input_dim, movie_input_dim, out_dim): super().__init__() self.movie_lin = torch.nn.Linear(20, out_dim) self.user_emb = torch.nn.Embedding(user_input_dim, out_di self.movie_emb = torch.nn.Embedding(movie_input_dim, out_

def forward(self, data): return { "user": self.user_emb(data["user"].node_id), "movie": self.movie_lin(data["movie"].x) + self.movie_ }

We applied a different embedding generation approach for the user and movie features.

We use a single-step approach for users, creating an embedding matrix with rows equal to the total number of user nodes (user_input_dim) and columns defined by the out_dim parameter. In this context, users do not have any initial features associated with them, and their embeddings are learned solely from this matrix during training.

For movies, we employ a two-step approach. First, we apply a linear transformation to the input features, represented as a 20-dimensional vector encoding movie genres. Then, we combine the output of this transformation with an embedding layer. This method enhances the expressiveness of the initial features, allowing the model to capture both learned embeddings and transformed feature representations.

The second step of the encoding phase employs a heterogeneous GNN. This means that, unlike the node classification in a financial transaction graph, the encoder is designed to manage multiple types of nodes and edges. Listing 12.24 shows the base class for implementing our heterogeneous GNN encoder in PyTorch Geometric (PyG).

Listing 12.24 Implementation of a heterogenous base encoder

import torch from torch_geometric.nn import to_hetero

class HeteroBaseModel(torch.nn.Module):

def __init__(self, metadata, input_dim, hidden_dim, out_dim, super(HeteroBaseModel, self).__init__() #B self.base_model = base_model(input_dim, hidden_dim, out_d self.hetero_model = to_hetero(self.base_model, metadata=m

def forward(self, x_dict, edge_index_dict): #E

return self.hetero_model(x_dict, edge_index_dict) #F

The heterogeneous base class in listing 12.23 includes more ingredients than the homogeneous version. First, it leverages the PyG to_hetero()function to automatically convert our base GNN homogeneous model to a heterogeneous GNN model, which is then used for the aggregation operation. This function requires two distinct parameters: a base GNN model and a set of metadata. To improve our intuition on using the base GNN model, consider the implementation of the heterogeneous version of our SAGE model provided in listing 12.25.

Listing 12.25 Implementation of the HeteroSAGE model

class HeteroSAGE(HeteroBaseModel):

def __init__(self, metadata, input_dim, hidden_dim, out_dim): super(HeteroSAGE, self).__init__( metadata, input_dim, hidden_dim, out_dim, SAGE

In this case, the initialization of the HeteroSAGE class requires as an argument the SAGE class we adopted for processing the homogenous graph in the context of financial transactions. Moreover, we remind you that this SAGE class is a GNN model composed of two SAGEConv layers provided by the PyG library.

The structure of HeteroSAGE and all the other heterogeneous models is based on the related homogenous version and applied to each type of edge in the heterogeneous graph. In our scenario, for the characteristics of the tasks and the data, we have a single edge type that is ("user", "rates", "movie"). However, in most complex scenarios, we could have multiple edge types, and we could decide which convolution layer applies to each of them.

The metadata method of the HeteroData object provided by PyG defines the set of edge and node types, which is the second argument passed to the to_hetero()function. Thus, we can inform the hetero GNN model in which edge types are used for applying the neighborhood's feature aggregation. In other words, the convolution operation is driven by the nodes and edges

specified in the metadata.

The Decoder

The decoder component computes the dot product between user and movie embeddings, derived from the GNN encoder, to determine the compatibility between users and movies. The dot product quantifies this compatibility based on the respective learned feature representations. A higher dot product value indicates a stronger likelihood of interaction or rating, such as the potential user's interest in a particular movie.

This decoding function is paired with the F.binary_cross_entropy_with_logits function from PyTorch, which integrates the sigmoid activation and binary cross-entropy loss. The sigmoid activation converts dot product scores into probabilities, making them interpretable as the likelihood of link existence. The binary cross-entropy loss then measures the difference between these predicted probabilities and the actual interaction labels, driving the model's optimization to enhance recommendation accuracy.

Figure 12.9 provides an overview of the different steps within the encoder- decoder architecture.

Figure 12.9 This diagram illustrates the pipeline for processing heterogeneous graph data in a link prediction system for movie recommendation. The data consists of users and movies as node types and user-movie ratings as edges. Embeddings are generated, with user embeddings learned by the model and movie embeddings initialized from genre features. The data is processed using three heterogeneous GNN encoders (H-GraphConv, H-GAT, and H-SAGE), which aggregate neighbor features to generate node representations. A dot product then quantifies the compatibility between users and movies, with higher scores indicating a greater likelihood of interaction. The scores are finally converted into probabilities representing the likelihood of a link between users and movies.

12.2.3 Evaluation and Analysis

In our analysis, we compare three distinct end-to-end models for link prediction. Each model employs a different GNN encoder: Graph Convolutional Networks (GCN), Graph Attention Networks (GAT), and GraphSAGE (SAGE). The results presented in this section were achieved after 55 epochs on a T4 Colab machine.

Table 12.10 shows the number of parameters and the total training time in seconds for each end-to-end model.

Table 12.10 Number of parameters and training time of the link prediction models leveraging different GNN encoders

Encoder Parameters Train Time (s) GCN 713408 826 GAT 1066880 956 SAGE 713408 777

The results presented in table 12.10 indicate that the SAGE model is the most efficient, the GAT model is the least efficient, and the GCN model falls between the two. The number of model parameters is much higher than the models adopted in node classification. This is related to multiple reasons, including adding an embedding layer to improve the expressiveness of user and movie features and adopting the heterogeneous GNN models to process our data. As you can guess, the parameter size directly impacted the training time, which is much higher than the node classification with the same infrastructure.

To evaluate the performance of GCN, GAT, and SAGE models in a link prediction task for movie recommendations, we can analyze their behavior using the metrics we adopted for the node classification task: precision, recall, and f1-score. These metrics provide insights into how well the models predict links (e.g., user-movie interactions) in a recommendation system. Specifically:

Precision: When the model predicts a link between a user and a movie (e.g., recommending a movie), how often is that prediction correct? Recall: Out of all the user-movie links (e.g., movies a user might like), how many did the model successfully identify? F1-Score: How effectively does the model balance precision (being accurate in its recommendations) and recall (finding as many relevant recommendations as possible)?

Figure 12.10 shows the trend of these metrics on the validation dataset during the model's training.

Figure 12.10 Trend of precision, recall, and f1-score of the GNN models on the validation dataset during the training

Comparing the results achieved by each model according to such metrics, let’s now answer the questions we have previously defined in the context of link prediction for recommendation.

Precision: When the models predict whether a user would rate a movie (link exists), the SAGE model demonstrates the highest precision. SAGE is the most reliable in minimizing false positives, effectively predicting whether a user will interact with a movie by rating it. For movie recommendation tasks, this translates to fewer irrelevant recommendations, where users are unlikely to rate a suggested movie. The GCN model performs slightly worse but maintains high precision, making it a good alternative when reliability is essential. The GAT model, however, exhibits the lowest precision, with more significant variability across epochs, suggesting it makes more incorrect predictions about user engagement (i.e., predicting users will rate movies they are unlikely to rate). Recall: In identifying all actual user-movie links (i.e., all movies a user would actually rate), the GCN model achieves the highest recall. This indicates that GCN is the most comprehensive at identifying movies a user might engage with, successfully capturing the most true ratings. The SAGE model follows closely, with slightly lower recall, meaning it misses slightly more movies than users would rate compared to GCN. However, the GAT model struggles with recall, missing a significant portion of actual user-movie links, which reduces its ability to provide comprehensive recommendations for movies that users would rate. F1-score: This metric reflects how well the models balance precision (accurately predicting user engagement) and recall (identifying most movies a user would rate). The SAGE model achieves the highest F1- score, showcasing its ability to strike the best balance between providing accurate recommendations and ensuring coverage of user preferences. This makes SAGE particularly effective for predicting which movies a user would rate while minimizing irrelevant predictions. The GCN model performs well in F1-score, benefiting from its high recall, but its slightly lower precision than SAGE weakens its overall balance. Due to its lower precision and recall, the GAT model struggles to achieve competitive F1 scores, making it less reliable in capturing the full range of user-movie interactions.

Figure 12.11 presents the confusion matrices of the GCN, GAT, and SAGE models on the testing dataset, providing detailed insights into how effectively each model predicts user ratings. These matrices reveal how well the models identify movies that users would actually rate (true positives) while minimizing recommendations for movies they are unlikely to rate (false positives). They also shed light on the models' ability to correctly disregard irrelevant or weak user-movie connections (true negatives) and their tendency to miss potential ratings (false negatives). This analysis highlights the strengths and limitations of each model in predicting user ratings, offering a clearer understanding of their performance in recommending movies tailored to user preferences.

Figure 12.11 Confusion matrices of GCN, GAT, and SAGE models on the testing dataset

The SAGE model demonstrates strong performance in identifying non- existing links, correctly predicting 94.6% of movies that users would not rate (true negatives: 19,084). However, for movies users would actually rate, SAGE identifies 71.5% of them (true positives: 7,211) while missing 28.5% of true ratings (false negatives: 2,872). This indicates that while SAGE is effective at filtering out movies users are unlikely to rate, it occasionally overlooks potential ratings. Significantly, SAGE minimizes false positives (1,082, or 5.4%), which means it rarely recommends movies users are unlikely to engage with, ensuring highly precise rating predictions. A confusion matrix, used in link prediction, evaluates the performance of a predictive model by comparing its predictions to the actual relationships in a graph. Link prediction typically involves forecasting whether a link (edge) between two nodes in a graph exists or will exist in the future.

The GCN model provides a strong balance between identifying movies users would and would not rate. It successfully identifies 91.7% of movies that users would not rate (true negatives: 18,488) and captures 78.6% of actual ratings (true positives: 7,924). GCN has a lower proportion of missed ratings (false negatives: 2,159) than SAGE, but it has a slightly higher rate of false positives (1,678, or 8.3%). This means GCN is better at finding potential ratings but occasionally suggests movies users may not rate.

The GAT model, in comparison, struggles the most in predicting user ratings. It correctly predicts 88.3% of movies that users would not rate (true negatives: 17,808) and identifies 75.4% of actual ratings (true positives: 7,607). However, its false negative count (2,476, or 24.6%) is higher than SAGE and GCN, meaning it misses more movies that users would rate. Additionally, its false positive count (2,358, or 11.7%) is the highest, indicating a higher likelihood of recommending movies users are unlikely to engage with.

These results show that while all three models perform well in predicting user ratings to varying degrees, SAGE stands out for its precision, ensuring users are less likely to receive irrelevant movie suggestions. GCN offers the best balance between capturing potential ratings and avoiding irrelevant recommendations. At the same time, GAT shows weaker performance, tending to miss true ratings and over-recommend movies users may not rate.

12.3 Summary

Graph Neural Networks (GNNs) allow us to address fundamental challenges in graph-based machine learning tasks, such as node classification and link prediction. These tasks are crucial for various real-world applications, including anti-money laundering (AML) and movie recommendation systems. The chapter also compares different GNN architectures, such as Graph Convolutional Networks (GCN), Graph Attention Networks (GAT), and GraphSAGE (SAGE), to evaluate their performance in these domains. Despite the differences in tasks and application domains, we can define a unique end-to-end framework that comprises multiple steps, from processing the input data as a graph to producing a trained model for the inference phase. This framework is based on the encoder-decoder model introduced in chapter 11. The encoder is a GNN that generates a unique representation of the nodes, and the decoder includes a set of operations that leverages this representation for the specific downstream task. This model is used to detect suspicious transactions in financial networks. Transactions are classified as licit, illicit, or unknown. The Elliptic Dataset, representing Bitcoin transactions, is utilized to perform this task. It includes 200,000 transactions (nodes) and 234,000 payment flows (edges). Each transaction contains 166 anonymized features and is labeled as licit, illicit, or unknown, making it ideal for training fraud detection models. To prepare the data for GNNs, transactions are assigned incremental IDs for a compact representation. Original transaction IDs are removed to create node feature tensors, and transaction labels are encoded numerically. Edge indices define relationships between transactions, structuring the data for learning. The model is trained using three homogeneous GNN architectures based on the PyTorch Geometric (PyG) implementations of GCN, GAT, and SAGE. The Log Softmax function transforms node embeddings into probabilities for classification, while cross-entropy loss optimizes model performance. SAGE outperforms GCN and GAT by achieving higher accuracy in identifying illicit transactions. The confusion matrices show that it

minimizes misclassification errors, while the precision, recall, and F1- score confirm that it provides the best balance between detecting fraud and minimizing false positives. In the context of link prediction, the encoder-decoder model seeks to predict potential interactions between users and movies based on previous ratings. By organizing user-movie relationships as a heterogeneous graph, the goal is to develop a recommendation system that suggests movies users are likely to engage with. The MovieLens dataset is used to conduct this task. The MovieLens dataset comprises 100,000 user ratings and 3,600 tag applications for 9,000 movies. It includes 600 users, each linked to multiple movie ratings. Movies are characterized by genre labels, which serve as input features for the model, enabling it to capture content- based similarities between films. To prepare the data for graph-based learning, movie genres are encoded as numerical feature vectors, enabling representation in a format suitable for GNN processing. Incremental IDs are assigned to users and movies to facilitate efficient indexing. The dataset is then organized into a heterogeneous graph with two distinct node types (users and movies) and one edge type (user-rates-movie), capturing user-movie interactions. The model employs heterogeneous GNN architectures of GCN, GAT, and SAGE to learn user-movie relationships. To enhance feature expressiveness, an embedding layer is implemented for both users and movies. Since users lack intrinsic features, the model learns their embeddings directly. In contrast, movie embeddings are initialized using genre-based feature vectors to provide a meaningful starting representation. To predict user-movie interactions, the dot product operation measures compatibility between embeddings. These scores are converted into probabilities, reflecting the likelihood that a user will engage with a particular movie. Among the three distinct GNN architectures tested, SAGE strikes the best balance between precision and recall, making it the most effective choice for movie recommendations. GCN excels in recall, indicating it captures more potential user-movie interactions but sacrifices precision, resulting in some irrelevant suggestions. Conversely, GAT struggles with accuracy, yielding more incorrect predictions about user

preferences. An analysis of the confusion matrix confirms that SAGE minimizes false recommendations, ensuring users receive more relevant and personalized movie suggestions. Based on these analyses, GNNs have proven to be highly effective for both node classification, such as fraud detection, and link prediction, as seen in recommendation systems. Among the tested architectures, SAGE consistently outperforms GCN and GAT, showing superior generalization capabilities across various tasks. The ability to capture complex relationships makes these models valuable for real-world applications. Furthermore, end-to-end GNN frameworks can be tailored to different domains and tasks, showcasing the versatility of graph-based machine learning models across various applications.

12.4 References

1. Hamilton, Will, Zhitao Ying, and Jure Leskovec. "Inductive

representation learning on large graphs." Advances in neural information processing systems 30 (2017). 2. Kipf, Thomas N., and Max Welling. "Semi-supervised classification

with graph convolutional networks." arXiv preprint arXiv:1609.02907 (2016). 3. Veličković, Petar, et al. "Graph attention networks." arXiv preprint

arXiv:1710.10903 (2017).

## Part_V:_Information_retrieval_with_knowledge_graphs_and_LLMs

The integration of Knowledge Graphs with Large Language Models reaches its practical culmination in this final part, where we explore how to effectively leverage these combined technologies for accurate and reliable information retrieval. While previous parts established the foundations and demonstrated various construction and enrichment techniques, these closing chapters focus on the practical implementation of systems that use knowledge graphs as ground truth to enhance LLM capabilities while preventing hallucinations.

The synergy between KGs and LLMs creates a powerful framework where the structured, verifiable nature of knowledge graphs provides the factual backbone that constrains and guides the natural language understanding and generation capabilities of LLMs. This combination enables the development of systems that can:

Provide accurate, knowledge-grounded responses to complex queries Transform natural language questions into precise graph queries Maintain contextual awareness across multiple interactions Deliver explainable results through transparent reasoning paths Scale effectively while preserving accuracy and reliability

Chapter 13 explores the integration of Knowledge Graphs with LLMs through Retrieval Augmented Generation (RAG), demonstrating how this combination enhances traditional approaches through a real-world case study from the Rockefeller Archive Center. It shows how Graph RAG systems can leverage both structured data and language understanding to provide more accurate and transparent responses.

Chapter 14 illustrates how to build sophisticated question-answering systems that emulate domain expert reasoning, moving beyond simple RAG approaches to create contextually aware solutions. Through a law

enforcement example, it demonstrates systematic approaches to intent detection, schema translation, and expert knowledge embedding.

Chapter 15 brings together all these concepts in a complete, working implementation using LangGraph and Streamlit. It demonstrates how to build a modular pipeline where specialized agents handle different aspects of question processing while maintaining system observability and extensibility.

These chapters provide the practical knowledge needed to implement production-ready systems that combine the strengths of both technologies, serving as a comprehensive guide for organizations looking to deploy reliable, knowledge-grounded AI solutions.

## 13_Knowledge_graph-powered_retrieval_augmented_generation

This chapter covers

The AI agents: LLMs made useful Retrieval Augmented Generation: grounding LLMs using context Building first KG-powered Retrieval Augmented Generation system

The year 2023 could be considered as a year of true AI upheaval. Data scientists and ML engineers alike working in Natural Language Processing and Understanding domain were not just given a new toy, but their whole work lives were soon turned upside down. The release of OpenAI’s GPT-3.5 model at the end of 2022 marked a truly transformative change. Suddenly, one didn’t need to spend months, or more likely years, on building custom training datasets and models for each specific downstream task. With as little as a bit of clever prompt engineering, almost anyone, not just ML specialists, could build applications in natural language domain.

Although Large Language Models are powerful, they are far from a magic solution for everything. As more and more people got hands-on, it became apparent that true transformative changes won’t happen quickly, they have to be deserved: the technology needs to go through the usual hype cycle. Anyone who tried to go beyond simple fun exercises with chatbotting in “question in, answer out” manner quickly realized that lots of work remains to be done, not just on the model architecture and training side of things. The concept of AI Agents, related implementation libraries and whole LLMOps was born.

The topics of this chapter are expressed in the mental model in Figure 13.1.

Figure 13.1 An AI Agent design for KG-powered question answering. The agent has multiple tools at its disposal, which use external data sources such a vector database and a Knowledge Graph to provide necessary context for the user’s question.

In part II and III, we saw how to transform private data into a Knowledge Graph. Now we shall explore how to build a chatbot that leverages this Knowledge Graph as an input.

13.1 The AI Agents

AI agents [1] represent a significant evolution in the capabilities of modern intelligent systems. At their core, AI agents are autonomous entities designed to perform specific complex tasks by interacting with their environment. Unlike traditional software programs that follow a predetermined set of instructions, AI agents exhibit a level of autonomy, adaptability, and “intelligence”, enabling them to make decisions, learn from experience, and respond to changing conditions in real-time.

What does it mean? Let’s think about expectations of a fully functional chatbot, driven typically by a sense of making it worthy, given all the money that the LLM development and deployment costs. We can start simple: “What is the capital of France?” Pretty much any LLM at the top of the current model leaderboard will promptly and correctly reply along the lines “The capital of France is Paris.” Great! It works! However, one can’t help but ask: is this really the kind of questions that will generate satisfactory return on investment, i.e. justify the immense costs of building and running models with hundreds of billions of parameters? Very likely not. Those kind of questions and tasks are much more complex - they typically require at least one of the following:

Advanced multi-step reasoning capabilities: think of deduction tasks such as solving mathematical puzzles, or an adaptable system that needs to adjust the original execution plan based on what outputs are provided by intermediate steps Understanding of deep relational patterns among concepts: think of analyzing a social network where you’d like to identify influencers of certain discussion topics, or a supply chain challenge where you are tasked to pinpoint and remedy bottlenecks Access to the latest, often external and non-public, data not seen by the model during training: this is often referred to as “knowledge cutoff” of LLMs, i.e. the fact that these models are so large that they cannot be

retrained often enough to be able to answer questions such as “What is the weather forecast for tomorrow?”, not to mention questions related to sensitive or even secret internal data, especially when you need to handle multiple personas with different access rights

Imagine we are building a content writing assistant that helps users write articles, blog posts or even social media posts. How would we approach it as humans? First, we would likely conduct research on the requested topic – study articles, blog posts, books and pull basic facts from knowledge bases such as Wikipedia. Then we’d write the first draft, considering all the good practices such as starting with an overall outline first. And finally, we would ask a friend or colleague for a review to make final improvements before publication. This process is exactly reflected in the AI agents design – for this intelligent system, we’d build several agents: Researcher, multiple Writers (depending whether it is an article, blog post or social media post) and a Reviewer. We can think of multi-agent systems as a role-playing game, where the different agents (players) assume different specialized roles in which they excel, and they communicate with each other to solve a task.

13.2 Chatting with the Large Language Model

Now that we understand AI agents conceptually, it is time to look at a practical example. Let’s consider the simplest possible scenario: a chatbot agent where the user communicates with an LLM in a fluent manner, i.e. after asking an initial question, she can decide to follow-up on it in a conversational manner. To build such an agent, we need only two things: an access to a pretrained LLM and a memory to remember the full question- answer set, so that we get a conversational experience. Such agent can be written as a simple class shown in Listing 13.1.

Listing 13.1 A conversational AI agent with memory.

import os from openai import OpenAI from dotenv import load_dotenv

_ = load_dotenv()

class Agent:

def __init__(self, model: str = "gpt-4o-mini", system: str = self.model = model self.system = system self.messages = list() #A self.client = OpenAI(api_key=os.environ['OPENAI_API_KEY']

if self.system is None or len(self.system) == 0: self.system = "You are an AI assistant providing stra self.messages.append({"role": "system", "content": self.s

def __call__(self, message: str) -> str: self.messages.append({"role": "user", #B ➥ "content": message})

answer = self.execute() self.messages.append({"role": "assistant", #C ➥ "content": answer})

return answer

def execute(self) -> str: completion = self.client.chat.completions.create( model=self.model, temperature=0, messages=self.messages) return completion.choices[0].message.content

if __name__ == "__main__":

agent = Agent()

q = "Who are the top influencers of cyclotron #D ➥ funding?"

print(f"> Question: {q}\n> Answer: {agent(q)}")

q = "And in the context of 1930s, related to the #E ➥ Rockefeller Foundation?"

print(f"> Question: {q}\n> Answer: {agent(q)}")

The agent is initialized by model version, system message that frames its scope and the OpenAI API key provided as an environment variable through the .env. The instance variable messages represents agent’s memory as it holds full message history. We could also run it in Jupyter to get more interactive experience, or ideally add a UI layer to have real conversational experience, but here we use it from a standard Python script for simplicity. We demonstrate it by asking it an initial question followed up by a

clarification, since we realized that the initial one was too broad and therefore it wouldn’t be fair to expect that the agent will give us the answer we expected. The output we get is in Listing 13.2.

Listing 13.2 Output of the conversation from Listing 13.1.

> Question: Who are the top influencers of cyclotron funding? > Answer: The top influencers of cyclotron funding typically incl

1. **National Institutes of Health (NIH)** - Provides significant 2. **Department of Energy (DOE)** - Supports research related to 3. **National Science Foundation (NSF)** - Funds various scientifi 4. **Private Foundations** - Organizations like the Wellcome Trus 5. **Universities and Research Institutions** - Institutions with 6. **Pharmaceutical Companies** - They may invest in cyclotron te

These entities influence funding through grants, partnerships, an > Question: And in the context of 1930s, related to the Rockefell > Answer: In the context of the 1930s, the Rockefeller Foundation

1. **John D. Rockefeller Jr.** - As a prominent philanthropist, h 2. **Vannevar Bush** - An influential engineer and science admini 3. **Ernest O. Lawrence** - The inventor of the cyclotron, whose

The Rockefeller Foundation's funding helped establish research pr

We can see that the answer to the first question is very generic. It is correct, but very generic. The follow-up question that clarifies the context in which we are interested is doing better, we see two familiar names (do you recall the Knowledge Graph we built in chapters 5 and 6?) – Ernest O. Lawrence and Vannevar Bush – but again this is not very helpful as both are very well known and therefore not surprising answers. Not surprisingly, out-of-the-box models cannot provide concrete specific answers unless they see, in one way or another, the data based on which we expect the answer to be generated. We will learn more about that in the following paragraphs.

Though we could build agents from scratch in this way by adding more and more complexity, for example handling long message histories that would not fit in model’s context window size or would become impractical and expensive to keep, fortunately we don’t have to. Hand in hand with the current AI boom came also the development of a whole engineering ecosystem around it. As we shall see throughout this chapter, we don’t have

to build agents from scratch.

13.3 Challenges in production environment

In the previous paragraphs, we have built a conversational agent with memory. This was a simple “question in, response out” scenario to get a better practical understanding of the agent paradigm. However, the development of a useful real-world application is much more complex, considering the Large Language Model challenges and concerns, especially:

Hallucinations also known as “making stuff up”: The LLM was trained to predict the most likely next token in the sequence, a process that inherently makes them susceptible to producing plausible-sounding but fabricated facts. This is particularly true when it is prompted about subjects sparsely or entirely absent in their training data. Despite lacking the actual knowledge, the model will still perform the task it was trained for: it will generate the most likely output given its experience encoded in billions of parameters, even though it lacks factual knowledge of the topic. The response will be coherent and fully convincing, although partially or even completely inaccurate. Freshness also known as “knowledge cut-off”: The LLMs are trained on vast amount of data, which makes them incredibly powerful, but also extremely expensive to train. Therefore, the retraining is performed quite rarely, for example once or twice a year, meaning they can’t provide accurate responses regarding (not-so-)recent developments. Transparency: We obtain coherent answer to our question, but without any insight into how it was generated - the information sources and reliability, reasoning processes, confidence level etc. are a concern when developing enterprise solution. Data privacy: Training the models using potentially personal and private sensitive data without leaking it is a concern in many applications. Consider also a very frequent scenario of fine-grained access control, that is a situation where, within your organization, different groups of people have different data access privileges. Costs: Training, deploying and maintaining the best current AI models come with significant costs, both financially and environmentally. The computational power required for training large models is immense,

leading to high energy consumption and substantial carbon footprints. This makes them accessible primarily to well-funded companies. Even though the proliferation of smaller specialized LLMs is lowering the costs, it remains a significant consideration. Ethical concerns and biases: Since the models learn from vast datasets that may contain prejudiced or harmful content, these models can inadvertently reproduce or amplify stereotypes, misinformation, and discriminatory viewpoints, raising serious ethical issues about their deployment and impact on society.

To address these issues, we need to move beyond the simplistic “question in, response out” scenario, which means building agents with higher complexity. For example, to address the freshness concern, we would like to equip our conversational agent with tools that would allow it to retrieve and utilize information necessary for answering questions from external sources, such as downloading latest weather forecast data, news articles or content from an up-to-date knowledge base … such as a Knowledge Graph. This is the topic of the remainder of this chapter.

13.4 Chatting with the AI about private data

If we find ourselves in a specialized domain, even as prolific as a biomedical one, we shall quickly realize that the LLM models will have limited knowledge of it. In such case, we need to find a way to specialize them, to make them experts on our domain and private, quite often sensitive or even secret, data, while preserving their superior language understanding capabilities and general knowledge.

Think of our good old Rockefeller Archive Center use case discussed in chapters 5 and 6. We built a Knowledge Graph which tracks grant-awarding processes that took place at the Rockefeller Foundation in the 1930s, the foundational years of many modern scientific disciplines. This KG captures not only the awarded grants with all the relevant information such as grant amounts, research topics and the university and/or concrete researcher(s), but also many behind-the-scenes conversations that took place between the Foundation’s representatives and the grant applicants prior to the grant approval, i.e. who talked with whom about what. We thus have a whole

“influence network” available, built from proprietary data never published before in its entirety, which allows us to answer accurately questions such as “Who were the influencers of the cyclotron research funding?”.

In the aforementioned chapters, we saw how to design a traditional KG-based system for such a use case through well-tuned graph visualization and dashboards. Now the exciting question is: could we develop an AI interface to deliver the same value to broad range of users without requiring them to speak the Cypher query language, read and interpret charts and tables or manipulate and navigate complex graph data structures? This is a job for AI agents utilizing a pre-trained LLM and multiple context-retrieval tools, called Retrieval Augmented Generation.

13.4.1 Retrieval Augmented Generation

Retrieval Augmented Generation (RAG) [2] is a technique developed to address limitations of pre-trained generative models discussed in section 13.3, especially hallucinations, freshness, transparency and data privacy. It combines the basic general knowledge and powerful language understanding capabilities of pre-trained LLM models with additional context relevant to the question retrieved from an external data source, be it a structured database or an unstructured dataset (text, images).

In practice, a RAG agent is coded as a combination of an LLM model, a prompt guiding the agent’s steps, and one or more tools, which are essentially functions that retrieve question-relevant external information. The model is then asked to generate the answer given the combination of the user’s question and the provided external information as a context. Think of the previously mentioned question about tomorrow’s weather forecast. An out- of-the-box model alone cannot obviously answer it, unless we allow it to call a tool that polls some weather forecast API. At this point, the AI capable to generate an accurate answer by combining its language understanding skills and the external accurate and up-to-date information.

In this sense, RAG is a grounding technique – instead of letting the model go potentially wild (hallucinations), we limit its scope for answer generation only to the provided carefully curated context, thus significantly reducing the

chance that it will make stuff up, instead of following facts. The important word in the previous statement is “limit”, as we can never get fully around the fact that these models are probabilistic in nature, trained to predict the most probable next token in a sequence, so despite a technique like RAG, they can still go astray. This is important to keep in mind when making design decisions of intelligent systems: instead of aspiring to replace humans, they should rather augment them. We believe that keeping human in the loop, typically through a feedback validation or supervision mechanism, is a concept that remains and will remain, despite huge advances in the AI industry, an essential one for any serious impactful product we are building.

Let’s explore a concrete example. In the early days of RAG, the context was almost exclusively coming from a database of textual documents. This situation is depicted in our mental model in Figure 13.2.

Figure 13.2 Vector search-based Retrieval Augmented Generation. The documents are embedded into dense vector representation and indexed in a vector database. When a user asks a question, it is also embedded, and the most similar documents are retrieved from the database. The agent then generates the final answer.

The documents were chunked into smaller portions (for example, paragraphs) and mapped into fixed-length vectors called embeddings, which capture their

semantics. The embeddings were then stored and indexed in a vector database. The process of generating and storing embeddings for the Rockefeller Archive Centre use case is in Listing 13.3.

Listing 13.3 Transform documents into embeddings.

import os from langchain_community.vectorstores import Neo4jVector from langchain_openai import OpenAIEmbeddings from dotenv import load_dotenv

_ = load_dotenv()

if __name__ == "__main__":

vector_index = Neo4jVector.from_existing_graph( #A embedding=OpenAIEmbeddings(), url=os.environ['NEO4J_URL'], username=os.environ['NEO4J_USER'], password=os.environ['NEO4J_PWD'], database=os.environ['NEO4J_DB'], index_name='embeddings', node_label="Page", text_node_properties=['text'], embedding_node_property='embedding' )

q = "What is known about cyclotron research?" resp = vector_index.similarity_search_with_score(q, #B ➥ k=2)

for r in resp: print(f"------\nScore: {r[1]}") print(r[0].page_content)

Here we use the LangChain library, which enables to build AI based systems with support for various models, databases, information retrieval tools and others. For building more complex AI systems, we could use also the LangGraph library, however for this exercise LangChain is more straightforward. The code simply retrieves the texts from Page nodes, uses the specified embedding model to vectorize them and stores the vectors back to Neo4j DB. When the question comes, it is embedded using the same model and the most similar text chunks are retrieved, the shortened output is in Listing 13.4.

Listing 13.4 The top two text chunks most similar to the question.

------ Score: 0.9180829524993896

text: Tuesday, January 31, 1939 (Cont'd) The reproduction cost of the large cyclotron would involve about $

------ Score: 0.9157187938690186

text: Dr. R. J. Van de Graaff, Massachusetts Institute of Physics Although he speaks with complete modesty and with complete genero

The first document is indeed highly relevant to the question asked. The second highest-ranking document also mentions cyclotron, though it resembles much more a sales pitch by Van de Graaff in favor of his, nowadays famous, generator.

13.4.2 Vector-based RAG limitations

We have seen how to start transforming “toys”, i.e. out-of-the-box pretrained LLMs used in “question in, answer out” manner, into useful conversational assistants capable of answering questions on private data using RAG agents based on vector search. Note that even though we have done it using OpenAI’s proprietary models for simplicity, the exact same process is valid for any other provider or open-source model.

What’s next? Is this the best we can do?

Although RAG in its vector-search form is definitely useful, especially (though not only) in the early Proof of Concept stages when one needs to take off fast, it has its challenges and shortcomings, the most notable ones being:

Limited reasoning due to context fragmentation: By providing simple list of matching document chunks, we implicitly encode in the process a tendency to treat documents independently, thus potentially missing more complex multi-hop relationships among them and the entities mentioned within. It also exposes the limitations of our chunking strategy: what if part of the information needed started in a previous

chunk (e.g. paragraph) and continues in the current one, will both still be retrieved, in the right order and ranked high? Simple chunking by splitting by pages or paragraphs won’t likely perform well, we’d have to put much more thought into its design to mitigate at least some of the issues. Scalability: Computationally expensive with larger corpora, often forcing us to resort to using more efficient but less accurate approximate search algorithms. Embeddings limitations: The attempt to encode in a single dense vector semantics of a whole document inevitably leads to over-simplification, i.e. failure to capture important fine-grained semantics and domain- specific nuances. Another limitation is the possibility of a sparsity in the training dataset of the embedding model, i.e. when certain terms are underrepresented. This leads to inaccurate embedding representation and therefore lower retrieval accuracy. Finally, consider the reliance on static (precomputed) vectors, which makes the RAG system less adaptive to new or evolving knowledge and nomenclature. Noise in retrieval: Vector search can return loosely related or even completely irrelevant documents, which leads to distraction phenomenon [3]: too much noise, especially in longer contexts, can confuse the model and results in degradation of output quality. The key is to provide context with as high density of relevant information as possible. Misses in retrieval: Just like the embeddings limitations and the usage of approximate vector similarity search lead to noise, they can also cause the exact opposite – failure to include (some of) the most relevant documents. And if we don’t provide all the facts, we cannot obtain fully correct answer no matter how good the AI model is. Another source of misses can be from questions such as “What are the key research topics in this dataset?”. The answer will be misleading, because the vector similarity search will return chunks the most semantically similar to the question, rather than providing comprehensive data needed to generate the expected answer.

Let’s illustrate the embedding limitations on one concrete example: say our question is “How is Lauritsen related to cyclotron research?”. Intuitively, we would expect that the most similar documents will, at the very least, mention

Lauritsen by name. Wrong. This is not how embeddings work. They encode the overall meaning of a text and as such, they represent a condensed “summary”. They can’t guarantee that mathematically the most similar document even mentions the exact entity we asked about. The semantic similarity of a question and a document can pick up on another linguistic meaning or pattern. And indeed, when we embed the question above and find the most similar documents based on cosine similarity, only one of the documents actually features Lauritsen (and cyclotron), while the other two mention only cyclotron and are therefore irrelevant to the question, even though they have nearly the same similarity score as the first document. The overview of the top three documents is in Table 13.1.

Table 13.1 Top three most similar documents to the question “How is August Krogh related to cyclotron research?”, and an indication whether “August Krogh” and “cyclotron” are mentioned within.

Document Cosine similarity

Lauritsen Cyclotron

Tuesday, January 3, 1939. Professor Karl Lark-Horovitz, Purdue University. The Van de Graaff machine at Purdue was built in two or three months and is now operating, producing 600 micro- amperes at 850 kilovolts. Thus as a neutron generator it is equivalent to several pounds of radium. The Purdue group has devised …

0.906 True True

Tuesday, May 2, 1939. Dr. Irving Langmuir, General Electric Company. L. very strongly favors continued support for Dr. Dorothy Wrinch. He bases this primarily on two considerations. First, and giving due recognition to the fact that W. is a difficult person whose scientific

0.903 False True

behavior is not always what it should be, L. considers it unquestionably true that W. has been responsible for stimulating … Dr. Dorothy M. Wrinch, April 3, 1939 (continued). In connection with the X-ray structure problem Langmuir obtained from Clowes some money which W. used for computing. She is not sure that she should have accepted funds from elsewhere, but WW assures her that …

0.902 False True

How can we overcome these limitations? For careful readers of this book, it will come as no surprise that our answer is: bring Knowledge Graphs into the picture. The graph-based approach to RAG is commonly referred to as “Graph RAG”. Besides mitigating or even completely solving the challenges related to vector-based RAG discussed above, it has additional benefits. The KG acts as a central knowledge repository, integrating not just raw texts, but also various document metadata and high-confidence structured data sources (tables, CSVs, ontologies …), all of which together unlock much wider range of use cases. And since the KGs represent knowledge in human accessible format, they are easy to maintain up to date and allow domain experts validate existing or enter new knowledge, thus having direct impact on the AI system’s output quality. This in turn helps to shed some light into the AI black box, resulting in higher user confidence due to increased transparency.

13.4.3 Graph RAG

Although the vector-based Retrieval Augmented Generation represents a significant improvement when it comes to real-world applications of the AI, we have also seen that it is still not quite there. A room for improvement remains and the structured Knowledge Graphs are here to help.

In the AI era, things are moving fast. The same is true for the reasoning about synergies between KGs and LLMs [4]. As it turns out, both technologies conveniently complement each other. In this book, we have already seen the

examples of what can be called LLM-augmented KGs, for e.g. when we leveraged LLMs for KG construction from unstructured data and enhanced them through LLM-powered entity disambiguation or KG completion. The time has come to explore the benefits of KG-enhanced LLMs, which helps to address issues such as hallucination, lack on interpretability or domain specificity, complex reasoning paths and others.

The Rockefeller Archive Center KG that we’ve built earlier in this book is a great example of an LLM-augmented KG. We’ve built it using prompt engineering on top of the OpenAI’s ChatGPT model, which not only extracted entities and relations, but also completed the identified information (entities and relations) using the model’s internal knowledge about scientific research, locations, universities or academic titles. The resulting KG is an example of a combination of a Text-Attributed Graph and a Text-Paired Graph [5], see Figure 13.3.

Figure 13.3 The Rockefeller Archive Center KG as a combination of Text-Attributed and Text- Paired graphs. The documents are represented as nodes, each having properties representing metadata such as author, type of document, and date. The extracted entities and relationships are traceable back to the original documents, which enables us to design a KG-based document selection tool for AI Agents.

The Text-Attributed Graph is a graph with nodes and relationships that have textual attributes, while the Text-Paired Graph is a graph whose nodes, relationships, and whole subgraphs are associated with the documents (diary entries) as their descendants. This design enables us to implement a Graph RAG system in ways that take advantage of all the thinkable aspects of such data and knowledge modeling, especially:

Metadata: The documents typically have various associated metadata such as publication date, type, source, author, and others. All this information is available in the KG as properties and relationships and can also be used, in addition to the content, to design a relevant context retrieval system. For example, communities can be identified in the graph of documents and each community can be represented by a summary, or by the most recent document (think of the most up-to-date version of a law or regulation). KG Retriever: A Knowledge Graph as a structured governed information source is a great context resource on its own merit. The nodes and typed relationships, both with textual as well as numerical properties, provide condensed, accurate, reliable, up-to-date information that can be directly used by an LLM to generate responses. A model can take the user’s question, the KG’s schema, and generate a Cypher query to retrieve the information it needs: an aggregate calculations or summaries, concrete node or relationship properties, whole nodes or even multi-hop paths. The KG retriever tool can also take as input the entities the user is asking about and return a whole subgraph that connects them (for e.g. all shortest paths), leaving it up to the final LLM to use whatever part of this data that it deems relevant and ignore the rest. There are various ways how to integrate the KG data source into LLM predictions. KG-Enhanced Document Retriever: If our KG is a text-paired graph, we can leverage it as a document retriever which is more accurate than the vector search-based one. For example, we can use it to retrieve only documents that mention all the entities the user asks about in her question to eliminate one of the vector-search failures discussed in the previous section. Or if we need more detailed information about a relationship (such as Person–[:TALKED_ABOUT]->Person) than a KG can provide (what exactly was said), we can retrieve only documents about which we know that mention this relationship, avoiding thus the distraction phenomenon. Combined retrieval: Sometimes, we face questions that span multiple data sources. For such cases, we can design an AI agent such that it decomposes the question into one that first retrieves the KG information (the KG Retriever), and another one that uses this information to find the most relevant documents. Both contexts are then merged before the final

answer is generated. Think of a question “What are this year’s transactions of the head of criminal group X?” Since it would be surprising to find that the financial documents contain “Head of X” as a title of that person, we first need to extract that name from a KG built by a law enforcement agency, and then use it to search through the financial documents database.

Now that we understand what Graph RAG is about, let’s implement a simple agent on top of the Rockefeller Archive Center KG, see Figure 13.4.

Figure 13.4 KG-powered Retrieval Augmented Generation agent with a grounding in external data sources: the Knowledge Graph and a vector database.

Our Graph RAG agent will have three tools at its disposal: the KG Retriever, the KG-Enhanced Document Retriever, and a Semantic Retriever (vector search tool) as a backup in case the other tools fail to return valuable context. The full code is available in our GitHub repository[1], here we shall have a close look at the KG-Enhanced Document Retriever. It is implemented as a parametrized tool for the use case of identifying documents where specific relation between two entities is discussed, so that we can answer questions such as “What did person X say about person Y”, see Listing 13.5.

Listing 13.5 KG-Enhanced Document Retriever tool for Graph RAG agent.

from pydantic import BaseModel, Field from langchain_community.graphs import Neo4jGraph

RE_SELECTOR_QUERY = """MATCH (p:Page)- #A ➥ [:MENTIONS_ENTITY]->(m1:Ent… WHERE e1.name = "{e1}" … … RETURN DISTINCT p.id AS id, p.text AS text """

graph = Neo4jGraph( #B

url=os.environ['NEO4J_URL'], username=os.environ['NEO4J_USER'], password=os.environ['NEO4J_PWD'], database=os.environ['NEO4J_DB'] )

class REDiarySelectorInput(BaseModel): #C

entity_source: str = Field(description="Source entity of the ➥ relationship as mentioned in the question.")

entity_source_class: str = Field(description= "Class of the source entity of the relationship. " "Available option is only one, 'Person'.")

entity_target: str = Field(description="Target entity of the ➥ relationship as mentioned in the question.")

entity_target_class: str = Field(description= "Class of the target entity of the relationship. " "Available options are Person, Organization, Occupation and relationship: str = Field(description= "Relationship class between source and target entity. "

"Available options: TALKED_ABOUT, TALKED_WITH, WORKS_WITH, W ➥ HAS_TITLE")

def kg_doc_selector(entity_source: str, entity_source_class: str, ➥ entity_target: str, entity_target_class: str, relationship: str ➥ List[AnyStr]: #D

query = RE_SELECTOR_QUERY.format(e1=entity_source, e1_class=entity_source_class, e2=entity_target, e2_class=entity_target_class, rel_class=relationship) print(f"kg_doc_selector's query:\n{query}\n")

try: res = graph.query(query) print(f"kg_doc_selector found {len(res)} matching documents" except Exception as e: print(f"Cypher execution exception: {e}") return [] return [x['text'] for x in res[:3]]

The tool takes as input the two entities mentioned in the question, their classes (for example Person) and the relationship type – these are provided by the AI agent based on the user’s question. The main selector function then uses them to complete a pre-canned Cypher query, which gets executed against the Neo4j DB and the documents are returned to the agent. We could also design more generic tool in a way that the document retriever Cypher query is generated automatically based on the question, however such step would introduce another possible point of failure, especially in this use case when the Cypher queries are very complex. That’s why many Graph RAG systems in production typically contain a variety of KG-related tools, many of which are based on pre-canned Cypher queries for frequently repeating type of questions.

13.4.4 Reasoning Agents

The time has come to glue everything together in a single agent. LangChain library provides several pre-canned agents that make it very easy. We have multiple tools with no clear execution order, so it calls for the use of a ReAct agent [6]. It integrates reasoning and acting capabilities (hence the name ReAct – Reason and Act) to improve problem-solving in complex

environments. The ReAct framework iteratively reasons about tasks to take, acts and observes the outcomes in a dynamic feedback loop, refining its approach based on real-time outcomes.

The agent takes the original question within constraints of the tools we provide it with, plans the next task (tool to execute), executes it and reasons about the outcome. If the obtained information is not satisfactory, it acts using another tool. When it obtains contextual information that it deems sufficient to answer the original question, it ends the loop. A shortened code that defines such agent is in Listing 13.6.

Listing 13.6 ReAct Agent implementing Graph RAG approach.

from langchain.tools import StructuredTool from langchain.agents import create_structured_chat_agent, AgentE from langchain_openai import ChatOpenAI from tools import REDiarySelectorTool, kg_doc_selector, REDiarySe from definitions import KG_SCHEMA

tools = [ #A

StructuredTool.from_function( func=kg_doc_selector, #B name="KG-based-document-selector", args_schema=REDiarySelectorInput, #C description=f"Use it for document (diary #D ➥ entries) retrieval when the question asks for detailed informat ➥ regarding interaction between two entities … Full KG ➥ schema:\n{KG_SCHEMA}"

), <KG RETRIEVER>, #E <VECTOR SEARCH> #F ]

llm = ChatOpenAI(model=“gpt-4o-mini”, temperature=0) prompt = hub.pull("hwchase17/structured-chat-agent")

agent = create_structured_chat_agent(llm, tools, prompt) #G agent_executor = AgentExecutor(agent=agent, tools=tools, max_iter ➥ return_intermediate_steps=True, verbose=True)

question = "What did August Krogh say about Lawrence Irving?" response = agent_executor.invoke({"input": question})

Since the KG-Enhanced Document Retriever is a parametrized tool, we have to use the structured ReAct agent that supports passing multiple input parameters to its tools. The tools have appropriately chosen names and descriptions to help the model to determine which tool to call in which situation. Pay attention to these descriptions when you design your agent – if you write them well, you can significantly improve the stability and predictability of your system. Don't hesitate to overwrite the default prompts provided in various agents and tools by LangChain if you notice that the system doesn’t behave as consistently as you would expect it to. Always test, test, test, repeatedly, even with the same setup and input question. It can reveal many insights into how to improve your application.

To conclude the code building part of this chapter, let us highlight that this serves only as an illustration how to build KG-powered Graph RAG systems. We do not suggest that this is a production ready system. The whole purpose of this chapter is to provide insight into how KGs and LLMs can be integrated, it does not aspire to be a comprehensive tutorial. There are many improvements that we could introduce, for example developing meaningful document re-ranking strategy for vector-search tool, adding Cypher self- correction loop to the KG Retriever, or add more tools to support other typical questions a user could ask. For more details, refer to this book [7].

13.4.5 Let’s chat with our Knowledge Graph!

Now that our first Graph RAG agent is ready, it is time to put it to the test. When you try it yourself, you might see different outputs depending on which version of the OpenAI’s GPT model you are using, not to mention when using a completely different model provider.

Imagine you are a researcher coming to the Rockefeller Archive Center to get information on Dorothy M. Wrinch (1894-1976), a mathematician and biochemical theorist. Her work is fairly well known, but you might be interested to know how she was perceived by her colleagues at her time. You could find her in the KG, look at incoming TALKED_ABOUT relationships, and drill down until you find the documents from which they were extracted and read them. Alternatively, you can use your new agent: Figure 13.5 uncovers what happens when you ask it question “What did her fellow researchers say

about Dorothy M. Wrinch?”.

Figure 13.5 Example of Graph RAG agent internal steps. The agent starts with a thought that leads it to use the KG Retriever to obtain a list of people who talked about Dorothy M. Wrinch. Then for each of them, the KG-enhanced Document Retriever is used to identify the relevant documents. Finally, all of this is used as a context by the agent for generating the final answer.

The agent takes into account all the tools it has available. It correctly realized that the best course of action is to combine two tools: first to retrieve the list of people who talked about her from the KG, then to use for each of them the specialized KG-enhanced Document Retriever to obtain relevant documents providing details of what was said. Four Thought/Action/Observation steps were taken altogether. The final answer, generated based on all the context retrieved, is impeccable: straight to the point and factual, clearly grounded in private documents provided to it.

In Table 13.2, we demonstrate two more questions.

Table 13.2 Graph RAG question-answer examples.

Question Graph RAG Answer What did August Krogh say about Lawrence Irving?

August Krogh spoke with enthusiasm about Scholander's work on the physiology of respiration of deep diving animals in Norway and expressed the importance of getting Scholander and Lawrence Irving together. Krogh noted that Irving had done excellent work and could provide Scholander with facilities in the United States that were not available in Norway. They explored the possibility of working at Boothbay Harbor, Maine, and potentially with porpoises at aquatic zoos in Florida.

Are there any shared research topics between Harvard University and Johns Hopkins University?

The shared research topics between Harvard University and Johns Hopkins University are astronomy and climatology.

The first question is a variant of the one about Dorothy M. Wrinch, except

this time the agent can use directly the KG-enhanced Document Retriever tool since we ask about two concrete people. If you asked the same question to the out-of-the-box LLM, you would get, in the better case, an answer saying that “There is no specific information available about what August Krogh said about Lawrence Irving …”, in worse case it would hallucinate the answer. With our Graph RAG system, however, we get a highly accurate insight into their relationship.

The second question is an example where structured KG content is sufficient to answer it. The KG Retriever tool generates a correct Cypher query – true, a bit clumsy one, but a correct one. The answer is therefore very straightforward and accurate. In contrast, if we would like to answer this question purely based on the vector-search, we would need to provide large set of documents as a context to have any hope that the complete answer will be contained somewhere within them. That’s because it is an example of an aggregate question that spans multiple documents: some diary entries discuss research topics done by people at Harvard, others describe topics at Johns Hopkins, but none directly discuss what Harvard and Johns Hopkins have in common. This is an insight that comes from connecting the dots across multiple documents, a task where KGs excel, and as a side product, they reduce the risk of the distraction phenomenon and hallucination, making predictions faster and cheaper (fewer context data is needed).

When it comes to RAG systems, there is a very wide range of approaches and measures one can take to improve their accuracy, reliability and stability. We only introduced the concept in this chapter, anything else is out of the scope of this book. We can add for example self-correction loops, so that the model generates the Cypher query in the first step, and then ask the LLM to double- check it and potentially correct the query in the follow-up step. Or we can add a more advanced document reranking step after the initial context selection to improve the relevance and limit its size. In the next chapter, we will dive deeper into the topic of Cypher generation. For more detailed insights into the Graph RAG systems, please refer to [7].

13.5 Summary

An AI agent is a combination of an LLM model functioning as the

agent’s brain, and a set of tools that allow the agent to interact with the outside world. Retrieval Augmented Generation (RAG) is a framework for building intelligent systems, such as AI agents, by combining generative models and information retrieval. As such, it addresses inherent LLM issues such as hallucinations, freshness, transparency or data privacy. Vector-based RAG systems suffer from multiple shortcomings such as limited reasoning capabilities, scalability issues, or inaccuracies in retrieval, including noise and missed relevant information. Graph RAG integrates KGs with LLMs to reinforce the reasoning capabilities and precision of information retrieval by leveraging the structured relational multi-hop patterns within the KGs. KGs also bring in more control and transparency into the overall question-answering process. The KG integration methods into RAG systems are determined by the graph design. The most useful KGs are a combination of text-attributed and text-paired graphs, which enable to leverage both the well-curated structured knowledge and documents along with their metadata.

13.6 References

1. Lanham, Michael. AI Agents in Action. Manning Publications, 2025.

ISBN 9781633436343. 2. Gao, Yunfan et al. Retrieval Augmented Generation for Large Language

Models: A Survey, arXiv:2312.10997 [cs.CL] 3. Liu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F.,

and Liang, P. Lost in the middle: How language models use long contexts. arXiv:2307.03172 [cs.CL]. 4. Shirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang, Xindong

Wu. Unifying Large Language Models and Knowledge Graphs: A Roadmap. arXiv:2306.08302 [cs.CL] 5. Bowen Jin, Gang Liu, Chi Han, Meng Jiang, Heng Ji, Jiawei Han. Large

Language Models on Graphs: A Comprehensive Survey. arXiv:2312.02783 [cs.CL] 6. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik

Narasimhan, Yuan Cao. ReAct: Synergizing Reasoning and Acting in Language Models. arXiv:2210.03629 [cs.CL]

7. Bratanič, T., Hane, O. Knowledge Graph-Enhanced RAG. Manning

Publications, 2025. ISBN 9781633436268.

[1] https://github.com/alenegro81/knowledge-graphs-and-llms-in-action

## 14_Ask_a_KG_with_natural_language

14 Ask a KG with natural language

This chapter covers

Understanding the limitations of RAG in complex scenarios Building an advanced question-answering system that mimics domain expertise on KGs Transforming query results into meaningful, actionable summaries

In this chapter, we will explore how to build an advanced system that can answer questions effectively. Using a law enforcement example as our guide, we'll compare the RAG approach and our new "expert emulation" method for capturing the expertise of skilled information retrieval. We will walk you through the key concepts and components needed to build this system, giving you a solid foundation for creating reliable question-answering applications.

The framework we'll develop rests on several key pillars:

Understanding and properly routing different types of user questions Extracting and representing domain knowledge in a form that LLMs can effectively utilize Implementing expert-like reasoning patterns for query construction Ensuring results are presented in meaningful, actionable ways

This framework is specifically designed to integrate with a front-end layer, ensuring that the question-answering system can be effectively presented to end users through a graphical interface. This integration-first approach influences many of our design decisions throughout the chapter, from how we structure query responses to how we handle data visualization.

This chapter walks through each of these components using concrete examples to illustrate the key concepts and design decisions, while chapter 15 will provide a detailed, hands-on implementation guide, complete with an end-to-end example that brings these concepts to life in a practical setting. By the end of these two chapters, you'll have both a deep understanding of the

underlying concepts and the practical knowledge needed to implement an expert-emulated question-answering system for your own knowledge graphs.

Throughout this chapter, we'll use examples from law enforcement to illustrate key concepts, but the principles and patterns we discuss can be readily adapted to any domain where expert knowledge needs to be systematically applied to complex data structures. Whether you're working in healthcare, finance, or scientific research, the framework we develop will provide a solid foundation for building intelligent question-answering systems that truly leverage domain expertise.

14.1 Querying a knowledge graph in the policing domain

Imagine you are an experienced analyst in a large law enforcement agency. Every day, you face the challenge of making sense of diverse and often overwhelming data streams. The policing domain is inherently complex and dynamic, with a mission to investigate crimes, prevent criminal activities, and protect the community. Your work sits at the heart of this mission.

The policing ecosystem encompasses a wide range of roles and responsibilities. Frontline officers respond to incidents and collect on-the- ground information, detectives and investigators connect the dots to solve cases, forensic experts analyze evidence, and analysts like you uncover patterns and trends from disparate data sources. Each role contributes vital pieces to the puzzle, and their collective efforts drive the interconnected processes within law enforcement.

A simple crime report can set off a cascade of activities—interviews, evidence collection, database searches, and more. Each step in this chain generates and consumes data, making it crucial to integrate and organize this information effectively. Recognizing the need for a unified approach, your agency has invested in building a knowledge graph—a single source of truth that structures and connects the roles, processes, and vast amounts of data within the policing domain.

In the policing domain, data rarely comes neatly packaged. Instead, it flows

in from countless sources—crime reports, surveillance logs, forensic databases, and more—each adding to the complexity of the ecosystem. But here’s where your expertise shines: throughout this book, you’ve developed the skills to work seamlessly with both structured and unstructured data. You’re now equipped to cut through the noise and extract meaningful insights.

In this section, we will concentrate on extracting information, uncovering knowledge, and deriving insights from a fully constructed, ready-to-query knowledge graph.

14.1.1 Enabling Domain Experts with Knowledge Graphs

As we continue, let’s focus on the qualities that define a seasoned analyst like yourself. You possess sharp intuition, a deep contextual understanding, and the ability to discern patterns in complex scenarios. In policing, these qualities often outweigh technical expertise when it comes to interpreting data effectively and making impactful decisions.

Despite this, querying knowledge graphs is traditionally seen as a task requiring deep technical skills—skills that are outside the typical toolkit of domain experts. However, empowering analysts like you to interact directly with KGs opens immense possibilities, addressing several key points:

1. Leveraging Expertise: You, as a domain expert, bring unparalleled

contextual knowledge to the table. This enables you to frame nuanced and insightful queries, driving highly accurate and relevant data retrieval. 2. Timely Decisions: Direct access to KGs eliminates bottlenecks, allowing

you to make quicker decisions—a necessity in critical fields like law enforcement, where time is often of the essence. 3. Overcoming Technical Barriers: Most domain experts, while masters of

their fields, lack the technical training to write complex queries for knowledge graphs. Removing this barrier expands the accessibility of these powerful tools. 4. Resource Efficiency: By reducing reliance on IT teams or data scientists

to generate queries, you free up technical resources for other priorities,

making workflows more efficient and collaborative. 5. Skill Specialization: Your focus is on mastery of your domain, not on

becoming a technical expert. Enabling intuitive KG interaction ensures that your energy remains devoted to what you do best. 6. New Perspectives: When you can engage directly with data, your unique

perspective allows for the discovery of innovative insights and applications that technical users might overlook. 7. Enhanced Problem-Solving: With the ability to explore data hands-on,

you can uncover creative approaches to solving problems, tailored to your domain’s needs. 8. Maximizing ROI: Knowledge graphs represent a significant investment.

Ensuring their full utilization by empowering analysts maximizes their value and impact. 9. Cross-Functional Collaboration: Easy access to KGs fosters

collaboration across different teams, bridging gaps between technical and non-technical stakeholders by creating a shared platform for data access. 10. Shared Understanding: By enabling broader access to KGs,

organizations promote a unified approach to data analysis and problem- solving, building alignment across roles and disciplines.

By breaking down these barriers, the true potential of knowledge graphs can be unlocked, bringing powerful insights and practical solutions to the forefront of policing challenges.

Whether you're considering implementing a knowledge graph system in your organization or need to make a case for one to decision-makers, these points highlight the concrete benefits from an analyst's perspective. They demonstrate how empowering domain experts with direct access to knowledge graphs can transform data interaction, improve operational efficiency, and ultimately deliver better outcomes across any field where domain expertise meets data analysis.

14.2 Limitations of RAG in complex real-world scenarios

Let’s consider the types of questions a law enforcement professional may ask an LLM. It’s important to remember that LLMs generate answers based on the data collected during their training phase, which encompasses a vast amount of information. While LLMs can indeed answer questions that require specialized expertise, their utility diminishes for users already experts in their field, possessing deep knowledge and years of on-the-ground experience.

In this context, it’s hard to think of questions beyond basic inquiries like, “What is the legal definition of “probable cause?” or “How should an officer properly collect and preserve evidence at a crime scene?” or “How does the ‘Miranda warning’ protect a suspect’s rights during an arrest?” The responses that an LLM can generate are likely to be of little use to these trained professionals. To be truly beneficial, an LLM should provide access to information that personnel can leverage to expedite their processes, whether it involves suspect identification, organized crime network analysis, or other complex tasks.

The most popular approach to answering questions grounded in the data available to an organization, such as a law enforcement agency, is Retrieval- Augmented Generation (RAG), as we discussed in chapter 13.

14.2.1 RAG in a law enforcement environment

There is little chance that an LLM alone can effectively handle requests like, “Generate a summary of witness statements related to CASE123”, simply because the model has no prior knowledge of that specific case.

Without any contextual information, the LLM cannot access the details necessary to provide a meaningful response. However, if we integrate that question with a list of all witnesses connected to CASE123, the situation changes significantly. It transforms into a straightforward summarization task, and LLMs excel at this type of work. When questions are accompanied by relevant contextual content, LLMs can generate richer and more pertinent answers.

Listing 14.1 Witness statements collection

1. Witness A, a barista at a local coffee shop, says: "The person

2. Witness B, a neighbor who saw the perpetrator leaving the scen

3. Witness C, a security guard at a nearby building, says: "I rev

4. Witness D, a friend of the victim, says: "When I went to visit

5. Witness E, a police officer who responded to the scene, says:

Using the witness statements collected in Listing 14.1, let's build a prompt to test this idea, as shown in Listing 14.2. We'll be employing a reusable template, where the portions within double curly brackets will be replaced with the relevant witness statement content and the question itself.

Listing 14.2 Template Prompt

You are a detective assistant helping answer the detective's ques [Witness statements] {{ witness_statements }} [Question] {{ question }}

If we use the prompt to ask to "summarize the witness statements", the model provides the results shown in Listing 14.3.

Listing 14.3 Question: summarize the witness statements

Here's a summary of the witness statements:

**Physical Characteristics:**

- Unique jacket with a distinctive logo on the left sleeve (Witne - Small bandage on the left index finger (Witness A)

**Actions and Behaviors:**

- Used right hand to take out wallet (Witness A) - Carrying a bag in right hand (Witnesses B and C) - Held phone up to ear with left hand while walking (Witness C) - Seemed to be in a hurry and walking quickly (Witness B)

**Scene Evidence:**

- Footprint with distinctive tread pattern, but unclear which foo - Book on bookshelf was out of place, but no other unusual detail

The model has clearly understood the task and delivered a concise and relevant summary of the key details from the witness statements. The summary is well-structured, with clear sections covering the physical characteristics, actions and behaviors, and scene evidence described by the various witnesses.

With the full list of witness statements, we can ask more than just for a summary; we can inquire whether there is anything related to a specific lead we are following. In this case, the LLM will generate an answer that is consistent with the content we provided—namely, the witness statements— directing us to relevant pieces of information. This capability allows the LLM to assist in uncovering insights that may be critical to the investigation.

For example, if we use the prompt to ask whether the suspect is left-handed, the model's response is shown in Listing 14.4. The output provides a detailed analysis of the relevant witness details, concluding that there is some evidence suggesting the suspect may be left-handed.

Listing 14.4 Question: Is the perp left-handed?

Let's analyze the witness statements to determine if the perpetra

Witness A mentions that the perpetrator used their right hand to

Witness C says that the perpetrator was holding a phone up to the

Witness B and Witness E's statements don't provide any direct inf

Based on the available evidence, I would say that it's likely tha

Suppose the case in question is a murder investigation, and we want to find any connections between the victim and known criminal organizations. Given that people may be reluctant to discuss local criminal activities openly, a complete list of witness testimonies may not yield useful insights. To obtain meaningful answers, we need more contextual information, and this is where LLMs can be particularly effective.

Additional context might include the victim’s criminal records, prior convictions, personal history, associations or relationships, social media profiles, or any officer reports mentioning him or his family. As you can imagine, this content must be retrieved from the respective sources, filtered for relevance, and then provided as context along with your question to the LLM for augmented answer generation. This retrieval, filtering, and contextual augmentation is exactly what Retrieval-Augmented Generation (RAG) aims to accomplish.

14.2.2 Understanding RAG's core limitations

Retrieval-Augmented Generation is a highly effective approach with a wide range of applications. However, it comes with its own set of challenges. The success of RAG is closely tied to the effectiveness of the retrieval process.

LLMs cannot generate accurate answers if they are not provided with sufficient context; therefore, for RAG to produce a meaningful response, the retrieval process must be capable of locating the correct information.

Listing 14.5 shows how strongly the response depends on the context provided, compared to the listing 14.4. Even the absence of a single witness statement (Witness C in this example) is enough to lead the model to a diametrically opposite conclusion.

Listing 14.5 Question: Is the perp left-handed? (witness C not provided)

Based on the witness statements, I think we can make an educated

Witness A, the barista, mentions that when the person went to pay

Additionally, Witness B, the neighbor, states that the perpetrato

There is no concrete evidence in the statements to suggest that t

So, based on the available information, it seems more likely that

Figure 14.1 provides a visual representation of this process. Even though the AI agent confidently processes the available information, the incomplete context - caused by the retriever failing to identify critical documents - leads

to wrong conclusions. This visualization emphasizes a key limitation of RAG systems: their outputs are only as good as their retrieval step, regardless of the model's confidence in its response.

Figure 14.1 RAG System Limitations. The diagram illustrates how a RAG system can produce incorrect answers when the retriever fails to identify critical documents.

In addition, RAG performs better when your sources of information can be divided into “passages” that contain fine-grained, independent pieces of information—essentially, many small blocks of information that can be used to construct a highly relevant context.

In practice, the answer you are seeking often spans multiple documents, and your retrieval process may not capture all of them. Furthermore, some documents may still hold relevance even if they do not contain the specific answer you are searching for, which adds an additional layer of complexity to the task for the LLM.

When the retrieved passages lack the appropriate level of granularity, or if the retriever fails to accurately assess relevance, the context provided to the LLM becomes fragmented. This fragmented context can push the model toward incorrect assumptions, often leading it to generate plausible but inaccurate details.

Without a cohesive, well-defined context, even a highly capable LLM can be led astray, producing responses that may sound correct but do not truly align with the underlying information.

These limitations highlight the challenges of using RAG with knowledge graphs, where meaningful answers often depend on understanding complex connections and relationships within the data. To address this, we need an approach that captures the nuanced insight an expert would apply when querying a knowledge graph, combining structured retrieval with context- aware reasoning.

14.3 Schema-Based approach for querying knowledge graphs

We have seen that RAG can sift through millions of documents to provide a pertinent and rich answer, provided that the retriever can deliver a comprehensive contextual picture to the LLM.

This is a significant achievement, given the scale of the problem—navigating through millions of documents—and the fact that a machine can automate

this process.

Let’s take a moment to scale this problem down to a more manageable size, say a dozen documents. Imagine that we ask a question to a group of experts who apply a “human-powered” RAG-like approach.

In practice, this means they would review the documents, marking any paragraphs that resonate with the question—those that contain similar content or pertain to the same domain. From these, they would select a dozen paragraphs that are most relevant to construct a context. Once they have their context, they would set aside everything else they read and attempt to answer the question using only the selected pieces of information and their common sense, even making assumptions if necessary.

This process can hardly be considered impressive, and the individuals involved would hardly be regarded as true experts, as their approach lacks the depth and rigor that genuine expertise demands.

What would an expert do in this scenario? Imagine we have a knowledge graph, and we need to answer a specific question. The first step is to understand the schema, which acts as the blueprint of the graph. For example, if the question involves people, it will likely involve a node labeled “Person”.

Understanding the schema is crucial because it allows us to convert specific requests like “show me the red Camaros seen in this area at that time” into a precise traversal. This traversal might involve entities and relationships such as “Person - Owns_car – Car - Captured_by ANPR (Automatic Number Plate Recognition)”.

The traversal can then be refined by applying constraints, such as requiring the “Car” to be red, the model to be Camaro, and the ANPR camera to be located within a specific area and time frame. Deciding which nodes and relationships to select, as well as how to apply these constraints, is guided by the schema.

Understanding the schema allows experts to write formal queries that pinpoint the specific information they are seeking, even amidst vast amounts of data.

14.3.1 Understanding and Leveraging Graph Schema

A graph schema is a structured representation of the domain as envisioned by the data modeler. It encompasses various entities and the relationships between them, providing a map of how different pieces of information are interconnected.

Understanding this schema is like understanding the layout of a city before navigating through it. It allows us to identify where specific types of information are stored and how they are linked. This understanding is crucial for formulating precise queries that can traverse these relationships to extract meaningful insights.

The diagram in figure 14.2 shows an example of the mental process an expert would follow when translating a natural language question into a formal Cypher query. This translation process demonstrates how a solid understanding of the schema - knowing which nodes exist, their properties, and how they're connected - is fundamental to constructing effective graph queries.

Figure 14.2 Translation Process from Natural Language to Cypher Query. This diagram illustrates the step-by-step process of translating a natural language query ("Red Camaros spotted in this area at this time") into a formal Cypher query. The translation occurs through three main stages: (1) parsing the natural language request into semantic components, (2) mapping these components to schema elements (Vehicle, CameraEvent, and ANPRCamera nodes with their relationships), and (3) constructing a formal Cypher query with the appropriate traversal patterns and constraints. The diagram shows how domain concepts are systematically transformed into graph database operations, demonstrating the bridge between user intent and executable queries.

When we receive a query like “show me the red Camaros seen in this area at that time”, the initial step is to map the concepts mentioned in the query (e.g., red Camaros, area, time) to their corresponding nodes and relationships within the graph schema. This involves identifying:

Entities: For instance, "red Camaro" maps to nodes labeled "Car" with properties such as color and model. Relationships: The query implies relationships like "Person owns Car" and "Car captured by ANPR”. Constraints: These are the specific conditions that refine the traversal, such as the color being red, the model being Camaro, and the ANPR camera's location within a specified area.

This results in constrained traversals, which address questions like, "Among all these entities (cars, people, cameras), connected in this specific way (e.g., OWNS_CAR, CAR_CAPTURED_BY_ANPR), which ones possess the characteristics I'm interested in?"

Constrained traversals are the fundamental building blocks for constructing formal queries of any complexity. These traversals can be composed in various ways to build sophisticated queries that navigate intricate data relationships.

For example, we might start with an initial constrained traversal—possibly with aggregations— to identify specific nodes or subgraphs, which can then serve as starting points for further exploration in subsequent stages. This compositional approach allows you to address complex questions that require multiple layers of exploration and refinement.

Ultimately, constrained traversals enable the translation of natural language intent into structured queries that a graph database can execute, effectively bridging the gap between human language and machine-readable data.

14.4 Think like an expert: leveraging metadata for enhanced querying

In the previous section, we explored how experts approach querying a knowledge graph by leveraging their understanding of the schema to construct precise, context-aware queries. This process, grounded in constrained traversals, serves as a blueprint for extracting specific insights from complex data structures. However, this expert-based approach is not only the domain of human experts. With the advancements in large language models, it is possible to translate this expert reasoning into a set of tasks that an LLM can perform, effectively allowing the model to think like an expert.

Expert reasoning, as we have outlined, involves a systematic approach to understanding and navigating the schema of a knowledge graph. LLMs, being trained on vast amounts of text data, can recognize patterns and making inferences that closely resemble expert reasoning.

They can parse and interpret the schema, understanding how different entities and relationships interconnect, and use this understanding to inform their reasoning process. This combination of reasoning and schema comprehension allows LLMs to generate more accurate and contextually appropriate queries, mirroring the expert’s ability to navigate the graph's structure.

In the following sections, we'll delve into the core steps involved in answering questions using knowledge graphs. These steps encompass planning, generating the query, retrieving results, and presenting them in a coherent manner.

First, we'll explore how to understand the intent behind the question. This step is crucial as it helps in routing the request to specialized pipelines, if necessary, ensuring that the most appropriate method is employed to answer the query. Next, we'll discuss how to extract and utilize metadata—essentially an enriched or annotated schema—that provides the LLM with a deeper understanding of the graph. This metadata acts as a guide, enabling the model to navigate the knowledge graph more effectively. We'll also examine techniques to mimic expert reasoning within the LLM. While there are intrinsic limitations in an LLM's reasoning capabilities, we’ll explore strategies to overcome some of these challenges, bringing the model's performance closer to that of a human expert.

Additionally, we'll explain how to integrate a summarization step to highlight the most important parts of the answer. This process not only helps in distilling the key information but also enables the inclusion of valuable insights.

Figure 14.3 represent an overview of our system's architecture and the interaction between its core components. As we progress through this chapter, we'll examine each component in detail, using this diagram as our roadmap to understand how the elements work together to process and transform user questions into meaningful results.

Figure 14.3 Overview of the system highlighting the main components: Intent Detection, Schema Extraction, Query Generation, Query Execution, Visualization and Summary generation. The system processes user questions and selection inputs, while supporting error feedback during query execution.

Before we move on, it's important to recognize a significant paradigm shift in

how we approach question-answering systems: shifting the focus from generating an answer to asking the right question.

In traditional RAG systems, the process involves building a context around a question so that a grounded answer can be generated from the combination of the question and the context. The emphasis is on how to craft an answer that is both relevant and accurate based on the provided context.

However, in the approach we're exploring, the focus shifts toward converting questions into formal queries. Here, the challenge is not just in answering the question but in properly formulating the question so that it can be directly used to extract factual data from the knowledge graph. While both questions and queries aim to uncover information, queries are essentially formalized questions designed to interact with the structure of the knowledge graph, ensuring precise and factual retrieval of data.

14.5 Intent detection: understanding user expectations

Let's dive into our system architecture by exploring the first component as depicted in Figure 14.4: the intent detection module.

Figure 14.4 The Intent Detection component analyzes user inputs to determine how to appropriately handle and classify the user's question, representing the first critical step in the request processing.

Why do we need to understand the intent to properly answer a question and why should we do it right at the beginning?

To answer the first question, we can revisit our “Red Camaros” example. How should the answer be presented to the user? One option is to create an investigation board similar to those seen in crime movies, where we can add nodes and relationships. In this case, we would include not only the nodes representing the cars but also the connections to the ANPR cameras that detected each vehicle.

Alternatively, we could use a table representation. This could involve returning a list of cars that includes the plate number, camera location, and time of capture. Another option might be to draw a map, which would require the locations of the camera nodes. Each of these presentation methods has its own advantages, depending on the user's needs and preferences.

To answer questions like these, we need to understand the user's intent when

they pose them. Grasping the user's intent can help identify the appropriate type of presentation and the right pipeline or chain to choose.

We perform this step at the beginning for a couple of reasons. First, we aim to mimic expert reasoning, and understanding the intent behind a question is a fundamental first step that all experts take to provide satisfactory answers to their users.

Secondly, this classification step relies on semantic understanding, a task that large language models excel at. Since it is a relatively straightforward task, it is easy to follow and debug, providing a solid foundation for our question- answering system.

14.5.1 Classifying by Visualization Type

To build a classifier for intents we must decide the set of classes any question should fall in. This set is not written in stone so that we could revisit it by adding or merging classes as we develop the system and as the system evolves over time in production.

Suppose that our questioning answering system is feeding a front-end layer capable of presenting:

graphs as an interactive canvas with nodes and relationship laid out tables with basic sort and search functions chart or plots, interactive maps in a GIS-like environment.

Figure 14.5 Intent detection system architecture for data visualization requests, showing how user requests are mapped to appropriate visualization formats (graph, chart, table, or map).

In this case we could start by inferring the type of representation the user would prefer to receive, this will be our intent detection, or at least the first version of it. The prompt we can use to implement such a classification task is described in Listing 14.6.

Listing 14.6 Intent detection by available presentation types

Given a Text delimited by triple backticks representing a user qu

Select one of the possible output in the following list:

"graph", "table", "chart", "map".

The first step is to understand if the user explicitly asks for a For example, if the user asks for graph elements such as paths or

If the output type is not explicit it is usually graph: - "table", only when the user asks about aggregation, ordering, a - "chart", if the users asks for plotting distributions; - "map", if the user asks for showing locations, places or other

If you do not understand the output from previous cases the outpu

Here you can find some examples:

Example: Location of last 10 narcotics related crimes Output: {"type": "map", "reason":"type is map because it involves

Example: Distribution of crimes over time Output: {"type": "chart", "reason":" type is chart because a dist

Example: "Maximum, minimun and average number of crimes per distr Output: {"type": "table", "reason":" type is table because aggreg

Example: "People involved in or related with crimes investigated Output: {"type": "graph","reason":" type is graph because entitie

The output must be in JSON format. Do not explain the result. ###Text:```{{question}}``` ###Output:

The classification prompt is divided in three sections:

The task definition: The first sentence defines the task very generically and other details are added with progressively with subsequent sentences. We add a clear bias toward the graph representation and then we move on describing the expectation for each class. Few shots section: we provide a set of examples including the expected responses. The format of the response is also defined here since the output structure is simple and it easier to be shown than to be explained. The examples are in the context of the law enforcement domain and the response includes a reasoning step beside the actual answer. The actual question: finally, we define insert the question following the

format we provided in the examples section and leave the LLM to complete the text with the answer.

The listing 14.7 contains examples of questions and classifications results obtained using the prompt in listing 14.6

Listing 14.7 Intent detection examples and results

###Text: What is the location of the latest shooting? ###Output: {"type": "map", "reason": "type is map because it invo

###Text: How many crimes were committed per month in 2020 compare ###Output: {"type": "chart", "reason": "type is chart because a d

###Text: Who are the main suspects in the recent string of burgla ###Output: {"type": "graph", "reason": "type is graph because ent

###Text: How many traffic fatalities occurred last year compared ###Output: {"type": "table", "reason": "type is table because agg

As we see in the examples, we have done a decent job, and it seems that we are now able to label each question with an intent that matches the type of output we are able to present. While the role of the “type” output is clear, one may wonder what the purpose of the “reason” field is. We will see in the next sections how generating extra tokens affects the overall response of the LLM and how to exploit this knowledge to improve the quality of the results. For now, we are not planning to use this “reason” field in any downstream processing, but we will use it for “debugging” purposes.

Let’s have a look to what characteristics a good classification prompt should have:

Clear Instructions: The task is explicitly stated at the beginning, explaining what needs to be done. Categories Defined: The categories are clearly defined, so the model knows how to differentiate between them. Examples Provided: By showing a few examples, the model gets a clear idea of how the classification should work.

Boundary Cases: The prompt includes examples that sit near the boundaries between categories to help the model understand the nuances. This is especially important in classification, where similar inputs might belong to different categories. Expected Output Format: This guides the model on how to present the answers, ensuring consistency. Fallback Options: If applicable, include a special category as a fallback, and instruct the model on when to use it. This can prevent forced or incorrect classifications.

Our prompt already meets most of these criteria, take it up a notch by focusing on Boundary Cases and Fallback Options. The easiest way to get more quality samples on the “boundary” is to observe the misclassification during normal usage.

Let’s take this example: “What are the most common types of organized crime involved in human trafficking?”. We may get this question classified as “graph” because “entities and relationships are implied” even if it requires some aggregation and summarization.

On one hand, the question is related to law enforcement, which often implies a need for visual representations of entities and relationships (e.g. networks of organized crime groups). On the other hand, the question asks for "common types" of organized crime, which suggests that some aggregation and summarization is required.

In this case, the system classified it as a “graph” because the relationship between the different types of organized crime and human trafficking is more important than simply listing out the common types, but it is not a clear-cut classification. There is not a single correct and undebatable answer here, but these are exactly the type of questions you want to have in the 'Few shot section', they will help shape the nuanced behavior of your system.

Regarding the “Fallback Options” criteria we took some liberty. We put a significative bias towards the “graph” class, which effectively serves as a “catch-all” class. This is a very reasonable choice; we are querying a graph database after all.

14.5.2 Is it data, documentation or just complaining?

To effectively empower non-technical domain experts to leverage the power of the knowledge graph, our solution must consider the broader spectrum of questions users may ask. While the intent detection approach we've discussed so far is well-suited for handling data-driven queries, we need to also address more generic, open-ended inquiries that may not have a straightforward mapping to the graph database.

Consider, for example, the following questions:

Is it possible to export data from the system into a CSV file? How do I use the system to assess the risk level associated with a suspect? The system is too slow and keeps freezing. What can be done about it? It would be great if. The system had [feature XYZ]. Can this be added?

These questions are as legitimate as they are unexpected. While they may not directly involve querying the knowledge graph, they reflect real user needs and expectation.

Intent can, of course, be much broader than just data retrieval. Users interacting with a system like this are not limited to purely data-driven queries; they may also seek help with system functionalities, request explanations, or even provide feedback. Given this, it's reasonable to expect a variety of question types, which can generally be grouped into three main categories:

Data-Related Questions: These are the questions we address so far, involving querying specific information from the knowledge graph. System-Related Questions: These are about how the system operates or its capabilities, exporting data and asses the risk being two examples Feedback and Complaints: Users may also express frustrations or make request for improvements, “the system is slow” or “I need feature XYZ”

Among system-related questions we can further distinguish between two important subcategories:

Documentation-Related Questions: These are questions that can be resolved consulting user documentation, user manual or help sections. “Can I export to CSV?”, “Can I change the results font color” are typical examples. Schema-Related Questions: These are more technical questions related to how the knowledge graph is structured and the decision made during data modelling. “What types of relationships exist between the ‘Person’ and ‘Organization’ entity types?” is a clear example but also the way you “asses the risk for a suspect” depends on an understanding of the underlying schema or metadata to provide a meaningful answer.

We can broaden the scope of the intent detection as depicted in Figure 14.6 to encompass a full range of possible intents.

Figure 14.6 Classification of system-related questions in the intent detection system, showing how requests are routed to either documentation (for system functionality and feature questions) or schema (for knowledge graph structure queries). The system also identifies feedback and issues as a separate category for user complaints or enhancement requests.

With the range of potential user intents identified, the next critical step is crafting effective prompts for the LLM to handle this classification task. A well-structured prompt can guide the model in not only detecting the type of question but also routing it to the appropriate pipeline—whether it's a data- related request, a system-related question, or user feedback.

The listing 14.8 shows how this broader intent detection can me implemented

Listing 14.8 Broader intent detection

You are an AI assistant tasked with categorizing questions relate

1. **Data-Related:** Questions that require direct access to or k 2. **System-Related:** Questions related to the system's function 3. **Feedback/Complaints:** Questions that are either not answera

**If the question is classified as "System-Related,"** you will f

- **Documentation-Related:** Questions that can be answered by re - **Schema-Related:** Questions related to the structure of the k

**Task:** For each question below, first classify it into one of

**Example Questions and Expected Output:**

1. **Question:** "Is it possible to export data from the system i

**Answer:** { "question": "Is it possible to export data from the system "category": "System-Related", "subcategory": "Documentation-Related", "reason": "The question asks about the capability of the sy }

2. **Question:** "How do I use the system to assess the risk leve

**Answer:** { "question": "How do I use the system to assess the risk lev "category": "System-Related", "subcategory": "Schema-Related", "reason": "The question involves assessing risk, which requ }

3. **Question:** "What permissions do I need to access restricted

**Answer:** { "question": "What permissions do I need to access restricte "category": "System-Related", "subcategory": "Documentation-Related", "reason": "The inquiry is about user permissions, indicatin }

4. **Question:** "Why isn’t the system responding to my queries?"

**Answer:** { "question": "Why isn’t the system responding to my queries? "category": "Feedback/Complaints", "reason": "This question expresses a complaint about system }

5. **Question:** "What types of data visualizations can the syste

**Answer:** { "question": "What types of data visualizations can the syst "category": "System-Related", "subcategory": "Documentation-Related", "reason": "The question seeks information on available feat }

**Begin classification:** {{question}}

The classification prompt is divided into three key sections: task definition, a few-shot section, and the actual question. Additionally, the expected output format includes a “reason” field, which is crucial for identifying potential misclassifications and understanding the model's reasoning as we can see in listing 14.9

Listing 14.9 Misclassification example

{ "question": "How often is the knowledge graph updated to reflect "category": "Data-Related", "reason": "The question concerns the frequency of updates within } #A

{ "question": "How often is the knowledge graph updated to reflect "category": "System-Related", "subcategory": "Documentation-Related", "reason": "The question is asking about the update frequency of t } #B

In this example, the question is related to how frequently the knowledge graph is updated. While it's possible to infer this information by analyzing

data, such as the creation or update timestamps of nodes, this approach would not be appropriate for a law enforcement knowledge management system. Attempting to determine the update frequency by analyzing the data may not provide an accurate conclusion, as the data could reflect how the information happens to be entered rather than how the updates are scheduled.

Larger models can detect that this question pertains to system functionality and refer to the documentation, labeling it as "System-Related" under "Documentation-Related." Smaller or quantized models, however, may misinterpret the question as concerning the data itself, and incorrectly classify it as "Data-Related."

The content of the “reason” field in both cases provides valuable insight. It suggests that the smaller model lacks important context. This signals that we should enhance the task definition section by including relevant background information, helping guide the model to the correct classification.

With the misclassification example in mind, it's clear that the quality of the prompt is essential for ensuring accurate request classification, especially when handling nuanced distinctions. To assess how well our prompt performs, we can now evaluate it based on the criteria we’ve already established.

Clear Instructions: The prompt provides clear and detailed guidance on how to classify user requests, though there is room for slight improvement in providing extra context if needed. Categories Defined: The three main categories, along with the two subcategories, are well-defined and distinct, making it easy for the model to classify questions appropriately. Boundary Cases: While the examples help clarify the boundaries between categories, certain borderline cases may still require additional examples or clarification to avoid ambiguity. Examples Provided: The five examples of questions, along with their respective classifications, are highly useful in illustrating the process and aiding the model in making accurate decisions. Expected Output Format: The expected response format, specified as a structured JSON, is clearly laid out and ensures consistent, reliable output.

As we consider the design of our prompt for intent detection, it's essential to weigh the trade-offs between using a single broader prompt and implementing a multi-stage classification approach, a consideration that applies broadly to classification tasks in general.

Opting for a single broader prompt can be beneficial if simplicity and reduced management overhead are priorities. This method streamlines the classification process and can be effective when the model's accuracy is acceptable, even for more complex tasks. It allows for quicker implementation and easier maintenance, particularly in environments where rapid deployment is necessary.

On the other hand, a multi-stage approach may be more suitable if accuracy and flexibility in classification are more critical or if you anticipate frequent adjustments to how questions are categorized. This method provides more granular control over the classification process, enhancing the handling of complex questions and edge cases. By breaking the classification into stages, you can refine each step according to specific requirements, ultimately leading to more robust and reliable results.

In this scenario, it is advisable to start with a single broader prompt. This approach maintains simplicity while allowing you to evaluate the prompt's effectiveness in practice. As the complexity of requests increases or new requirements emerge, transitioning to a multi-stage classification system can be considered.

Postponing the decision to split the prompt into multiple stages also provides the opportunity to gather real-world examples. These cases can be invaluable in refining the classification process, ensuring that the eventual multi-stage approach is tailored to meet the specific needs of your application.

14.6 From schema to LLM-ready context

Before diving into how metadata can enhance querying in knowledge graphs, let's briefly recall what we covered earlier. We discussed how domain experts leverage the schema to map out the entities, relationships, and constraints within the graph, essentially using the schema as a guide for constructing

precise queries. This expert approach, rooted in an understanding of the graph's structure, enables efficient traversal and retrieval of information.

As shown in Figure 14.7, we're focusing specifically on the schema extraction phase of our pipeline. This critical step forms the foundation for transforming raw schema information into formats that LLMs can effectively process and understand.

Figure 14.7 Schema processing pipeline, highlighting the transformation of database schema into LLM-compatible formats. The diagram illustrates how raw schema structures are processed through Schema Extraction to create representations that LLMs can effectively process.

Now, we will build on this foundation by exploring how metadata, an enriched layer of schema information, can empower LLMs to replicate this expert reasoning, ultimately enabling non-technical users to query knowledge graphs effectively. Here, the enrichments are those that would be beneficial to experts themselves, as anything that makes the schema more

understandable or actionable for an expert can be exploited by our LLMs.

14.6.1 Schema Extraction and Representation

The challenge we must face is to extract the schema from the knowledge graph and convert it into a form that can be processed by a language model. We can then leverage that schema information to help the language model better understand the question and produce a proper Cypher query.

In addressing this challenge, we encounter two distinct problems. The first is to extract the schema itself, while the second is to make it interpretable for a language model. At first glance, we could solve the first problem by simply invoking the apoc.meta.schema[1] function, which computes the schema based on the graph’s current structure as shown in listing 14.10

Listing 14.10 Response format for apoc.meta.schema

[

{ "label": "[LabelName]", "properties": { "[PropertyName1]": { "type": "[PropertyType1]", "mandatory": [true|false], "unique": [true|false] }, "[PropertyName2]": { "type": "[PropertyType2]", "mandatory": [true|false], "unique": [true|false] }, ... }, "relationships": [ { "type": "[RelationshipType]", "target": "[TargetLabelName]", "properties": { "[RelationshipPropertyName]": { "type": "[RelationshipPropertyType]", "mandatory": [true|false], "unique": [true|false] }

} }, ... ] }, ... ]

However, a closer look reveals that the schema provided by this method is a Technical Database Schema, which includes many details irrelevant to our needs, such as:

Helper or Administrative Nodes: Nodes that are included for database management, versioning, or system purposes but don't add to the conceptual model. Technical Metadata Properties: Attributes related to the database’s internal management or IDs used only internally (as opposed to meaningful domain-specific IDs). Unnecessary or Redundant Type Labels: If certain labels or types are only used to tag nodes temporarily or to handle specific database operations, such as ToBeProcessed, that do not define the essential roles of entities in the domain. Unused Relationships: Relationships that are rarely or never used but may still appear in a full technical schema extraction. Unused Properties: Like unused relationships, properties that were initially defined in the schema but are seldom or never populated with data.

These extraneous details clutter the schema with elements that are useful for database management but do not contribute to the core understanding of the domain. To address this, we need a conceptual knowledge graph schema, a distilled, meaningful structure that focuses only on the entities and relationships that convey the essential model of the domain.

The conceptual schema serves as a simplified, domain-relevant subset of the technical schema, stripping away technical complexities and irrelevant metadata. Instead of presenting every relationship, property, and node type, it includes only the logical entities and core relationships that define the structure and essence of the domain. By removing auxiliary elements, the

conceptual schema prioritizes clarity and usability, capturing the logical relationships and characteristics that experts rely on when reasoning about the graph.

This simplification is crucial for several reasons:

Alignment with human reasoning: The conceptual schema reflects how domain experts understand the knowledge graph, using only the relevant entities, relationships, and properties to answer domain-specific questions. This aligns the schema more closely with the natural language model’s approach to reasoning, making it easier to map questions accurately to graph elements. Reduced cognitive load for LLMs: Language models, particularly large language models (LLMs), are more efficient when given focused, relevant data rather than the full, complex structure of a technical database schema. By providing a streamlined conceptual schema, we allow the model to concentrate on meaningful information, increasing its ability to generate accurate and relevant Cypher queries without being distracted by extraneous details. Minimized risk of query errors: Technical schemas often contain implementation-specific elements, redundant labels, or metadata that could confuse the model, potentially leading to errors in query generation. The conceptual schema eliminates this noise, reducing the likelihood of misinterpretations that could result in incorrect queries. Enhanced model interpretability: A conceptual schema presents the knowledge graph in a more human-readable form, aligning it with how LLMs are trained to interpret text. By focusing on the essential model of the domain, the conceptual schema captures the structure that LLMs can most effectively leverage to infer intentions and mappings, which is key to generating meaningful responses.

To transition from the raw output of apoc.meta.schema to a conceptual schema, some human intervention is necessary. One approach is to describe the conceptual model manually, effectively “curating” the schema; alternatively, we could define a skip list to filter out unneeded elements from the APOC results, thereby distilling the technical schema down to its conceptual core.

To make the conceptual schema interpretable for a language model, we must represent each node class and relationship in a clean, structured format that focuses on the essentials. This approach ensures that the LLM has a clear understanding of core classes and connections, as well as their properties and types, without the distractions of technical details.

Figure 14.8 illustrates the key steps in transforming a technical Neo4j schema into an LLM-friendly conceptual representation.

Figure 14.8 The technical schema obtained through APOC call is filtered so it is reduced to the conceptual graph schema representation. The conceptual schema is then mapped into a textual format that can be effectively understood by LLMs.

In a narrative schema representation, nodes and relationships are described in greater detail and natural language, including rich context and examples (see Listing 14.11). While this format is useful for human readers who need a thorough understanding, it can be more challenging for a language model to process when the goal is efficient query generation.

Listing 14.11 Narrative schema representation

**Nodes:**

1. **Vehicle**:

- **Properties**: `color`, `make`, `model`, `style`, `plate_n - Node example: - (:Vehicle {make: "Toyota", model: "Camry", style: "Sedan", plate_number: "XYZ123"})`

**Relationships:**

1. **OWNED_BY**: Represents the relationship of ownership between

- Example: - (:Vehicle {plate_number: "XYZ123"}) -[:OWNED_BY]-> (:Person {name: "Alice"})

This narrative representation provides a detailed understanding of the entities and their relationships, but as the schema grows, this approach can be verbose and less systematic. For a language model, this kind of representation may introduce some challenges in parsing and understanding, particularly when the schema becomes large and complex.

In contrast, a more LLM-friendly format distills the schema into a concise, consistent structure that focuses on key entities and relationships, stripping away extra verbiage while retaining the essential details, as we can see in listing 14.12

Listing 14.12 LLM-Friendly schema representation format

Nodes: (:Vehicle {

color: STRING, make: STRING, model: STRING, style: STRING, plate_number: STRING })

Relationships: (:Vehicle)-[:OWNED_BY {since: DATE}]->(:Person)

This structured representation highlights just the essential components, i.e. entity names, properties, and relationships, without extraneous descriptions or complex formatting. It is easy for the LLM to process because it uses a

standardized syntax that aligns well with both the entity types and their attributes. The relationships are described clearly with property types and minimal but necessary contextual information.

14.6.2 Enriching Schema with Descriptive Annotations

In the previous section, we introduced schema descriptions as a way to bridge the gap between the knowledge graph's structure and the LLM’s ability to generate accurate, meaningful queries. However, relying on schema structure alone can often hinder this process in several ways.

Consider, for instance, if the user asks to "find all black vehicles involved in a crime." The LLM might naively translate this to Vehicle.color == "black", assuming "black" is stored in a readable string format. However, if the database actually uses abbreviations like BLK for "black," this query will miss all relevant records, giving the impression that no black cars were involved in any crimes.

Another similar issue may arise with relationship ambiguity. Suppose the knowledge graph includes both the COMMITTED and CO_OFFENDS_WITH relationships. If the user asks to "list all suspects who have been in multiple crimes with another person”, the LLM has no clear basis for determining which relationship to use. Is the question about those directly involved (COMMITTED) or individuals with co-offending histories (CO_OFFENDS_WITH)? Lacking a clear distinction, the LLM may choose incorrectly, amplifying the inherent ambiguity of natural language and leaving the user feeling as if they’re communicating with a system that doesn’t fully grasp their request.

To tackle these issues, we can approach the solution by mimicking the actions an expert would take in similar situations. When faced with unfamiliar data, an expert would begin by familiarizing themselves with the knowledge graph’s structure, consulting any available documentation and data dictionaries. This careful review helps them understand the correct paths to take through the data and allows them to set constraints with precision, avoiding the pitfalls of ambiguous or misinterpreted terms.

As part of this familiarization process, experts may create a kind of "cheat

sheet" to capture key insights—such as terminology, abbreviations, and relationship meanings—that aid in accurately constructing queries. For example, they might note that "BLK" is used instead of "black" in the vehicle color property, or that specific relationship types like COMMITTED or CO_OFFENDS_WITH capture involvement in criminal activities from different angles. By capturing this information, experts avoid mistakes and ensure their queries align with the knowledge graph structure and semantics.

We can replicate this cheat sheet building process by systematically annotating the knowledge graph’s schema, documenting node classes, relationship types, and properties with descriptions that capture these nuances. This annotated schema becomes a guide for the LLM, allowing it to make more informed and contextually accurate query translations. An example of such an annotated schema representation is shown in Listing 14.13

Listing 14.13 Annotated Schema representation format

Nodes: /* Represents a vehicle involved in various incidents or owned by (:Vehicle {

color: STRING, /* Color of vehicle, BLK, GRY, SIL, WHI, etc*/ make: STRING, /* Manufacturer: BMW, BUIC, CADI, CHEV, etc */ model: STRING, /* Model of the vehicle: IMP, ALT, SON, SEB, C style: STRING, /* Body style: SUV, SEDAN, etc */ plate_number: STRING /* Vehicle license plate */ })

Relationships: /* Ownership relationship from vehicle to person, with start date (:Vehicle)- [:OWNED_BY {since: DATE /* Date ownership began, ISO format */}] ->(:Person)

With these annotations as inline comments, the LLM has access to detailed descriptions that guide it in accurately interpreting the schema, leading to more precise queries that reflect the true data structure.

14.6.3 A Practical Approach to Schema Representation

Building on the previous discussions we now propose a practical approach to

streamline the process of refining and annotating the knowledge graph schema. This approach leverages a YAML configuration file to manage the output of apoc.meta.schema, distill it by skipping irrelevant elements, and add rich descriptions to help LLMs understand the schema more effectively.

The YAML file serves as a central configuration point where users can specify which parts of the schema to exclude from processing, as well as provide detailed descriptions for key entities, relationships, and properties.

The basic structure of the YAML configuration file includes two main sections: skip and descriptions.

Skip List

The skip section (see listing 14.14) allows users to filter out certain classes, relationships, or properties that are deemed to not be part of the core conceptual knowledge graph schema as we described it.

Listing 14.14 Schema Config – Skip section

schema:

skip: classes: [] # Classes to skip relationships: [] # Relationships to skip properties: [] # Properties to skip

Descriptions

The descriptions section is where detailed annotations are added for various schema elements. Here, users can describe the purpose and semantics of classes, relationships, and properties, helping the LLM interpret the graph more effectively. These descriptions can be as granular as needed, ensuring that both the LLM and any human users interacting with the schema have a clear understanding of its structure. The schema config description section, as seen in Listing 14.15, allows users to provide these detailed annotations.

Listing 14.15 Schema Config – Descriptions section

descriptions: classes: Class1: "Description of class 1" relationships: Rel1: "Description for relationship type 1" properties: Class1: property1: "Description for Class1.property1" property2: "Description for Class1.property2" [...] Rel1: property1: "Description for relationship property

The benefit of this approach includes:

Customization: Users can easily adjust the schema representation by editing the YAML file to include only the necessary components and tailor the descriptions to their needs. Maintainability: The YAML file serves as a centralized, human-readable configuration that can be easily updated as the schema evolves, ensuring that the schema remains aligned with the needs of the domain and the LLM. Scalability: As the knowledge graph grows, this approach allows for a manageable and scalable way to handle schema modifications, ensuring that the LLM can keep up with new data and relationships without becoming overwhelmed by unnecessary complexity.

An example of the output from this schema configuration approach is shown in Listing 14.16.

Listing 14.16 Schema Description - Output example

### Graph Schema Overview

#### Node Types (:Vehicle /* Represents a vehicle involved […] */ {

color: STRING, /* Color of vehicle, BLK, GRY, SIL, WHI, etc*/ make: STRING, /* Manufacturer: BMW, BUIC, CADI, CHEV, etc */ model: STRING, /* Model of the vehicle: IMP, ALT, SON, SEB, C style: STRING, /* Body style: SUV, SEDAN, etc */ plate_number: STRING /* Vehicle license plate */ })

#### Relationships (:Vehicle)-[:OWNED_BY /*<description>*/ {since: DATE /* <descript

14.7 It’s time to think: understanding LLM reasoning

Now that we have a structured understanding of user intent and an LLM- friendly graph schema, we’re almost ready to tackle one of the most challenging steps: translating text into Cypher queries. The diagram in figure 14.9 illustrates how Query Generation acts as a convergence point, combining processed user intent and schema data, while also leveraging execution error feedback to refine and improve query formulation through an iterative process.

Figure 14.9 Query Generation stage of the system architecture, highlighting its central role in converting user inputs and schema information into formal database queries.

Just as we’ll soon learn how important it is to guide our LLM to 'take its time' with answering, we shouldn’t rush through the prompt either. So, let's begin with a quick look at the foundational workings of LLMs before moving on to craft our queries.

Large Language Models (LLMs) are designed to understand context and generate responses that seem remarkably human by leveraging the immense amounts of text data they were trained on. This enables them to perform well on a variety of language tasks. However, because these models are optimized to generate answers quickly, they can sometimes produce responses that seem plausible but are ultimately inaccurate. When presented with complex or nuanced questions, LLMs may "rush" to a conclusion, relying on shortcuts in the data patterns they’ve observed rather than fully reasoning through the problem.

This “rushed” answer can often seem accurate on a first glance but, upon closer inspection, may reflect inconsistencies or oversimplifications. To tackle this tendency, researchers have explored techniques like Chain-of- Thought Prompting and Scratchpad Mechanisms, both of which help encourage models to "think out loud" before arriving at a final response.

Chain-of-Thought Prompting [2] introduces intermediate steps into the model’s response generation. By structuring prompts that require the model to articulate its reasoning step-by-step, this technique leverages an LLM's pattern-based capabilities, encouraging it to take a more calculated approach rather than jumping to conclusions.

Similarly, Scratchpad Techniques [3] work by embedding intermediate “workings” in the LLM’s output, where the model produces tokens that represent various steps or computations needed for reasoning through complex questions. Both approaches effectively give the model “time to think,” by encouraging it to spend more computational resources on the problem-solving process before generating a final output.

Essentially, these techniques recognize that with a limited number of layers and computation per prompt, the model isn’t always able to adapt the depth of its computations to the difficulty of the problem at hand. By modifying the prompt to “force” token-by-token generation before producing the final

answer, these methods help allocate more computational capacity to each layer, effectively scaling the thought process in response to question complexity.

14.7.1 The Order Matters: Answer-First vs. Reasoning-First

If you remember, we used a similar technique in the Intent Detection phase, where we generated both the question classification and a reasoning field. But there's a twist here.

If an LLM is instructed to first provide the answer, followed by the reasoning behind it, the model may "stick" with the initial answer it has generated. This is because LLMs typically produce text one word (or, more precisely, one token) at a time, effectively committing to their previous choices as the generation progresses.

This can lead to the model generating reasoning that is tailored to support the predetermined response, rather than truly reflecting an impartial, step-by-step thought process.

This consistency-driven tendency has been studied in recent research, which introduced the concept of semantic consistency [4], is in part a reflection of the training data used to develop LLMs. The text corpora that these models are trained on typically exhibit a certain degree of logical coherence and internal consistency - what is said earlier is often used to support what is said later, and radical departures or contradictions are relatively rare. This consistency acts as a guardrail, preventing the models from engaging in what we would perceive as rambling speech.

However, this same consistency can also become a constraint, as the models may be reluctant to acknowledge mistakes or significantly revise their initial positions. The model may feel compelled to generate reasoning that justifies its first answer, rather than openly exploring alternative perspectives that could lead to a different conclusion:

Cumulative Context: Since the model generates tokens sequentially, the reasoning and context build cumulatively. This means that the model's commitment to a particular answer can be influenced by the tokens it

has already generated, reinforcing the idea of output consistency. Error Propagation: If an error occurs early in the token generation process, it can propagate through subsequent tokens, as each new token is generated based on the previous ones. This highlights the importance of careful reasoning and validation at each step.

Figure 14.10 shows an example of how simply inverting the order of reason and answer generation can lead to significant differences in the output.

Figure 14.10 Comparison of two prompt structures for the same path-finding task. Left: The Answer-First approach encourages quick, potentially biased responses with post-hoc justification. Right: The Reasoning-First approach promotes systematic analysis before reaching a conclusion. Note how the JSON structure in each prompt guides the LLM's thinking process.

However, this dynamic can be mitigated to some extent using techniques like beam search, which are commonly employed in LLM decoding. Beam search allows the model to explore multiple potential generation paths in parallel, rather than committing to a single trajectory from the beginning.

To encourage transparent and reliable reasoning from LLMs, it's important to prompt the model to first provide its step-by-step reasoning, followed by the final answer. This structure forces the model to engage in a more thoughtful, deliberative process, as it must justify its conclusion through a balanced exploration of the problem, rather than simply rationalizing a predetermined response.

Postponing the reasoning step until after the answer can make sense in classification tasks. In these cases, we are more interested in understanding how the system justifies its choices, especially when it misclassifies. By leveraging the reasoning provided by the model, we can gain insights into why the classification was incorrect and use that information to provide good examples that help clarify the classification boundaries, as we did in the intent detection section.

14.7.2 Thinking in Queries: From Text to Cypher

Now that we’ve given both the model and ourselves plenty of 'time to think,' it’s finally time to put that thinking into action. Let’s dive into crafting the prompt that will guide our LLM in generating the Cypher queries we need, leveraging what we have built so far.

The prompt is structured in the following way:

A brief task description and question - listing 14.17 A schema definition with annotations - listing 14.18 Intent-dependent requirements - listing 14.19 Examples - listing 14.20 Optional user selection - listing 14.21 Knowledge graph specific annotations - listing 14.22 A reminder of the question - listing 14.23 A reminder of the requirements - listing 14.23

Output format specification - listing 14.24

This structured approach forms a complete pipeline for transforming natural language questions into Cypher queries, as illustrated in Figure 14.11

Figure 14.11 Query Generation Pipeline. The diagram illustrates the complete flow of transforming a natural language question into a Cypher query, showing how three inputs are processed through structured prompt components. The process is organized into three main stages (Input Processing, Context Building, and Final Guidelines) that culminate in a structured JSON.

As we will see, the prompt includes elements we introduced in previous sections as well as new components, so let's dive deeper into each section.

Listing 14.17 Brief task description and question

Your task is to generate a Cypher query for a Neo4j graph databas

The question we need to answer is: <QUESTION> {{ question }} </QUESTION> [...]

Brief task description and question: we begin by introducing the task of translating a question in natural language into a Cypher query, anticipating that we will provide both the question and a detailed description of the schema. We use an HTML-like tag to wrap the user's question, which will help the model clearly understand the boundaries of the question and separate it from the rest of the instructions.

Listing 14.18 Schema definition with annotations

[...]

The knowledge graph has the following schema, which the Cypher qu <SCHEMA> {{ schema }} </SCHEMA> consider the comments as annotations [...]

Schema definition with annotations: In this section, we leverage what we developed during the schema extraction phase. The LLM-friendly format we introduced will seamlessly integrate within another HTML-like tag for this purpose. Using the markdown such as ### Graph Schema Overview or #### Node Types for headings, will help the LLM create a clear structure for representing the schema. The /* comments */ we added to annotate classes, relationships and properties integrate the schema in a natural way.

Listing 14.19 Intent-dependent requirements

[...]

{%if output_type == "graph" or output_type == "map" -%} The result must be a graph so make sure to follow the schema and - Return all the nodes and relationships matched,do not use anony - Aggregate multiple traversals in a single MATCH pattern if poss {%-endif%}

{%if output_type == "table" -%} The result must be a table, i.e. you must should select nodes and {%-endif-%}

[...]

Intent-dependent requirements: we leverage the output of intent detection to incorporate requirements that depend on the identified intent. In the case of a graph or map, we specify how the query should be structured. Specifically, we aim to retrieve not only the nodes that answer the question but also the relationships involved. For example, if I ask about the red Camaros spotted by cameras in a certain area, I want to obtain information about the vehicles, the cameras, and the relationships that connect these elements.

In case of table, we clarify that the tabular format is expected so only property may be returned.

Listing 14.20 Examples

[...]

Use only the provided labels, relationships and properties, do no If you cannot generate a Cypher statement based on the provided s

{{examples}}

You must respect relationship types and directions.

[...]

Examples: this is one of the new components, even though we have already introduced the “few shots technique” when addressing the intent detection classification problem. The idea here is to assist the system in answering foundational questions that can serve as building blocks for more complex queries, while also demonstrating the expected format in a single instance. Examples are a powerful and relatively inexpensive way to enhance the

quality of responses in many situations, although it is best to limit the number of examples to a reasonable amount.

Listing 14.21 Optional user selection

[...]

{%if selection -%} Current selection: {% for node in selection%}

- {{node.label}} node with this properties {{node.properties}} {% endfor %} {%-else-%} The selection is currently EMPTY. If there are references to sele certainly an error and therefore it is not possible to generate a In this case, 'success' should be false. {%-endif-%}

{{annotations.notes if annotations.notes}}

Do not include any explanations or apologies in your responses.

[...]

Optional user selection: This is yet another new component. If the system we are envisioning has the capability to represent nodes and relationships, it is likely that it will also allow users to select nodes. Incorporating the current selection into the prompt enables users to refer to it in their questions. Users may choose to refer to the selected node as a whole or to its properties, as in "Give me the older siblings of the selected person." The selected nodes are represented as a list of items, each containing the node label and its properties. Conversely, if the selection is empty, the prompt clearly states this, allowing the model to highlight the issue to the user if they reference the selection while it is empty.

Listing 14.22 Knowledge graph specific annotations

[...]

{{annotations.notes if annotations.notes}}

Do not include any explanations or apologies in your responses. [...]

Knowledge graph specific annotations: In this section, we allow for the inclusion of clarification notes that are relevant to the specific knowledge graph. These annotations serve to provide context and enhance understanding, helping the model interpret the schema more effectively. By avoiding the integration of these notes directly into the prompt itself, we ensure that the prompt can be reused across different knowledge graphs. This flexibility allows for a more adaptable approach to query generation, as the annotations can be tailored to the specific nuances of each knowledge graph without altering the core prompt structure.

Listing 14.23 Reminder of the question

[...]

The question we like to answer may have some information that is <QUESTION> {{ question }} {{ information }} </QUESTION>

{%if output_type == "graph" or output_type == "map" -%} Remeber the requirements: - Return all the nodes and relationships matched, never use anony - Aggregate multiple traversals in a single MATCH pattern if poss {%-endif%}

[...]

Reminder of the question: we repeat the question near the end of the instructions prompt to reinforce the context and intent, which can lead to a more focused response. The schema definition section can be substantially long, making it likely for the question to be located many tokens away from the end of the instructions and the start of the response.

While language models, particularly those based on transformer architectures, do not inherently prioritize local context over global context—meaning they can theoretically focus on both local and distant tokens—these models can learn to give more weight to nearby tokens if they are more relevant for a particular task.

At this stage, we also provide an optional information field that can be relevant for generating the query. This field can be used, for example, to

report a previous failure of the generated query, including the error message returned by the database. This gives the model the opportunity to review its previous decisions and generate an error-free query in subsequent attempts.

Listing 14.24 Output format specification

[...]

Use the "reasoning" field to explain your plan for the cypher que

Answer only in valid JSON in the following JSON format, nothing e {

"relationships": [...], list of relationships to traverse, empt "reasoning": "...", this is the scratch pad for your reasoning "query": "<Cypher query>", must be a string and a valid Cypher "success": <true/false>, where true means that a Cypher query ( }

Output format specification: we finally specify the format of the response, which must be a valid JSON object so that we can easily process it programmatically. We define each field we expect, provide hints for the expected value types for these fields, and include comments about the type of content we expect the model to fill in for these fields.

14.7.3 Structuring Output for Reliable Query Generation

Let’s dive a bit deeper into the format of the output and the reason why we select each field in that specific order.

We start by asking a list of relationships the model believe it should traverse. We do this because LLMs may sometimes hallucinate relationships that we didn’t mention in the schema description. This can happen because the models are trained on a vast amount of data, and they learn that if something obvious is missing, it may be missing by mistake, for example, because the user forgot to mention it. The models then "believe" that the missing relationship "should be there", even though it is not present in the actual schema. We instruct the model explicitly “use only the provided labels, relationships and properties”, but asking to list the relationships it intends to travers “out loud”, reduces the chance of this type of hallucination significantly.

Depending on the type of the model we may have to tune this part a bit. If, for example, the hallucination of relationships does not happen, we can safely remove this part. On the other hand, if the model commits too early to the relationships to traverse, it may not leverage the reasoning stage fully. If we observe this behavior, we can soften the requirement for this fields by asking to list the relationships that can be “potentially traversed” for example.

We then ask then to generate the “reasoning” field. With this fields we are actively giving the model time to think through the answer by breaking down the problem, define a plan and so on - in other words to think out loud before committing to an answer.

Modern instruction-following LLMs are already trained with concepts like chain of tough, and they can handle this reasoning step reasonably well out of the box. Nevertheless, if for specific use cases you want to drive the reasoning process more precisely, you can add extra details under the “Use the "reasoning" field to explain your plan for the cypher query” section of the prompt, as well as through the examples.

We finally ask the model to produce the cypher query to answer the question. Since we ask the model to generate this field after the “relationships” and the “reasoning” field, it will try to be coherent to the reasoning step and not using nonexistent relationships even if they “look obvious”. By guiding the model through the step-by-step process of identifying relationships, reasoning through the problem, and then formulating the final query, we help ensure the output is well-grounded and aligned with how a human expert would tackle the task. This helps us get results that are more reliable and trustworthy compared to simply asking for the Cypher query directly.

The format of the examples you provide should be conform to the structure of the response we are asking to avoid introducing ambiguity during the response generation, as shown in Listing 14.25.

Listing 14.25 Format for the example section

{%for example in examples%} Example: <QUESTION>{{example.question}}</QUESTION> {

... "query": "{{example.answer}}" {{ '"reasoning":"'+eample.reasoning+'"' if example.reasoning el } {%endfor %}

We use the "..." notation in the examples as a placeholder, indicating the fields where we want to give the model the freedom to generate the content autonomously. If we want to include some key information into the reasoning step but we don't want to influence how the model is generating its own reasoning too much, we can run the prompt asking the question in the example without including the reasoning field in the example itself. This gives us the chance to check how the model would tackle the reasoning step without that guidance, and then we can modify the prompt to add the critical reasoning content we want to include, without radically changing the overall response.

14.8 Response Summarization: From Results to Insights

Having established a robust system that transforms natural language questions into actionable Cypher queries, our focus now shifts to ensuring the results are as accessible and insightful as the queries themselves. While visualizing data as graphs provides a powerful way to explore relationships and structures, it can sometimes leave users seeking clarity or deeper context. This is where summarization comes into play as depicted in figure 14.12.

Figure 14.12 Output generation pipeline, highlighting the system's final stages of data presentation and analysis. The diagram shows how processed query results are transformed into both visual presentations and analytical summaries, demonstrating the dual output approach of visualization and summarization of query results.

The summarization step holds a unique position in our pipeline as the first and only component that has access to the actual data the user is seeking. While the previous steps focus on understanding the question and formulating the right query, summarization bridges the gap between raw data and user understanding.

While graph visualization excels at showing relationships and structures, valuable information often lies within node properties or in the broader context of the results. A well-crafted summary can highlight these key findings that might not be immediately visible in the visual representation. It provides users with a quick overview before they dive into detailed exploration of the graph itself.

Beyond its basic role of presenting results clearly, the summarization step can potentially be extended through post-processing to provide additional context and insights. This capability positions it as not just a final formatting step, but as a crucial component for enhancing the overall user experience.

With these considerations in mind, let's examine listing 14.26, which presents the prompt structure we'll use for summarization to achieve these goals.

Listing 14.26 Summarization prompt

Our user asked this question: <QUESTION> {{ question }} </QUESTION>

To answer the question we decided to execute this cypher query: <QUERY> {{ query }} </QUERY>

The query returned in a graph containing this data: <RESULTS>

{{ records }} <RESULTS>

{%if selection -%} Current selection: {% for node in selection%}

- {{node.label}} node with this properties {{node.properties}} {% endfor %} {%-endif-%}

Your task is to summarise the results we sent to the user with th Please keep in mind that many of the resulting data is actually i

Does the question request analysis of the returned data? If so, i

This is the question again <QUESTION> {{ question }} </QUESTION>

Answer only in valid JSON in the following JSON format, nothing e {

"results_analysis": true|false, Check if the questions contains "reasoning": "...", Scratch pad for your reasoning. include rea "summary": "..." must be a string and a meaningful an factual s }

Let's break down how this prompt is structured to achieve our summarization goals while maintaining flexibility for future enhancements.

The prompt begins by reconstructing the complete context chain, from initial question through query execution to results. Each element is carefully wrapped in HTML-like tags, creating clear boundaries that help the LLM understand the role of each component

The original question provides the user's intent The executed Cypher query shows how we interpreted that intent The query results represent our raw data The current selection (if any) maintains user context

The task description explicitly acknowledges the dual nature of our results presentation: users will see both a visual graph representation and our textual summary. This is crucial because it guides the LLM to focus on complementing rather than merely repeating what's visible in the graph interface.

We explicitly instruct the LLM to filter out irrelevant data, this is a critical step given that our graph queries often return complete paths for visualization purposes. This filtering ensures our summaries remain focused and meaningful to the user's original question.

The prompt introduces result analysis capabilities through a simple question: "Does the question request analysis?"

This approach keeps the analysis tied to user intent rather than generating unrequested insights and provides a foundation for future expansion of analytical capabilities while it maintains a clear separation between summary and analysis functions.

Before specifying the output format, we include a reminder of the original question. This repetition, like our approach in previous prompts, ensures the model maintains focus despite potentially lengthy results sections.

The JSON output format follows our established pattern of progressive generation.

First, we generate a flag to identify whether the results analysis is required by the user, even if only implicitly. This flag is useful for monitoring purposes,

as it compels the model to make an explicit decision about whether to perform the analysis. Additionally, when the flag is true, it significantly influences the subsequent reasoning step.

Next, the reasoning step, which must align with both the prompt and analysis decision, meaning it should include reasoning related to the analysis if applicable.

Finally, we request the generation of the actual summary, including indications for typographic formatting to highlight the content when necessary.

This structured approach ensures our summaries are consistent, relevant, and adaptable to future enhancements. The clear separation of concerns from decision-making through reasoning to final output, makes the component both maintainable and extensible.

The summarization step thus completes our pipeline by transforming raw data into accessible insights while maintaining the flexibility to grow with evolving user needs. Its position as the final touchpoint with our users makes it a critical component for ensuring the system not only answers questions accurately but presents those answers in the most useful way possible.

14.9 Summary

Expert emulation provides a systematic framework for building, improving and extending knowledge graph systems. When facing any challenge - whether implementing new features, fixing issues, or enhancing existing capabilities - we can find solutions by asking "what would an expert do?" and breaking down their approach into implementable steps. A well-structured intent detection system requires two layers of classification. The first layer handles broader query categories (data- related, system-related, feedback) while the second identifies the visualization needs. Converting technical database schemas into LLM-friendly formats involves more than just reformatting - it requires carefully filtering

unnecessary elements, adding contextual annotations, and structuring information in ways that align with how LLMs process and understand data. Prompt engineering for LLMs requires giving them "time to think." This means structuring prompts to encourage reasoning before answers and using techniques like chain-of-thought prompting to improve response quality and reliability. Query generation prompts need several key elements to work effectively: comprehensive schema context, current user selection state, intent-specific requirements, and carefully chosen examples that demonstrate desired patterns. Result summarization works best as a complement to visualization. Rather than repeating what's visible in the graph, effective summaries highlight insights and patterns that might not be immediately apparent visually.

14.10 References

1. Neo4j APOC Library: apoc.meta.schema

https://neo4j.com/docs/apoc/current/overview/apoc.meta/apoc.meta.schem 2. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi,

E., Le, Q., & Zhou, D. (2023). "Chain of Thought Prompting Elicits Reasoning in Large Language Models." arXiv:2201.11903 3. Nye, M., Andreassen, A. J., Gur-Ari, G., Michalewski, H., Austin, J.,

Bieber, D., Dohan, D., Lewkowycz, A., Bosma, M., Luan, D., Sutton, C., & Odena, A. (2021). "Show Your Work: Scratchpads for Intermediate Computation with Language Models." arXiv:2112.00114 4. Raj, H., Gupta, V., Rosati, D., & Majumdar, S. (2023). "Semantic

Consistency for Assuring Reliability of Large Language Models." arXiv:2308.09138

## 15_Building_a_QA_agent_with_LangGraph

15 Building a QA agent with LangGraph

This chapter covers

Implementing the expert-emulated approach Practical example of investigation through question answering How to adapt and improve the system

In this chapter, we'll create a practical application for querying knowledge graphs using large language models. Drawing together the concepts and techniques explored in chapter 14 - visualized in the mental model in figure 15.1 - we'll demonstrate how to build an integrated solution. Using LangGraph as our orchestration framework, we'll show how each stage can be combined into a seamless pipeline. To make this accessible and user- friendly, we will use Streamlit as a front-end interface.

Figure 15.1 Overview of the system architecture introduced in the previous chapter. We'll implement this using Streamlit to handle user input (questions and user selection) and output (visualization and summaries), while LangGraph will orchestrate the core pipeline.

For hands-on learning, this chapter is accompanied by a code repository containing the complete implementation and configuration files, allowing you to easily follow along and reference the code as we progress through the concepts.

15.1 Building the LangGraph pipeline

LangGraph is an innovative library designed for building stateful, multi-actor applications powered by LLMs. This framework is particularly suited for orchestrating workflows that involve complex reasoning and decision-making processes, which are central to our knowledge graph querying pipeline.

To better understand how LangGraph implements these concepts in practice, let's start with a familiar example: a basic RAG system that retrieves relevant documents and generates an answer. While simpler than our expert-emulated architecture, this example demonstrates the core principles we'll build upon.

As shown in figure 15.2, the workflow consists of two main operations: document retrieval and answer generation. What makes LangGraph distinctive is its approach to component communication: rather than passing data directly between components, each one interacts with a shared state, similar to a whiteboard where each agent can read previous work and add their own results.

Figure 15.2 State-based communication between agent functions in LangGraph. The diagram illustrates how agents remain decoupled while communicating through an evolving state object. Each agent function receives and updates the global state independently.

The state begins with just the user's question. The first agent, responsible for document retrieval, reads the question from the state and adds the relevant documents it finds. The second agent can then access both the original question and the retrieved documents to generate an appropriate answer, which is in turn added to the state.

At its architectural core, LangGraph implements these workflows as directed graphs, where each node represents a distinct agent function. These agent functions carry out their responsibilities by interacting with the global state— reading the relevant data and updating it with their results after execution. The edges in the graph determine the execution flow, specifying which node should be executed next. Importantly, LangGraph supports dynamic edge resolution, enabling workflows to branch based on arbitrarily complex logic. For instance, the next node to execute can be chosen based on the output of a preceding agent, ensuring flexibility and adaptability in designing workflows.

This approach ensures that the system maintains a coherent state throughout the workflow while enabling AI system designers to create a spectrum of applications—from router systems where LLMs are just one component among many, to fully autonomous systems where LLMs can determine and shape their own execution paths. This flexibility in flow control makes LangGraph ideal for implementing our expert-emulated architecture, where we need to coordinate multiple specialized phases such as intent detection, schema extraction, and query generation.

15.1.1 System Architecture Overview

With LangGraph's capabilities clear, let's examine how the expert-emulated approach translates into an executable workflow. The diagram shows the core components of our knowledge graph querying system: from initial user input processing through intent detection, query generation, and result presentation.

Notably, this component structure maps naturally to LangGraph's agent/state architecture, where each processing step can be implemented as an agent function that receives and updates the workflow state.

Figure 15.3 illustrates how our expert-emulated components translate directly

into a LangGraph workflow. The directed graph structure shows the natural flow from intent detection through schema extraction and query processing, with each node representing a discrete agent function.

Figure 15.3 LangGraph implementation of the knowledge graph querying pipeline. The solid arrows show the main flow from intent detection through schema extraction and query execution, while dashed arrows indicate conditional paths based on query execution outcomes. This directed graph structure directly maps each component of our expert-emulated approach to a LangGraph agent function.

The LangGraph workflow, though central to our backend system, integrates with several key supporting components. Figure 15.4 provides a broader architectural view, showing how the LangGraph pipeline integrates with supporting components and interfaces with frontend applications.

Figure 15.4 Backend architecture showing how the LangGraph pipeline integrates with supporting components. The Configuration Provider manages prompts and settings, while the Schema Provider handles database schema access. The Question Processing Interface bridges the core pipeline with frontend applications through an event-based API.

At the core of the backend sits our LangGraph workflow, consuming prompts and configuration from the Configuration Provider while dynamically accessing graph schema information through the Schema Provider. These supporting components handle critical setup tasks: the Configuration Provider manages prompt templates and system settings, while the Schema Provider extracts and formats database schema for LLM consumption.

The Question Processing Interface acts as a bridge between the core pipeline and frontend applications. It exposes the LangGraph workflow as an event stream, allowing frontends to track pipeline progress in real-time. This interface processes incoming questions, feeds them through the workflow, and streams state updates and final responses back to the user interface.

This architecture cleanly separates concerns while maintaining the flexibility needed for a conversational AI system. Each component has a clear responsibility: LangGraph handles the core question-answering logic, providers manage configuration and schema access, and the processing interface handles frontend communication.

In the following sections, we'll examine each of these architectural elements in detail, starting with the supporting components that provide configuration management and schema translation services. We'll then explore the state management design that enables effective communication between pipeline stages, followed by the implementation of individual pipeline agents that form the core of our question-answering system. Finally, we'll look at how the pipeline integration layer brings these elements together into a cohesive system that can interact with frontend applications.

15.1.2 Configuring Pipeline Components

Before diving into the configuration details, let's examine how the configuration component fits into our system architecture. Figure 15.5 illustrates the complete system architecture, with special emphasis on the Configuration Provider component

Figure 15.5 System architecture diagram highlighting the Configuration Provider component. The provider manages system configuration and prompt templates needed by LangGraph agents

to process user questions.

The configuration component serves as a centralized repository for storing textual elements that our question-answering system relies on, primarily prompt templates and knowledge graph annotations. This separation of concerns helps maintain clean and maintainable code by preventing these often lengthy text elements from cluttering the main implementation.

Our templates utilize Jinja2 templating language, allowing for dynamic content generation at runtime. By isolating template definitions in the configuration, we create a clear boundary between template authoring and template rendering. The main code can then interact with these templates through a clean interface, remaining unaware of the underlying template composition process.

This approach offers several advantages. First, it simplifies the tuning of knowledge graph descriptions and prompt templates, as all adjustments can be made in a single location without touching the core logic. Second, it provides a structured foundation for future extensions - adding new processing steps or modifying existing ones becomes more straightforward when all text-based resources are organized in one place. Finally, having a dedicated configuration component makes it easier to manage different versions of prompts and annotations, which is particularly valuable during the development and refinement of the system.

Let's examine a concrete example of our configuration structure in listing 15.1.

Listing 15.1 Configuration file example

notes: >

- all POINTS properties are Neo4j Points (`point.distance()` and similar functions work for them) - do not expand ANPRCameraEvent unless you need to connect it to both Vehicle and ANPRCamera - a previous offender or known offender is defined by the fac the node is connected to crimes examples:

- question: Crimes that occurred on March 14th, 2025 answer: MATCH (c:Crime) WHERE c.date starts with "2025-03-1 reasoning: >- To find the crimes that occurred on that date, we lever the <b>date</b> property of the crime node. Since it is formatted as an ISO string, we can use the prefix "2025-03-14" to get all crimes that occurred on Since there is no traversal, no paths are returned [...] - question: Return one male known offender aged 20 to 22 answer: >- MATCH path = (person:Person) -[committed:COMMITTED]->(crime:Crime) WHERE (person.sex CONTAINS 'MALE' AND person.age >= 20 AND person.age <= 22) RETURN path LIMIT 1 prompts:

text_to_cypher: system: >- Your task is to generate a Cypher query for a Neo4j g database, based on the schema definition provided,

that answers the user question. template: templates/text_to_cypher.template intent_detection: template: templates/intent_detection.template generate_summary: template: templates/summary.template

The configuration combines three key elements: operational notes that guide query generation, concrete examples that demonstrate proper query construction, and prompt template references. The notes capture essential domain knowledge and best practices for working with the graph database, while the examples provide question-answer pairs with detailed reasoning that help the LLM understand the expected query patterns. This combination of declarative knowledge and practical examples creates a rich context that guides the LLM in generating accurate queries.

The prompts section references external templates for different pipeline stages (intent detection, query generation, and summary generation), keeping the actual prompt templates separate from their configuration. This separation allows for easier maintenance and version control of both the configuration structure and the prompt content.

Each template can be modified independently without affecting the overall configuration, making it easier to experiment with different prompt variations during development.

Listing 15.2 demonstrates how the configuration component manages both template loading and dynamic content generation.

Listing 15.2 Configuration Component

class ChainConfiguration:

def __init__(self): self.base = Path(__file__).parent self.config = self.load()

def load(self): config_file = self.base / "chain_config.yaml" return yaml.load(config_file.open(), Loader=yaml.FullLoad

def get_prompt(self, name, **kwargs):

system = self.config["prompts"][name].get("system") template_file = ( self.base / self.config["prompts"][name]["template"]) template = template_file.read_text() prompt = jinja2_formatter(template, **kwargs) return system, prompt

def getAnnotations(self, reload=True): if reload: self.config = self.load() return { "notes": self.config["notes"], "examples": self.config["examples"]}

The ChainConfiguration class provides a clean interface for accessing our configuration elements. It offers two main methods: get_prompt for retrieving and formatting prompt templates, and getAnnotations for accessing the notes and examples. This implementation ensures that all configuration elements are easily accessible while maintaining a clear separation between configuration storage and usage.

15.1.3 Schema Translation Service

Let's examine the Schema Provider component and its interaction with the graph database in our system architecture. Figure 15.6 highlights these components and their connections in our question-answering pipeline.

Figure 15.6 System architecture diagram emphasizing the Schema Provider component, which connects to the graph database to extract and transform technical schema information into LLM- friendly formats.

The Schema Provider is a crucial component in our expert-emulated question answering system. In chapter 14, we thoroughly discussed the challenge of bridging the gap between technical and conceptual schemas, as well as the importance of having a conceptual understanding of the data model for effective LLM-based question answering.

While our goal is to automate schema extraction, we face a fundamental challenge that we previously explored: we need access to the conceptual schema, but we can only programmatically access the technical schema.

To solve this, we've developed a configuration-based transformation approach that consists of two key components. The first component is a skip list that identifies which elements should be excluded from the conceptual model. This includes technical nodes and relationships that don't represent business concepts, implementation-specific properties such as internal IDs

and timestamps, and elements that would add unnecessary complexity to LLM prompts.

The second component is a description section that enriches the filtered schema by adding business-level descriptions for nodes and relationships, contextual information about properties, and domain-specific terminology and constraints.

We store this configuration in YAML format, making it easy to maintain and update as the data model evolves. The Schema Provider follows a three-step process to transform the raw technical schema into an LLM-friendly format: it extracts the technical schema using Neo4j's apoc.meta.schema, filters out technical elements using the skip list, and enriches the remaining elements with business descriptions.

Let's examine how this is implemented in practice. First, we need a data model that can represent both technical and enriched schema information as shown in Listing 15.3.

Listing 15.3 Schema provider – data model

@dataclass class Property: #A

"""Represents a node or relationship property with an optiona description""" name: str type: str description: str = None

def __str__(self): """Represents the property as string in the format: property_name:TYPE /* optional description */ """ ret = f"{self.name}: {self.type}" if self.description is not None: ret += f" /* {self.description} */" return ret

@dataclass class Node: #B

"""Represents a node type.""" items = {} #C

name: str properties: list[Property] description: str = None

@classmethod def mk_node(cls, name, value): #D """Creates a new node with the given name and properties from a dictionary.

Args: name (str): The name of the node. value (dict): the node description as returned by `apoc.meta.schema` """ properties = [Property(name=k, type=v["type"]) for k, v in value["properties"].items()] properties = sorted(properties, key=lambda x: x.name)

node = Node(name=name, properties=properties)

for rel_name, rel_value in value["relationships"].items() Relationship.mk_rels(source=name, name=rel_name, value=rel_value)

cls.items[node.name] = node #E

def drop_properties(self, skipProperties): """Drops specified properties from the node.

Args: skipProperties (list): A list of property names to be dropped. """ self.properties = [prop for prop in self.properties if prop.name not in skipProperties] #

def __str__(self): #G """Represents the node as string in the format: (:NodeType /* node class description */ { property_one:TYPE /* property one description */, property_two:TYPE /* property two description */, ... }) """ descr = ("" if self.description is None else f"/* {self.description} */ ")

return ( f"(:{self.name} {descr}{{\n " + ",\n ".join(str(prop) for prop in self.propert "\n})\n" )

This data model provides the foundation for our schema transformation process. The Property class handles individual attributes with their descriptions, while the Node class manages the overall structure and filtering capabilities.

Building on this data model, Listing 15.4 shows how the Neo4jSchema class implements our core schema management functionality.

Listing 15.4 Schema provider main class

class Neo4jSchema:

[...]

def get_schema(self): #A with self.driver.session() as session: result = list(session.run( "CALL apoc.meta.schema({sample:-1})" #B ))[0]["value"]

[Node.mk_node(k, v) for k, v in result.items() if v["type"] == "node"] #C

@staticmethod def apply_configuration(config: dict = None): #D if config is None: #E config_file = Path(__file__).parent / "schema_config. config = yaml.load(config_file.open(), Loader=yaml.FullLoader)["schema"]

items = {node.name: node for node in Node.items.values() if node.name not in config["skip"]["classes"]} Node.items = items #F

for node in Node.items.values(): node.drop_properties(config["skip"]["properties"]) #

for node in Node.items.values(): node.description = (config["descriptions"]["classes"]

.get(node.name)) #H for prop in node.properties: property_description =(config["descriptions"]["pr .get(node.name, {}) .get(prop.name)) #I prop.description = property_description

skip = config["skip"] relationships = {rel_name: rel for rel_name, rel in Relationship.items. if rel.source not in skip["classes"] #J if rel.dest not in skip[“classes"] #K if rel.name not in skip["relationships"] } for rel in Relationship.items.values(): rel.drop_properties(config["skip"]["properties"]) #M

Relationship.items = relationships

def __str__(self): #N ret = ["### Graph Schema Overview\n", "#### Node Types"] ret += [str(node) for node in Node.items.values()] ret.append("#### Relationships\n") ret += [str(rel) for rel in Relationship.items.values()] return "\n".join(ret)

The get_schema method retrieves the technical schema, while apply_configuration handles the transformation process according to our configuration. This implementation ensures that LLMs receive a clean, conceptual view of our data model while maintaining all necessary information for query generation.

In practice, this transformed schema serves several crucial purposes for our LLM components. It enables them to understand the domain model at a conceptual level, generate queries using proper entity and relationship names, and consider business rules and constraints during query construction. This approach creates an effective balance between maintaining technical accuracy and providing LLMs with an accessible, business-oriented view of our data model.

15.1.4 State Management Design

The cornerstone of agent communication in LangGraph is the state object, which serves as a shared memory space where agents can read from and write to. Each agent is responsible for populating specific portions of this state, creating a clear chain of responsibility throughout the pipeline. Let's examine the structure of this state object represented in listing 15.5

Listing 15.5 Pipeline’s Agent State

class AgentState(TypedDict):

question: str output_type: str output_type_reason: str schema: str query: str query_reasoning: str query_message: str results_error: list summary: str summary_reason: str summary_analisys: bool information: str retries: int

This state structure can be broken down into several logical sections:

Question input (question): Stores the original user request Intent detection results (output_type, output_type_reason): Captures the detected visualization intent and the reasoning behind it Schema information (schema): Contains the graph schema in LLM- friendly format Query generation (query, query_reasoning, query_message): Holds the generated Cypher query and associated metadata Error handling (results_error): Tracks any errors encountered during query execution Summary generation (summary, summary_reason, summary_analysis): Contains the generated summary and analysis flags Queries retry mechanism (information, retries): Manages the retry logic for failed queries

Each field in this state object tells a story about the pipeline's progression: from the initial question, through intent detection and query generation, to

execution and finally summarization. The state not only carries data between agents but also maintains the context necessary for making routing decisions and handling errors gracefully.

15.1.5 Pipeline Agent Implementation

As we discussed earlier in this chapter, our expert-emulated approach maps naturally to a LangGraph pipeline, where each step in our question-answering process is implemented as a specialized agent. Figure 15.7 shows again this pipeline structure, highlighting how agents are connected and how the flow of information progresses from the initial question to the final answer. In this section we will look closely how these agents are made and how they interact with the LangGraph state object.

Figure 15.7 LangGraph implementation of the knowledge expert-emulated graph querying pipeline.

Intent detection agent

The intent detection agent serves as the entry point of our pipeline, responsible for determining how the user's question should be visualized. It

operates solely on the user's input question and enriches the state with visualization intent information.

This agent leverages the intent detection prompt we discussed in chapter 14 to analyze the question and determine the most appropriate visualization type. The agent updates two key fields in the state:

output_type: The determined visualization format (table, graph, or map) output_reason: The reasoning behind the selected visualization type

Let’s examine the implementation in Listing 15.6

Listing 15.6 Intent Detection Agent Implementation

def run_prompt(self, prompt, system=""): #A

messages = [HumanMessage(content=prompt)] if self.system or system: system = self.system if not system else system messages = [SystemMessage(content=system)] + messages #B

message = self.model.invoke(messages)

logger.debug(f" got {message.content}") payload = message.content payload = re.sub(r'^\s*```json\s*|\s*```\s*$', '', payload, flags=re.DOTALL) #C return json5.loads(payload) #D

def intent_detection(self, state: AgentState):

system, prompt = self.config.get_prompt( #E "intent_detection", question=state["q results = self.run_prompt(prompt, system) return { #F "output_type": results["type"], "output_reason": results["reason"]}

The implementation is straightforward: the agent retrieves the intent detection prompt template from the configuration, executes it with the user's question, and processes the LLM's response. The response is expected in a JSON format containing the visualization type and the reasoning, which are then mapped to the corresponding state fields.

Schema extraction agent

The schema extraction agent serves as a bridge between our knowledge graph and the LLM components by leveraging the Neo4jSchema object we introduced in Section 15.1.2. Its primary responsibility is to convert the knowledge graph schema into an LLM-friendly format that subsequent agents can use for query generation and reasoning.

Listing 15.7 shows how the intent detection agent handles prompt execution and response processing, with particular attention to JSON parsing and error handling.

Listing 15.7 Schema Extraction Agent

def schema_extraction(self, state: AgentState):

assert self.neo4j_schema is not None, \ "you need to provide a neo4j schema"

self.neo4j_schema.get_schema() #A self.neo4j_schema.apply_configuration() #B return {"schema": str(self.neo4j_schema), "retries": 0} #C

While the agent's code is concise, the heavy lifting is performed by the Neo4jSchema instance. The agent first ensures that a neo4j_schema object has been provided, then retrieves the current schema and applies any configuration settings we've defined.

The resulting schema is converted to a string format that LLMs can effectively process. Additionally, the agent initializes the retry counter to zero, preparing the state for potential query retries later in the pipeline.

Text to cypher agent

The text to Cypher agent transforms the user's natural language question into a Cypher query, considering both the graph's schema and any currently selected elements in the visualization.

This contextual awareness allows users to reference selected nodes or

relationships without explicitly describing them, making queries more natural and concise as we discussed in section 14.7.2

As we see in listing 15.8, the agent adds two pieces of information to the state before executing the prompt: annotations from the configuration provider that help guide query generation, and the current selection state from the visualization interface.

Listing 15.8 Text to Cypher Agent

def text_to_cypher(self, state: AgentState):

extra = { "annotations": self.config.getAnnotations(), "selection": self.selection } params = dict(state) | extra

system, prompt = self.config.get_prompt("text_to_cypher", **p logger.debug(f"prompt: {prompt}")

results = self.run_prompt(prompt, system)

return {"query": results["query"], "query_reasoning": results["reasoning"], "query_message": json.dumps(results)}

The agent merges the current state with the additional context (extra), retrieves and populates the appropriate prompt template, and then processes it through the LLM. The results include the generated Cypher query, the reasoning behind its construction, and the complete LLM response for debugging purposes. These are stored in the state's query, query_reasoning, and query_message fields respectively.

Query execution agent

Listing 15.9 showcases two key aspects of the query execution agent: robust error handling and dynamic result formatting based on visualization needs.

Listing 15.9 Query Execution Agent

def query_execution(self, state: AgentState):

try: results = self.neo4j_schema.run(state["query"]) if state["output_type"] in {"graph", "map"}: self.results = list(results) else: self.results = results.to_df() results_error = None information = "" except neo4j.exceptions.ClientError as e: self.results = None results_error = str(e) logger.info(f"got error: {e}") information = f"""We tried: {state['query']} and we got: ``` {str(e)} ```""" retries = state.get("retries", 0) + 1 return {"results_error": results_error, "retries": retries, "

The agent's logic is straightforward but robust. It attempts to execute the query stored in the state and processes the results based on the detected intent. When dealing with graph or map visualizations, it preserves the results in their native format as a list of records. However, for tabular output, it converts the results into a Pandas DataFrame using Neo4j's built-in conversion functionality.

Error handling is a crucial aspect of this agent. If the query execution fails (typically due to syntax errors or schema mismatches), the agent performs several key operations: it captures the error details, logs the failure for debugging purposes, constructs a detailed error message that includes both the attempted query and the error description, and increments the retry counter to keep track of attempts.

The agent updates the state with three key pieces of information. The results_error field contains the error message if execution failed, remaining None otherwise. The retries field tracks the number of execution attempts, while the information field provides detailed context about the error for potential retry attempts. This error information becomes crucial for the post-execution routing logic, which we'll examine next.

Post query execution

Figure 15.8 illustrates the routing logic implemented by this component, highlighted in the context of the overall pipeline.

Figure 15.8 Post-query execution routing logic (highlighted) in the QA pipeline, showing decision paths for retry, summarization, and direct completion

Unlike other components we've discussed, the post-query execution is not an agent but rather implements the routing logic as a dynamic edge in our LangGraph pipeline. Listing 15.10 shows its implementation

Listing 15.10 Post Query Execution dynamic edge

def post_query_execution(self, state: AgentState): #A if state["results_error"] is not None: if state["retries"] < 3: logger.info(f"{state['retries']} runs, we retry") return "retry" else: logger.info(f"{state['retries']} runs are enough" return "END"

#B if state["output_type"] in ("map", "graph"): logger.info("summarizing..") return "summarize" else: logger.info("no summarization is needed") return "END"

The routing logic follows two main decision paths. First, it handles query execution failures by checking the results_error field in the state. If an error occurred, the component implements a retry mechanism that allows up to three attempts to execute the query. This gives our system resilience against temporary failures or cases where the LLM might need multiple attempts to generate a correct query.

Second, for successful queries, the routing decision depends on the visualization intent captured in output_type. When dealing with map or graph visualizations, the component routes the flow to the summarization step, as these visualization types benefit from additional context and explanation. However, for tabular results, which are typically self- explanatory, the pipeline can conclude directly.

This dynamic routing capability is a key feature of LangGraph, allowing us to implement complex flow control based on both execution results and user intent. The component's simplicity belies its importance in orchestrating the overall pipeline behavior.

Generate summary agent

The final agent in our pipeline that handles the generation of summaries for graph and map visualizations is shown in listing 15.11

Listing 15.11 Generate Summary Agent

def generate_summary(self, state: AgentState): extra = { "records": self.results, "selection": self.selection } params = dict(state) | extra

system, prompt = self.config.get_prompt( "generate_summary", **params logger.debug(prompt)

results = self.run_prompt(prompt, system) return {"summary": results["summary"], "summary_reason": results["reasoning"], "summary_analisys": results["results_analysis"]}

The agent combines the query results and the schema selection to create a comprehensive context for the summary generation. It merges these elements with the existing state to populate the parameters for the summary prompt template. The configuration provider supplies the appropriate prompt template, which is then executed through the LLM.

The agent's output enriches the state with three components: the actual summary text, the reasoning behind the summary, and a flag indicating whether additional analysis was performed. This completes our pipeline, transforming the original user question into a meaningful summary of the retrieved information.

Building the LangGraph pipeline

The implementation of our expert-emulated pipeline culminates in the construction of the LangGraph workflow. Listing 15.12 shows how we connect our agents into a cohesive graph structure

Listing 15.12 LangGraph pipeline graph building

class Agent:

def __init__(self, model): self.neo4j_schema: Neo4jSchema = None self.selection = [] self.results = None self.config = ChainConfiguration() graph = StateGraph(AgentState) graph.add_node("intent_detection", self.intent_detection) graph.add_edge("intent_detection", "schema_extraction") graph.add_node("schema_extraction", self.schema_extractio graph.add_edge("schema_extraction", "text_to_cypher") graph.add_node("text_to_cypher", self.text_to_cypher) graph.add_edge("text_to_cypher", "query_execution")

graph.add_node("query_execution", self.query_execution) graph.add_conditional_edges("query_execution", self.post_query_execution, {"retry": "text_to_cypher", "summarize": "generate_summa "END": END}) graph.add_node("generate_summary", self.generate_summary) graph.add_edge("generate_summary", END) graph.set_entry_point("intent_detection") self.graph = graph.compile(checkpointer=self.memory) self.model = model

The graph construction follows a clear sequential pattern that mirrors our question-answering workflow. The StateGraph class from LangGraph provides the foundation, initialized with our AgentState type to ensure type safety throughout the pipeline.

Each agent is added as a node in the graph, with edges defining the standard flow between them: from intent detection through schema extraction and query generation to execution.

This graph structure not only implements our expert-emulated approach but also provides the flexibility to handle errors and different visualization requirements seamlessly within a single unified pipeline.

15.1.6 Pipeline Integration Layer

While LangGraph provides powerful capabilities for building complex workflows, we need to consider how applications will interact with these pipelines in practice. The simplest approach would be to use LangGraph's invoke mode, where we provide an initial state and receive the final result once the pipeline completes. However, this creates a less than I deal user experience - users might wait for extended periods without any feedback about what's happening behind the scenes.

Figure 15.9 illustrates the integration architecture that enables real-time interaction between the LangGraph pipeline and frontend applications.

Figure 15.9 Pipeline integration architecture showing the Question Processing Interface mediating between LangGraph state updates and frontend interactions

LangGraph offers a stream execution mode that provides visibility into intermediate steps, but managing these updates directly can complicate the application logic. To balance the benefits of streaming with ease of use, we've developed an interface layer that transforms the pipeline execution into a series of well-defined events that frontends can easily consume.

The core of this interface is a generator function that processes questions and yields a sequence of events. Generator functions are particularly well-suited

for this task because they allow us to maintain a simple, linear code flow while producing intermediate results. Listing 15.13 reveals how pipeline execution transforms into a clean event stream, featuring strong event typing and comprehensive state tracking.

Listing 15.13 Question Processing Interface function

def processQuestion(question, selection=None):

config = { "configurable": {"thread_id": uuid.uuid4().hex}} if selection is not None: #B pipeline.selection = [{"labels": list(node.labels)[0], "properties": dict(node)} for node in selection] else: pipeline.selection = [] input = {"question": question} results = pipeline.graph.stream(input, config=config, stream_mode="updates") #C yield "update", "*detecting intent...*", input #D for result in results: #E node, value = list(result.items())[0] #F logger.info(f"got results: {node}, keys: {list(value.keys current_state = pipeline.graph.get_state(config).values match node: case "intent_detection": yield "update", "*extracting schema...*", current_ case "schema_extraction": #H yield "update", "*generating query...*", current_ case "text_to_cypher": #I yield "update", "*executing the query...*", curre yield "result", ("Reasoning", value["query_reason current_state case "query_execution": if value["results_error"]: #J yield "result", ("ERROR", value["results_erro current_state else: #K output_type = current_state["output_type"] yield output_type, pipeline.results, current_ if output_type in {"graph", "map"}: yield "update", "*summary generation...*" current_state case "generate_summary": #L yield "result", ("Summary", value["summary"]),

current_state logger.info("no more results sendin END") current_state = pipeline.graph.get_state(config).values #M yield "END", current_state, current_state #N

The implementation demonstrates how we handle the different stages of pipeline execution. The function begins by setting up the initial configuration and state, then yields its first update event to inform users that intent detection has started. As the pipeline processes through each node - from intent detection to schema extraction, query generation, and beyond - the generator yields appropriate events to keep the frontend informed. Each case in the pattern matching structure corresponds to a specific pipeline stage, producing events that align with the current operation.

Each event produced by the generator follows a consistent structure: a triplet containing the response type, the response payload, and the current state of the pipeline. The response types fall into three main categories:

First, we have update events that inform users about the pipeline's progress. These events carry simple status messages like "detecting intent" or "generating query", helping users understand which step is currently executing.

Second, result events deliver textual outputs such as reasoning steps, potential errors, or generated summaries. These provide deeper insight into the pipeline's decision-making process and help users understand how the system arrived at its conclusions.

Finally, visualization events represent structured outputs like graphs, maps, charts, or tables. These events carry the data needed to create visual representations of the query results, allowing frontends to present information in the most appropriate format.

By including the current pipeline state with each event, we provide frontends with complete context without making assumptions about how they might use this information. This approach maintains a clean separation of concerns - the interface layer focuses on transforming pipeline execution into a stream of well-defined events, while leaving presentation decisions entirely to the frontend.

The result is an interface that keeps users informed and engaged throughout the question-answering process while maintaining clean architectural boundaries. By transforming complex pipeline execution into a simple stream of events, we've created a foundation that supports rich interactive experiences without sacrificing maintainability or flexibility.

15.2 Streamlit application

Having built our LangGraph pipeline for expert-emulated question answering, we need an interface that allows users to effectively interact with and validate the system's capabilities. This interface must support several key requirements that emerge directly from our expert-emulated approach.

The interface needs to support interactive graph visualization, allowing users to explore and select nodes and relationships. It must provide real-time feedback as the pipeline processes questions through its various stages, from intent detection to final summarization. A chat-like interface is essential for natural language interaction, while the system needs to maintain complex state information about selected graph elements and processing context.

Streamlit's features align particularly well with these requirements. Its native support for chat interfaces provides the foundation for implementing our question-answering interactions, complete with user messages and system responses. The framework's built-in data visualization capabilities, combined with its extensibility through custom components, enable us to create effective graph representations. Most importantly, Streamlit's Python-first approach ensures seamless integration with our LangGraph pipeline - there's no need to build complex APIs or handle cross-language serialization, as both frontend and backend operate within the same Python environment.

As our pipeline processes questions, it generates a stream of updates about its progress and intermediate results. Streamlit's session state system, combined with its automatic UI updating, allows us to reflect these changes in real-time without building complex event handling mechanisms. This enables users to see exactly how the expert-emulated system processes their questions, from initial intent detection through to final visualization and summarization.

These characteristics make Streamlit particularly suitable for prototyping and testing our system. The rapid iteration cycle means we can quickly validate how different types of questions are processed and how various visualization options perform. The framework's low implementation overhead lets us focus on testing and refining the core expert-emulated functionality rather than dealing with frontend complexity. While a production deployment might warrant a more specialized interface, Streamlit provides exactly what we need for developing and demonstrating our system's capabilities.

15.2.1 Application overview

With the requirements and framework established, our next step is designing a functional interface that fully supports expert-emulated graph exploration. The interface must transform abstract requirements like "natural interaction" and "real-time feedback" into concrete components that work together seamlessly.

Figure 15.10 shows the main interface layout of our application, which creates an intuitive environment for graph exploration and question answering

Figure 15.10 Application interface layout demonstrating a question answering system with selection capabilities, interactive graph visualization, and real-time response tracking

The interface components directly map to the key capabilities of our expert- emulated pipeline. Each element is designed to support specific aspects of the system, from intent detection through query generation to result presentation.

At the heart of the application is the graph canvas, which visualizes the knowledge graph's nodes and relationships. This central view is complemented by different interaction areas that support natural exploration and questioning workflows.

The selection column serves a specific purpose in making questions more natural and context-aware. Users can select specific nodes from the graph and then refer to these selections in their questions using natural language. For example, with certain nodes selected, a user might ask "What are the companies related to these assets?" rather than having to specify each asset explicitly.

This selection mechanism is crucial for testing our system's context- awareness, allowing us to verify how well the pipeline understands and incorporates visual context into its query generation process.

The question input area at the bottom enables users to pose their questions in natural language. These questions can range from simple fact-finding to complex relationship analysis, all while maintaining a natural conversation- like interaction style.

The history area on the right provides a comprehensive view of the question- answering process. As our expert-emulated approach can generate different types of responses, this area adapts to display various formats. When answers include geographic information, they are presented as interactive maps. When the system determines that tabular data would be most informative, it displays well-formatted tables. Importantly, this area updates in real-time as the system processes each question, showing intermediate steps and final results as they become available through our pipeline.

The history area's real-time updates serve a dual purpose: they keep users informed of progress while also making the expert-emulated pipeline's reasoning process visible. This transparency helps users understand how their

questions are being processed and why particular visualizations or response formats were chosen.

This design creates a fluid experience where users can naturally explore the knowledge graph, ask questions about what they see, and receive rich, multi- format responses that best suit the information being conveyed. While each question stands as an independent interaction, the continuous updates and preserved history create a seamless exploration experience.

Through this interface, we can effectively evaluate all aspects of our expert- emulated approach: from the system's ability to handle context-aware queries to its capacity for generating appropriate visualizations and providing meaningful explanations of its reasoning process.

15.2.2 LangGraph Integration

The integration between Streamlit and our LangGraph pipeline creates a real- time, interactive experience for users. Let's explore how these systems work together.

When a user submits a question through the interface, this triggers a chain of events that connects our frontend with the underlying expert-emulated system. The Question Processing Interface, which we discussed earlier, acts as the bridge between these components.

The integration follows a clear event-driven pattern. As soon as the user clicks the send button, the question travels through the Question Processing Interface to the LangGraph pipeline. Rather than waiting for the final result, our system provides immediate feedback as each agent in the pipeline processes the question. This real-time visibility helps users understand how their questions are being analyzed and answered.

To manage this flow of information, we implement a MessageHistory object that serves two crucial purposes. First, it maintains the complete history of interactions, allowing users to review past questions and answers. Second, it stores the current state of the pipeline, tracking which agent is actively processing the question and what intermediate results have been produced.

Listing 15.14 demonstrates the MessageHistory class's versatility in handling diverse content types while maintaining Streamlit's display conventions.

Listing 15.14 Message History Implementation

class MessageHistory:

def __init__(self): self.messages = [{}] #A

def update(self, message, finalize=False): self.messages[-1].update(message) #B if finalize: #C self.messages.append({})

@staticmethod def display_message(msg): #D with st.chat_message("user"): st.markdown(msg["question"]) with st.chat_message("assistant"): #E if "query_reasoning" in msg: st.markdown(f"##### Reasoning\n\n**output type**: `{msg['output_type']}`\n\n\ {msg['query_reasoning']}") if "table" in msg: st.table(msg["table"]) if "map" in msg: map_ = folium.Map() nodes_to_map(msg["map"], map_) #F st_folium(map_) if "query" in msg: #G with st.expander("Query...", expanded=False): st.markdown(f"```cypher\n\n{msg['query']}\n`` st.json(msg["query_message"]) if "summary" in msg: st.markdown(f"##### Summary\n\n{msg['summary']}") with st.expander("extra...", expanded=False): st.json({ #H "summary_reason": msg["summary_reason"], "summary_analisys": msg["summary_analisys }) st.json(msg, expanded=False) #I

def display_messages(self): #J for message in self.messages: if not message: continue

self.display_message(message)

This implementation showcases several important aspects of our integration. The MessageHistory class maintains a list of message dictionaries, with each message potentially containing different types of content - from simple text to complex visualizations. The update method allows for progressive building of messages, reflecting the step-by-step nature of our pipeline's processing. The display_message method handles the rendering of different content types using Streamlit's components: markdown for text, tables for structured data, and folium for maps. The implementation carefully organizes the information hierarchy, using expandable sections for detailed information like queries and summary reasoning.

The integration between user input and the LangGraph pipeline uses a reactive pattern that manages both temporary and permanent state updates. When processing a question, the system needs to show immediate feedback while also maintaining a permanent conversation history. This is achieved through two complementary mechanisms:

Event handling with temporary placeholders shows real-time updates as the pipeline processes the question. These updates give immediate feedback but are transient, using Streamlit placeholders to display the current pipeline state. MessageHistory accumulates the permanent state of the conversation. Rather than showing updates directly, it collects the complete state for each message until receiving an END event. When this happens, the page re-renders using MessageHistory's display logic, replacing the temporary updates with the final, persistent version of the conversation.

This approach, implemented in Listing 15.15, ensures users see both immediate feedback during processing and a clean, permanent record of their interaction history

Listing 15.15 User Input Handler

[...] if question := st.chat_input("What is up?"):

with chat: with st.chat_message("user"): #A

st.markdown(question) with st.chat_message("assistant"): #B placeholder = st.empty()

selection = [state.canvas["byId"][int(item)] for item in state.selection] #C

for response_type, response, current_state in \ chain.processQuestion(question=str(question), selection=selection): #D state.messages.update(current_state) #E match response_type: #F

case "update": #G placeholder.markdown(response)

case "graph" | "map": placeholder.markdown("*updating canvas...*") store_to_canvas(response) #H if response_type == "map": state.messages.update( {"map":state.canvas["nodes"]}) #I

case "table" | "chart": state.messages.update({"table": response}) # placeholder.table(response) #K with st.chat_message("assistant"): #L placeholder = st.empty()

case "result": #M title, content = response response = f"##### {title}\n\n{content}" placeholder.write(response) with st.chat_message("assistant"): #N placeholder = st.empty()

case "END": state.messages.update(current_state, finalize st.rerun() #P

This event handling code demonstrates how we process user interactions in real-time. When a user submits a question, the system creates placeholder elements in the chat interface that are progressively updated as the pipeline processes the question. The match statement elegantly handles different types of responses:

Updates to the chat interface for text responses Graph and map visualizations that are rendered in the canvas Tabular data that is displayed using Streamlit's table component Formatted results with titles and content A final “END” event that triggers a rerun to ensure all UI elements are properly updated

The combination of MessageHistory and this event handling system creates a responsive and interactive experience where users can see their questions being processed in real-time, while maintaining a consistent and organized history of all interactions.

15.3 Expert-emulated investigation

To demonstrate how our expert-emulated system works in practice, let's follow a realistic investigation workflow. We'll see how an investigator can use natural language queries to explore connections between crimes, surveillance cameras, and vehicles, while leveraging the system's ability to understand context and provide meaningful insights.

Our investigation will work with a subset of the knowledge graph focused on criminal incidents. As shown in Figure 15.11, the schema connects Crime nodes (containing properties like location, description, and date-time) to ANPR (Automatic Number Plate Recognition) cameras through spatial relationships. The ANPR cameras generate CameraEvents when they detect vehicles, recording both the timestamp and location of each sighting. These events link to Vehicle nodes, which store properties such as model, color, and plate number. Vehicles in turn connect to Person nodes representing their owners, who may have links to other criminal incidents through COMMITTED relationships.

Figure 15.11 Focused schema visualization showing how Crime, ANPRCamera, CameraEvent, Vehicle, and Person nodes interconnect for investigative queries.

This seemingly simple structure allows for sophisticated queries that combine spatial analysis, temporal patterns, and relationship exploration. Let's see how an investigator might use our system to identify vehicles of interest related to a criminal trespass incident.

15.3.1 Identifying the initial case

Our investigation begins by asking the system to identify a crime currently under investigation. This demonstrates a key aspect of our expert-emulated approach: the ability to understand and translate natural language queries that combine both explicit constraints ("currently under investigation") and implicit expectations (returning a meaningful crime for analysis).

The system processes this request through our expert-emulated pipeline, first detecting that a graph visualization would be most appropriate for displaying a single node with its properties. The query generation component understands that we're looking for an active investigation and includes relevant constraints in the generated Cypher query.

Let's pose our first question to the system: "Return one crime node currently

under investigation". Figure 15.12 shows the system's response, displaying a crime node representing a criminal trespass incident.

Figure 15.12 Initial query response showing a crime node currently under investigation. The interface displays the current selection, the node's detailed properties in the selection panel (left), the node visualization in the canvas (center), and the query processing details in the chat interface (right).

Notice how the selection panel on the left provides detailed information about the incident, including a crucial narrative that mentions a black vehicle with a partial license plate number starting with "EB". This information will become valuable as our investigation progresses.

In the chat interface, we can see the system's reasoning process, explaining why it chose this crime and how it structured the query to ensure we received an active investigation. This transparency helps users understand how their natural language questions are being interpreted and executed.

The summary generated by our system highlights key aspects of the crime, including its classification as criminal trespass and the presence of vehicle information in the narrative. This demonstrates how our summary generation component can extract and emphasize relevant details that might be buried in property fields.

This initial interaction showcases several key capabilities of our system: natural language understanding, appropriate visualization selection, and intelligent summarization of node properties. But more importantly, it sets the stage for more complex queries that will leverage this context as we proceed with our investigation.

15.3.2 Spatial analysis of surveillance coverage

With our crime node identified, the next logical step is to check for surveillance coverage in the area. Our system's spatial reasoning capabilities allow us to naturally ask about nearby ANPR cameras without needing to specify exact coordinates or complex distance calculations.

We first double-click the crime node to add it to our selection, then ask: "Return any ANPR camera node located within 1km from the selected crime." Note how we can refer to "the selected crime" directly, the system understands this context from our current selection.

Figure 15.13 captures this interaction, showing how the system responds with both a graph visualization and an interactive map.

Figure 15.13 Spatial query response showing an ANPR camera near the crime location. The system automatically chose a map visualization to display the spatial relationship between the crime and the nearby ANPR camera

The canvas now includes the crime node, along with a newly discovered ANPR camera node in yellow. What makes this response particularly powerful is the system's decision to provide a map visualization alongside the graph view. The map displays two markers - one for the incident location and another for the camera - using colors that match their respective nodes in the

graph.

The camera's position appears strategically valuable - it's located near an intersection that could be a likely entry or exit point for the area where the trespass occurred. This spatial insight, made immediately apparent through the dual visualization, suggests this camera's data could provide useful leads for our investigation.

This step demonstrates several sophisticated capabilities of our system: spatial query processing, selection-aware natural language understanding, and most importantly, intelligent visualization selection. The system recognized that while a graph view helps understand relationships, a map view is crucial for spatial comprehension. This kind of adaptive response, choosing the most appropriate visualization for the data at hand, is a key feature of our expert- emulated approach.

15.3.3 Vehicle pattern detection

With both the crime location and a nearby ANPR camera identified, we can now search for vehicles that match the description from the incident report. We include the ANPR camera by double-clicking its node, adding it to our current selection alongside the crime node.

Our next question leverages this selection while adding specific vehicle criteria: "Return the vehicles detected by the selected camera on June 15, 2023. The vehicle is black and its's license plate must start with EB." Note how we can naturally combine references to selected elements ("selected camera") with explicit constraints about the vehicle's appearance and license plate.

Figure 15.14 shows how the system expands our visualization to include matching vehicles and their detection events.

Figure 15.14 Vehicle query results showing matching vehicles and their detection events. Each path represents a complete vehicle detection record, with timestamps visible on the event nodes. The system's response includes both the graph visualization and a detailed summary of each vehicle's properties.

The graph now displays multiple vehicle nodes connected to our ANPR camera through detection events, with the query's reasoning visible in the chat interface. The system has understood the temporal constraint ("June 15, 2023"), the vehicle's physical description ("black"), and the partial plate number ("EB"), incorporating all these elements into a single coherent query.

The summary highlights each matching vehicle, providing a clear overview of potential suspects. This demonstrates our system's ability to handle multi- faceted queries that combine node selection, temporal constraints, and property matching - all while maintaining a natural language interface that aligns with how investigators actually think about and explore their cases.

This step marks a significant advancement in our investigation, transforming isolated data points about a crime and a camera location into a concrete set of vehicles that could be connected to our incident. But can we get more insight by providing additional context about our investigative goals? Let's explore this in our next question.

15.3.4 Context-aware request refinement

Let's refine our approach by leveraging two key capabilities of our expert- emulated system: its ability to extract information from selected nodes and its context-aware summarization. Instead of explicitly stating the vehicle description and date constraints, we can rely on the system to extract these details from our selected crime node. Moreover, by providing context about our role and investigative intent, we can guide the system toward generating more analytical insights.

We rephrase our request to reflect this investigative context: "I'm an investigator and I'm working on the selected crime. I need all the vehicle nodes that are compatible with the description and were detected by the selected camera during the day of the incident. There are any that seems significantly more likely to be involved in the incident?"

The response, as shown in Figure 15.15, demonstrates how additional context transforms the system's analysis.

Figure 15.15 Enhanced analysis showing the same vehicle data with investigative context. The system augments its response with an analysis section that identifies patterns of interest, demonstrating how additional context leads to more insightful summarization of the same underlying data.

While the query returns the same vehicles as before, the summarization now includes a deeper analysis of the results. The system identifies a particularly interesting pattern: one of the matching vehicles was detected twice around the time of the incident, suggesting a potential circuit of the area that warrants further investigation.

This enhanced response showcases two important aspects of our expert- emulated approach. First, it demonstrates how the system can autonomously extract and apply constraints from node properties, eliminating the need for explicit restatement. Second, it illustrates how providing investigative context - in this case, identifying ourselves as investigators seeking suspicious patterns - enables the system to generate more relevant and insightful summaries.

The system has moved beyond simply matching criteria to actively analyzing temporal patterns that might indicate suspicious behavior. This sets the stage for our final investigative step: examining whether any of these vehicles have connections to known offenders.

15.3.5 Historical record analysis

Building on our discovery of a vehicle with suspicious movement patterns, we can further enrich our investigation by considering another crucial aspect: the criminal history of vehicle owners. This type of background check is a standard investigative practice that our system can integrate naturally into its analysis.

We modify our question slightly to prompt this additional analysis: "I'm an investigator and I'm working on the selected crime. I need all the vehicle nodes that are compatible with the description and were detected by the selected camera during the day of the incident. Some of these vehicles may be owned by previous offenders. What vehicles are the most likely to be involved in the incident?"

Figure 15.16 reveals a significant breakthrough in our investigation. The system now expands its analysis to include ownership relationships and criminal records, discovering that the vehicle previously flagged for suspicious movements is owned by an individual with a relevant criminal history.

Figure 15.16 Final investigative insight revealing criminal history. The graph expands to show that a vehicle owner has connections to multiple prior crimes, including a previous criminal trespass. The summary provides a detailed breakdown of the prior offenses, demonstrating the system's ability to integrate temporal, spatial, and historical evidence into a cohesive investigative narrative.

Most notably, this person's record includes a previous conviction for criminal trespass – the same type of offense we're currently investigating. The summary agent recognizes the heightened significance of this connection, highlighting it prominently in both the general summary and the analysis section. This combination of factors – the vehicle's suspicious movement

pattern and the owner's criminal history – provides compelling leads for further investigation.

This final step demonstrates the full potential of our expert-emulated system. It shows how the system can:

Maintain contextual awareness across multiple query refinements Integrate various types of evidence (spatial, temporal, and historical) Identify meaningful patterns and connections Present findings in a way that directly supports investigative decision- making

Through this investigation, we've seen how natural language interaction, intelligent summarization, and context-aware analysis come together to support real-world investigative workflows.

15.4 Future directions and enhancements

Throughout chapters 14 and 15, we've explored the development of a question answering system for knowledge graphs that takes a distinct approach to the challenge. Rather than treating it as purely a language modeling problem, we've built a system that emulates how human experts work with graph databases - understanding schema context and constructing queries through reasoned steps. The implementation we've demonstrated shows how this expert-emulated approach can be realized through a practical pipeline of specialized agents, each handling distinct aspects of the process.

However, this implementation should be viewed as a solid foundation rather than a turnkey solution. Its value lies not just in what it does, but in how it's built. The foundation's strength comes from its underlying architecture, particularly in how it approaches observability and expert emulation.

The system's comprehensive observability - with reasoning exposed at every step - provides clear insights into its operation. This transparency isn't merely for debugging; it creates natural points for collecting feedback and identifying improvements. Each component's decision-making process is visible, making it possible to understand not just what the system does, but

why it does it.

Equally important is the expert emulation pattern that underlies the entire design. When faced with new challenges or opportunities for improvement, teams can begin with a simple but powerful question: "What would an expert do in this situation?" This human-centric approach to system design makes improvements feel natural rather than forced - they emerge from understanding and modeling expert behavior.

In the following sections, we'll explore several paths for building upon this foundation. These aren't merely theoretical possibilities - they're concrete examples of how the system's observable, expert-emulated nature enables specific types of improvements. From leveraging user feedback to enhancing core capabilities and even exploring advanced approaches like fine-tuning, we'll see how the architectural choices made in our implementation create natural evolution paths for the system.

15.4.1 Learning from usage

A natural starting point for system evolution leverages one of our core design principles: comprehensive observability. Every query processed by the system generates rich information about user intentions, query patterns, and outcome effectiveness. This wealth of observable data creates opportunities for systematic improvement at multiple levels.

The most direct application of this observability lies in collecting and categorizing "complaint-like" questions - those where users indicate the system's response didn't meet their needs. Rather than treating these as simple failures, our observable pipeline allows us to extract meaningful patterns from these instances where users report issues with the system. Through systematic categorization of these patterns using LLM-based analysis, we can surface common user challenges in a dynamic dashboard of pain points that would become a strategic tool for guiding development priorities and resource allocation.

Equally valuable would be the collection of particularly successful interactions. When the system generates queries that users find especially

helpful, the chain of reasoning that leads to these successful outcomes could be preserved. These success patterns could then be systematically analyzed to identify what makes them effective, creating a foundation for enhancing our example database. This systematic approach to refining examples would be particularly powerful, as our expert-emulated approach incorporates demonstration through examples as an important element for query generation.

Beyond these direct applications, the observable nature of our system enables a more sophisticated form of intent detection enhancement. By analyzing patterns in user questions, we can identify clusters of similar queries that might benefit from specialized handling. This understanding can be used to optimize downstream processes - for example, by selecting only the most relevant portions of the schema or by providing more focused examples for specific types of questions.

This learning from usage represents more than just incremental improvement - it demonstrates how our foundational architecture enables the system to evolve based on actual use patterns rather than predetermined rules. The combination of comprehensive observability with the expert emulation pattern creates a framework where improvements emerge naturally from usage patterns while remaining grounded in understandable, expert-like behavior.

15.4.2 Enhancing core capabilities

The expert-emulated approach provides natural pathways for enhancing the system's core capabilities. By consistently asking "What would an expert do?" we can identify and implement improvements that align with human expertise patterns while leveraging our observable pipeline architecture.

A key area for enhancement is schema handling, where we can emulate how human experts build their understanding of a knowledge graph's structure. Experts often run preliminary queries to understand how the data is structured and used, going beyond the basic schema definition. We can model this behavior by implementing schema enrichment agents that analyze data patterns and augment the base schema with additional context. This enriched

schema information can then flow downstream to improve query generation and result interpretation.

For large-scale knowledge graphs, experts typically work with mental models at different levels of abstraction. A multi-layer schema management approach would help mirror this mental process. For instance, in our investigative knowledge graph, the bottom layer would contain all detailed nodes and relationships (crimes, vehicles, camera events, people), while higher layers could organize these into domain-focused views like vehicle monitoring or criminal justice records. The system could use these higher-level views for broad understanding (e.g., which domains to query) while preserving detailed descriptions for specific query generation. This layered approach would help manage complexity without sacrificing precision, much like how human experts zoom in and out of different levels of detail as needed.

The intent detection system can evolve beyond basic classification of question types. By analyzing clusters of similar questions identified through our usage monitoring, we can create more nuanced understanding of user intentions. This enhanced intent detection can then inform downstream processes more effectively - selecting relevant schema portions, choosing appropriate example sets, and guiding the query generation process.

These enhancements build naturally upon our foundation while maintaining the system's core principles. Each improvement follows expert behavior patterns, benefits from our observable architecture, and contributes to a more sophisticated yet understandable system. The result is a progressively more capable system that remains true to its expert-emulated origins.

15.4.3 Advanced evolution paths

While the improvements discussed so far build directly on our current implementation, there are more ambitious evolution paths that could significantly enhance the system's capabilities. These advanced approaches maintain our core principles while incorporating emerging techniques in language model applications.

A particularly promising direction involves moving beyond pure in-context

learning for schema understanding and query generation. While our current approach of embedding schema information and examples in prompts is effective, it faces scalability challenges with larger knowledge graphs. Fine- tuning approaches offer an alternative path. By using the schema itself as training data, we could develop Knowledge Graph-Aware agents that have a deeper, more efficient understanding of the graph structure and relationships.

Studies have shown that in-context learning, while flexible, consistently underperforms compared to task-specific adaptation approaches [1][2]. This performance gap persists even when both approaches have access to the same example sets. These findings suggest that moving toward fine-tuned components could significantly improve our system's performance while potentially reducing its computational overhead.

The transition to fine-tuned components wouldn't require abandoning our expert-emulated architecture. Instead, we could selectively replace or augment in-context learning components with fine-tuned alternatives while maintaining our observable pipeline structure.

This evolution path also opens possibilities for more sophisticated query planning. Fine-tuned agents could develop a more nuanced understanding of query patterns and their relationships to different graph structures, potentially leading to more efficient and effective query generation. The system could maintain its step-by-step reasoning approach while building upon a stronger foundation of graph knowledge.

15.5 Summary

System implements expert-emulated approach, mimicking how human experts interact with graph databases LangGraph's state-based design enables creation of modular, observable AI pipelines where each component maintains independent reasoning Pipeline implements dynamic flow control with error handling and retry mechanisms through conditional edges Configuration and schema management are separated from core logic, allowing easy updates Schema Provider transforms technical database schemas into LLM-

friendly formats through filtering and enrichment Intent Detection agent analyzes questions to determine appropriate visualization approaches Text-to-Cypher agent generates database queries considering schema context and user selections Message history management enables stateful conversations and real- time progress updates Event streaming architecture allows seamless integration between LangGraph pipeline and Streamlit interface Interactive visualization supports multiple formats (graphs, maps, tables) and context-aware questioning System supports complex analysis through spatial, temporal, and historical context integration Context-aware summarization helps users understand complex query results Observable architecture enables systematic collection of user feedback for improvement Core capabilities can be enhanced through schema enrichment and intent refinement Advanced evolution paths include potential fine-tuning for improved performance

15.6 Reference

1. Bhatia, K., Narayan, A., De Sa, C., & Ré, C. (2023). TART: A plug-

and-play Transformer module for task-agnostic reasoning. arXiv preprint. 2. Lester, B., Al-Rfou, R., & Constant, N. (2021). The Power of Scale for

Parameter-Efficient Prompt Tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (pp. 3045-3059). Association for Computational Linguistics.

## Appendix_A._Introduction_to_graphs

This appendix covers

What is a graph and the fundamental mathematical concepts behind graph theory How graphs serve as models for representing real-world networks across domains Standard methods for representing graphs using adjacency lists and matrices

Even though graphs are simple data structures, it is important to understand how to represent them and to know the main concepts around it. This appendix introduces the key elements of the graph world. If you already know such concepts, or you read Alessandro’s previous book [1] from which the following sections have been extracted, you can skip this appendix.

A.1 What is a graph?

The graph is a simple and quite old mathematical concept. It is a data structure consisting of a set of vertices (or nodes/points) and edges (or relationships/lines) that can be used to model relationships among a collection of objects.

Legend says that it was the lazy Leonhard Euler who first started talking about graphs in 1736. While visiting Königsberg in Prussia (now Kaliningrad, Russia), Euler didn’t want to spend too much time walking in the city that sat on both sides of the Pregel River and included two large islands that were connected to each other and to the two mainland portions of the city by seven bridges. Planning a walk through the city, Euler formalized the problem to cross each of those bridges once and only once. This led to the invention of graphs and graph theory [2]. He proved that it was an impossible task, and so he stayed home instead. Figure A.1 illustrates an old representation of Königsberg and the related graph representation used by

Euler to prove his thesis.

Figure A.1 The bridges in Königsberg, Russia that led Leonhard Euler to the invention of graph theory in 1736

More formally, a graph is a pair of vertices and edges represented by G = (V, E), where V is a collection of vertices denoted by V = {Vi, i = 1, n}, and E is a collection of edges over V designated by Eij = {(Vi, Vj), Vi ∊ V, Vj ∊ V}. E ⊆ [V]2. The elements of E are, thus, two-element subsets of V [3]. The simplest way of representing a graph is by drawing a dot or a small circle for each vertex and joining two of those c by a line if they form an edge. This more formalized description is shown in figure A.2.

Figure A.2 The undirected graph on V = {1, 2, 3, 4, 5} with the edge set as E = {(1,2), (1,5), (2,5), (2,4), (4,3)}

Graphs can be either directed or undirected, depending on whether a direction of traversal is defined on the edges. In directed graphs, an edge Eij can be traversed from Vi to Vj but not in the opposite direction; Vi is called the tail or start node, and Vj is called the head or end node. In undirected graphs, edge traversals in both directions are valid. Figure A.2 represents an undirected graph, whereas figure A.3 represents a directed graph.

Figure A.3 The directed graph on V = {1, …, 5} with the edge set as E = {(1,2), (2,5), (5,1), (2,4), (3,4)}

In figure A.3, the arrow indicates the direction of the relationship. By default, edges in a graph are unweighted (and, thus, the corresponding graphs are said to be unweighted). When a weight—a numerical value used to convey some

significance—is assigned to the edges, the graph is said to be weighted. Figure A.4 shows the same graphs as the previous two figures but with a weight assigned to each edge.

Figure A.4 An undirected weighted graph (a) and a directed weighted graph (b)

Two vertices, x and y of G, are defined as adjacent or neighbors if {x,y} is an edge of G. The edge Eij connecting them is said to be an incident on the two vertices Vi and Vj. Two distinct edges, e and f, are adjacent if they have a vertex in common. If all the vertices of G are pairwise adjacent, then G is complete. Figure A.5 shows a complete graph, where each vertex is connected to all the other vertices.

Figure A.5 A complete graph, where each vertex is connected to all the others

One of the most important properties of a vertex in a graph is its degree, defined as the total number of edges incident to that vertex, which is also equal to the number of neighbors of that vertex. For instance, in the undirected graph of figure A.4, the vertex 2 has degree 3 (it has the vertices 1, 4, and 5 as neighbors); the vertices 1 (its neighbors are 2 and 5), 4 (neighbors are 2 and 3) and 5 (neighbors are 1 and 2) have degree 2; the vertex 3 has degree 1 (it is connected only with 4).

In a directed graph, the degree of a vertex Vi is split into the in-degree of the vertex, defined as the number of edges for which Vi is its end node (the head of the arrow), and the out-degree of the vertex, defined as the number of edges for which Vi is its start node (the tail of the arrow). In the directed graph of figure A.5, vertices 1 and 5 have an in-degree and an out-degree of 1 (they each have two relationships, one ingoing and one outgoing), vertex 2 has an in-degree of 1 and an out-degree of 2 (one ingoing relationship from 1 and two outgoing to 4 and 5), vertex 4 has an in-degree of 2 and an out- degree of 0 (two ingoing relationships from 2 and 3), and vertex 3 has an out- degree of 1 and in-degree of 0 (one outgoing relationship to 4). The average degree of a graph is computed as follows, where N is the number of vertices in the graph:

A sequence of vertices with the property that each consecutive pair in the sequence is connected by an edge is called a path. A path with no repeating vertices is called a simple path. A cycle is a path in which the first and the last vertex coincide. In figure A.2, [1,2,4], [1,2,4,3], [1,5,2,4,3], and so on are paths; in particular, the path of vertices [1,2,5] represents a cycle.

A.2 Graphs as models of networks

Graphs are useful to represent how things are either physically or logically linked to each other in simple or complex structures. A graph in which we assign names and meanings to the edges and vertices becomes what is known

as a network. In these cases, a graph is the mathematical model for describing a network, whereas a network is a set of relationships between objects, which could include people, organizations, nations, items found in a Google search, brain cells, or electrical transformers. This diversity illustrates the great power of graphs and their simple structure (which also means that they require a small amount of disk storage capacity) that can be used to model a complex system.[1]

Let’s explore this concept using an example. Suppose we have the graph shown in figure A.6.

Figure A.6 A nontrivial generic graph

It is a pure graph, in the mathematical sense, that can be used to model several types of networks, according to the types of edges and vertices. For example, it can model

A social network, if the vertices are people, and each edge represents any sort of relationship between humans (friendship, family member, coworker, etc.). An informational network, if the vertices are information structures like

web pages, documents, or papers, and the edges represent logical connections, such as hyperlinks, citations, or cross references. A communication network, if the vertices are computers or other devices that can relay messages, and the edges represent direct links along which messages can be transmitted. A transportation network, if the vertices are cities, and the edges represent direct connections using flights or trains or roads.

This is a small set of examples that demonstrates how the same graph can represent multiple networks by assigning different semantics to edges and vertices. Figure A.7 illustrates different types of networks.

Figure A.7 Clockwise from top left: co-occurrence network (https://en.wikipedia.org/wiki/Co- occurrence_network), ARPA network 1974 (https://en.wikipedia.org/wiki/ARPANET), London Tube network (https://tfl.gov.uk/tfl/syndication/widgets/tubemap/default-search.html), and electrical grid (https://en.wikipedia.org/wiki/Electrical_grid)

Looking at figure A.7, we can spot another interesting characteristic of graphs: they are highly communicative. They can display information in a clear manner, and this is why they are often used as information maps. Representing data as networks and using graph algorithms, it is possible to

Find complex patterns. Make those patterns visible for further investigation and interpretation.

When the power of machine learning (ML) is combined with the power of the human brain, it enables efficient, advanced, and sophisticated data processing and pattern recognition. Networks are useful for displaying data by highlighting connections between elements. Newspapers and news websites are increasingly using them, not only for helping people navigate the data but as a powerful investigation tool.

Recently (at the time of this writing), the Panama Papers (https://panamapapers.icij.org) showcased the astonishing features of networks. The International Consortium of Investigative Journalists (ICIJ) analyzed leaked financial documents to expose highly connected networks of offshore tax structures used by the world’s richest elites. The journalists extracted the entities (people, organizations, and any sort of intermediaries) and relationships (protector, beneficiary, shareholder, director, and so on) from the documents, stored them in a network, and analyzed them using visual tools. Figure A.8 shows the results.

Figure A.8 An example of the graph visualization for the Panama Papers

In the figure, networks, graph algorithms, and graph visualization made evident something that otherwise would have been impossible to discover using traditional data mining tools. A lot of interesting examples in this direction are also available in blog posts by Valdis Krebs (http://www.thenetworkthinkers.com), an organization consultant who specializes in social network applications. His work contains examples of mixing graph-powered ML with the human mind, passing through graph visualization. We’ll consider one of the more famous examples.

The data in figure A.9 was gathered from Amazon.com and represents their list of the top political books purchased in the US in 2008 [4]. Krebs employed network analysis principles to the data to create a map of books related to that year’s presidential election. Two books are linked if they were often purchased by the same customer. These are known as also-bought pairs (a customer who bought this book also bought that book).

Figure A.9 Network map of US political books in 2008 [4]

In the figure, there are three different and non-overlapping clusters:

An Obama cluster of books in the upper-left corner A Democratic (blue) cluster in the middle A Republican (red) cluster in the bottom-right corner

In 2008, the US political climate was highly polarized; this is mirrored in Amazon’s political book data, where figure A.9 shows a deep divide between conservative and liberal voters. There were no connections or intermediaries between red and blue books; each cluster was completely distinct from the other. As mentioned, there was a separate cluster of people reading biographies of presidential hopeful Barack Obama, but they were apparently not interested in reading or purchasing other political books. Four years later, in 2012, the same analysis produced a network that appeared substantially different (see figure A.10).

Figure A.10 Network map of US political books in 2012 [4]

In figure A.10, there are a lot of books that act as bridges between the different clusters. Moreover, potential voters appear to be reading books about both presidential candidates. The result is a more complex network in which there are no longer any isolated clusters.

The example of a network of political books introduces an important aspect. If a graph is a pure mathematical concept that lives in its own Platonic world, networks, as abstractions of some concrete system or ecosystem, are subjected to forces that act on them and change their structure.[2] We refer to these as surrounding contexts: factors that exist outside the vertices and edges of a network but which, nonetheless, affect how the network’s structure evolves over time.

The nature of such contexts and the types of forces are specific to the kind of network. For example, in social networks where each individual has a distinctive set of personal characteristics, similarities and compatibilities between two people’s characteristics influence link creation or deletion [5]. One of the most basic notions governing the structure of social networks is homophily (from the Greek, love of the same): links in a social network tend to connect people who are similar to one other.

More formally, if two people have characteristics that match in a proportion greater than expected in the population from which they are drawn or in the network from which they are a part, then they are more likely to be connected [6]. The converse is also true: if two people are connected, then they are more likely to have common characteristics or attributes. This is why our friends on Facebook, for example, don’t look like a random sample of people but are generally similar to us along ethnic, racial, and geographic dimensions; they tend to be similar to us in age, occupation, interests, beliefs, and opinions.

This observation has a long history, with its origins long before Zuckerberg wrote his first line of code. The underlying idea can be found in the writings of Plato (e.g., “similarity begets friendship”) and Aristotle (people “love those who are like themselves”), as well as in folk propositions such as “birds of a feather flock together.” The homophily principle also applies to groups, organizations, countries, or any aspect of social units.

Understanding the surrounding contexts and the related forces that act on a network helps with ML tasks in multiple ways. For example,

Networks are conduits for both wanted and unwanted flows. Marketers always try to reach and persuade people. Personal contact is most effective if one can find a way to start the conversation. This is the concept at the base of so-called viral marketing. Understanding such forces allows for the prediction of how networks evolve over time. This enables data scientists to proactively react to such changes or use them for specific business purposes.

There are findings in sociological and psychological disciplines that point to the relevance of a person’s social network in determining their tastes, preferences, and activities. This information is useful when building recommendation engines. One of the problems related to recommendation engines is the cold start problem: you can’t predict anything for a new user because you have no history about them. Social networks and the homophily principle can be used to make a recommendation based on the tastes of connected users.

A.3 Representing graphs

There are two standard ways to represent a graph, G = (V, E), in a suitable way for processing: as a collection of adjacency lists or as an adjacency matrix. Each way can be applied to directed, undirected, and unweighted graphs [7].

The adjacency list graph consists of an array (Adj) of lists, one for each vertex in V. For each vertex u in V, the adjacency list Adj[u] contains all the vertices v for which there exists an edge Euv between u and v in E. In other words, Adj[u] consists of all the vertices adjacent to u in G.

Figure A.11 shows an adjacency list representation (b) of the undirected graph on the left (a). For example, vertex 1 has two neighbors, 2 and 5, so Adj[1] is the list [2,5]. Vertex 2 has three neighbors, 1, 4, and 5, so Adj[2] is [1,4,5]. The other lists are created in the same way. It is worth nothing that because there is no order in the relationships, there is no specific order in the

lists; hence, Adj[1] could be [2,5] as well as [5, 2].

Figure A.11 An undirected graph (a) and the related representation as an adjacency list (b)

Similarly, figure A.12 provides an adjacency list representation (b) of the directed graph on the left (a). Such list is visualized as a linked list, where each entry contains a reference to the next one. For example, in the adjacency list for the node 1, the first element is node 2 with a reference to the next, the element for node 5. This is one of the most common approaches for storing the adjacency list because it makes addition and deletion of the elements efficient. In this case, we consider only the outgoing relationships, but the same can be done with the ingoing relationships. What is important is to choose a direction and keep it consistent during the adjacency list creation.

In figure A.12 vertex 1 has only one outgoing relationship with vertex 2, so Adj[1] is [2]. Vertex 2 has two outgoing relationships with the vertices 4 and 5, so Adj[2] is [4,5]. Vertex 4 has no outgoing relationships, so Adj[4] is empty ([]).

Figure A.12 A directed graph (a) and the related representation as an adjacency list (b)

If G is a directed graph, the sum of the lengths of all the adjacency lists is |E|. Because every edge can be traversed in a single direction, Euv appears only in Adj[u]. If G is an undirected graph, the sum of the lengths of all the adjacency lists is 2 * |E|. If Euv is an undirected edge, then Euv appears in Adj[u] and

Adj[v]. The memory required by an adjacency list representation of either a directed or an undirected graph is directly proportional to |V| + |E|.

We can easily adapt adjacency lists to represent weighted graphs by storing the weight w of the edge Euv in Adj[u]. The adjacency list representation can be similarly modified to support many other graph variants too. A disadvantage of this representation is that it provides no faster way to determine whether a given edge, Euv, is present in the graph than to search for v in the adjacency list Adj[u]. An adjacency matrix representation of the graph remedies this disadvantage, but at the cost of using asymptotically more memory.

For the adjacency matrix representation of a graph, G = (V, E), we assume that the vertices are numbered 1,2,...,|V| in some arbitrary manner and that these numbers are kept consistent during the life of the adjacency matrix. Then the adjacency matrix representation of a graph G consists of a |V| × |V| matrix A = (auv) such that auv = 1 if Euv exists in the graph; otherwise, auv = 0.

Figure A.13 shows the adjacency matrix representation (b) of the undirected graph on the left (a). The first line is related to vertex 1. This row in the matrix has a 1 in columns 2 and 5 because they represent the vertices to which vertex 1 is connected. All the other values are 0. The second row, related to vertex 2, has a 1 in columns 1, 4, and 5 because those are the connected vertices, and so forth through the remaining rows.

Figure A.13 An undirected graph (a) and the related representation as an adjacency matrix (b)

Figure A.14 shows the adjacency matrix representation (b) of the directed graph on the left (a). As for the adjacency list, we should choose one direction and use it during the matrix creation. In this case, the first row in the matrix has a 1 in column 2 because vertex 1 has one outgoing relationship to vertex 2; all the other values are 0. An interesting feature of the matrix representation is that by looking at the columns, it is possible to see the inbound relationships. For example, column 4 shows that vertex 4 has two inbounds connections from vertices 2 and 3.

Figure A.14 A directed graph (a) and the related representation as an adjacency matrix (b)

The adjacency matrix of a graph requires memory directly proportional to |V| * |V|, independent of the number of edges in the graph. In an undirected graph, the resulting matrix is symmetrical along the main diagonal. In such cases, it is possible to store only half of the matrix, cutting the memory needed to store the graph almost in half.

Like the adjacency list representation of a graph, an adjacency matrix can represent a weighted graph. For example, if G = (V, E) is a weighted graph and w is the weight of the edge Euv, auv will be set to w instead of 1. Although the adjacency list representation is at least as asymptotically space efficient as the adjacency matrix representation, adjacency matrices are

simpler, so you may prefer them when graphs are reasonably small. Moreover, adjacency matrices carry a further advantage from unweighted graphs: they require only one bit per entry.

Because the adjacency list representation provides a compact way to represent sparse graphs, those for which the number of edges is less than the number of vertices, it is usually the method of choice. But you may prefer an adjacency matrix representation when the graph is dense, when |E| is close to |V| × |V|, or when you need to be able to tell quickly if there is an edge connecting two given vertices.

A.4 References

1. A. Negro, Graph-Powered Machine Learning. Manning, June, 2021 2. L. Euler, “Solutio Problematis ad Geometriam Situs Pertinentis.”

Comment. Acad. Sci. U. Petrop 8 (1736): pp. 128–40 3. R. Diestel, Graph Theory. 5th ed. New York: Springer, 2017 4. V. Krebs, “Political Book Networks.” TNT: The Network Thinkers,

October 2012. http://www.thenetworkthinkers.com/2012/10/2012- political-book-network.html 5. D. Easley and J. Kleinberg, Networks, Crowds, and Markets: Reasoning

About a Highly Connected World. Cambridge, UK: Cambridge University Press, 2010 6. L. M. Verbrugge, “The Structure of Adult Friendship Choices.” Social

Forces 56 (1977): 576–597 7. T. H. Cormen, C. E. Leiserson, et al., Introduction to Algorithms. 3rd ed.

Boston, MA: MIT Press, 2009

[1] In this context the verb model is used in terms of representing a system or phenomenon in a simplified way. This model also aims at representing the data in such a way that it can be easily processed by a computer system.

[2] Mathematical Platonism (https://plato.stanford.edu/entries/platonism- mathematics/) is the metaphysical view that there are abstract mathematical objects whose existence is independent from us and our language, thought, and practices.

## Appendix_B._Neo4j

This appendix covers

Introduction to Neo4j database and its key features for graph data management Installation procedures for Neo4j Server and Desktop across different operating systems Essential Cypher query language basics and configuration of APOC and GDS plugins

Throughout the book, examples, code, and exercises are based on a specific graph database: Neo4j. Nevertheless, all the theories, the algorithms, and even the code can be easily adapted to work with any graph database on the market now and (with a good approximation) in the future. The reasons why I selected this database among others are

I’ve work with this database for the last 10 years (and I know it inside out). It is a native graph database (with all the consequences that it brings as explained throughout the book). It has a broad community of experts.

According to DB-Engines (https://db- engines.com/en/ranking_trend/graph+dbms), Neo4j has been the most popular graph DBMS for the last several years (figure B.1).[1]

Figure B.1 DB-Engines ranking of Graph DBMSs

This appendix provides the minimum amount of information necessary to get started with Neo4j and how to use it throughout the book. It covers a generic introduction about Neo4j, installation instructions, and a description of the Cypher language (the language used to query the database). We’ll also discuss the configuration of some plugins used in the examples.

B.1 Neo4j Introduction

Neo4j is available as a GPL3-licensed, open source Community Edition. Neo4j Inc. also licenses it with backup, scaling extensions, and other enterprise-grade features under its closed-source commercial terms: the Enterprise Edition. Neo4j is implemented in Java and is accessible over the network through a transactional HTTP endpoint or through the binary Bolt protocol (https://boltprotocol.org/). Neo4j is widely adopted due to the following features:

It implements a labeled property graph database.[2] It uses a native graph storage based on an index-free adjacency.[3] It provides native graph querying and a related language, Cypher (https://www.opencypher.org/), which defines how the graph database describes, plans, optimizes, and executes queries. Every architecture layer, from the queries using Cypher to the files on disk, is optimized for storing and retrieving graph data. It provides an easy-to-use developer workbench with a graph visualization interface.

Neo4j provides a full-strength, industrial-grade database, and transactional support is one of its many strengths. This differentiates it from many NoSQL solutions. Neo4j provides full ACID support [2], defined as:

(A) Atomicity—You can wrap multiple database operations within a single transaction and make sure that they’re all executed atomically. If one of the operations fails, the entire transaction is rolled back. (C) Consistency—When you write data to the Neo4j database, you can be sure that every client accessing the database will read the latest data. (I) Isolation— Operations in a single transaction are isolated one from another so that writes in one transaction won’t affect reads in another

transaction. (D) Durability—Neo4j writes your data to disk, and it becomes available after database restart or a server crash.

This support makes it easy for anyone used to traditional relational database guarantees to transition to Neo4j and makes working with graph data both safe and convenient. In addition to ACID transactional support, other features to consider when choosing the right database for an architectural stack include

Recoverability—This has to do with the database’s ability to set things right after a failure. Databases, like any other software system,

“. . . are susceptible to bugs in their implementation, in the hardware they run on, and in that hardware’s power, cooling, and connectivity. Though diligent engineers try to minimize the possibility of failure in all of these, at some point it’s inevitable that a database will crash. And when a failed server resumes operation, it must not serve corrupt data to its users, irrespective of the nature or timing of the crash. When recovering from an unclean shutdown, perhaps caused by a fault or even an overzealous operator, Neo4j checks in the most recently active transaction log and replays any transactions it finds against the store. It’s possible that some of those transactions may have already been applied to the store, but because replaying is an idempotent action, the net result is the same: after recovery, the store will be consistent with all transactions successfully committed prior to the failure [1].”

Moreover, Neo4j offers an online backup procedure that lets you recover the database when the original data is completely lost. In such case, a recovery to the last committed transaction is impossible, but it is better than losing all the data [1]. Availability—To increase the chance of recoverability,

“A good database needs to be highly available to meet the increasingly sophisticated needs of data-heavy applications. The database’s ability to recognize and, if necessary, repair an instance after crashing means that data quickly becomes available again without human intervention. And of course, more live instances increases the overall availability of the

database to process queries. It’s uncommon to want individual disconnected database instances in a typical production scenario. More often, we cluster database instances for high availability. Neo4j uses a master/slave cluster arrangement to ensure that a complete replica of the graph is stored on each machine. Writes are replicated out from the master to the slaves at frequent intervals. At any point, the master and some slaves will have a completely up-to-date copy of the graph, while other slaves will be catching up (typically, they will be but milliseconds behind) [1].”

Capacity—Related to the amount of data it is possible to store in a database or, in our specific case, in a graph database, the adoption of dynamically sized pointers in Neo4j 3.0 and higher allow the database to scale up to run any conceivably sized graph workload with an upper limit “in the quadrillions” of nodes [3].

Two excellent books on this topic are Graph Databases [1] and Neo4j in Action [2]. At the time of writing, the latest Neo4j version available is 4.2.x, so the code and the queries for this book were tested using this version.

B.2 Neo4j Installation

As mentioned, Neo4j is available in two editions: Community and Enterprise. You can download the Community Edition freely from the Neo4j website and use it indefinitely for noncommercial purposes, respecting the GPLv3 license (https://www.gnu.org/licenses/gpl-3.0.en.html). You can download the Enterprise Edition and try it for a limited time under specific constraints (it requires you to buy a proper license). The book’s code works perfectly with the Community Edition, so I recommend using it. That way, you have time to evaluate Neo4j. If you like, you can also use Neo4j packaged as Docker image.

Another option is to use the Neo4j Desktop GUI (https://neo4j.com/developer/neo4j-desktop/). The Neo4j Desktop is a sort of developer environment for Neo4j. You can manage as many projects and database servers locally as you like, and you can also connect to remote Neo4j servers. The Neo4j Desktop comes with a free developer’s license for

the Neo4j Enterprise Edition. From the Neo4j download page (https://neo4j.com/download-center/), you can select which one of the editions you would like to download and install.

B.2.1 Neo4j Server installation

If you decide to download a Neo4j server (either Community or Enterprise), the installation is quite straightforward. On Linux or Mac,

1. Make sure that you have Java 11 (or later) installed 2. Open your terminal/shell. 3. Extract the contents of the archive using tar xf <filecode> (for

example, tar xf neo4j-community-4.4.7-unix.tar.gz). 4. Place the extracted files in a permanent home on your server. The top-

level directory is referred to as NEO4J_HOME. 5. To run Neo4j

a. As a console application, use <NEO4J_HOME>/bin/neo4j console. b. As a background process, use <NEO4J_HOME>/bin/neo4j start. 6. Visit http://localhost:7474 in your web browser. 7. Connect using the username neo4j with the default password neo4j. You

will then be prompted to change the password.

On Windows machines, the procedure is quite similar. Unzip the downloaded file and proceed with the previous steps. At the end of the process, when you open the specified link in the browser, you’ll see something like the following image (figure B.2).

Figure B.2 An image of the Neo4j browser

The Neo4j browser, a simple web-based application, allows users to interact with a Neo4j instance, submitting queries and performing basic configurations. At this point, you should be ready to go.

B.2.2 Neo4j Desktop installation

When you download the Desktop, the installation procedure is available in the installation guide. If you need to, refer to that guide for your specific machine’s operating system. To get the Desktop up and running quickly, the directions for a MacOS are provided in this section.

1. In your Downloads folder, locate and double-click the .dmg file. This

starts the Neo4j Desktop installer. 2. Save the app to the Applications folder (either in the global one or in

your user-specific one) by dragging and dropping the Neo4j Desktop

icon to the Applications folder (figure B.3).

Figure B.3 Saving the Neo4j Desktop app on a MacOS

3. Locate the Neo4j icon in the Applications folder and double-click it to

launch the Desktop (figure B.4).

Figure B.4 Launching the Neo4j Desktop

4. Once you’ve activated the Desktop, click the New button, then select

Create project (figure B.5).

Figure B.5 Creating a new project in the Neo4j Desktop

5. Click the Add button and select Local DBMS (figure B.6).

Figure B.6 Adding and selecting a new local graph or connecting to an existing one

6. Enter the graph name and provide a password in the respective fields

and click the Create button (figure B.7).

Figure B.7 Entering a graph name and password. In the case of a new local graph, create an admin password.

7. Once you’ve created the database, you can start it by clicking Start

(figure B.8).

Figure B.8 Starting the new database instance

8. After the database starts, click the Open button to open the Neo4j

Browser in a new window (figure B.9).

Figure B.9 Opening the Neo4j Browser

The result will be the same as in the steps leading to figure B.2. You will have access to the browser, where you can interact with Neo4j.

If you would like to avoid all this effort, Neo4j has also a cloud version called Aura (https://neo4j.com/cloud/aura/). At the time of writing, they offer a free tier version if you want to play around a bit before jumping in to the examples and exercises in the book. Keep in mind that for the exercises and the learning curve, it would be better to have Neo4j installed locally on your machine or where you can run the Python code.

B.3 Cypher

Neo4j uses Cypher (https://neo4j.com/developer/cypher/) for its query language. As is the case in SQL (for which it is inspired), Cypher allows users to store and retrieve data from the graph database. Via Cypher, Neo4j

provides a language that’s easy-to-learn, understand, and use, but at the same time, it incorporates the power and functionality of other standard data-access languages.

Cypher is a declarative language for describing visual patterns in graphs using ASCII Art syntax. Using this syntax, it is possible to describe a graph pattern in a visual and logical way. The following is a simple example where we are looking for all the nodes of type Person in the graph.

Listing B.1 The simplest query

MATCH (p:Person) RETURN p

We can use this pattern for both searching for nodes and relationships in the graph or for creating them. It allows users to state what we want to select, insert, update, or delete from our graph data without describing exactly how to do that.

Cypher is not only used by Neo4j, but throughout the years, it has become open source! The openCypher (https://www.opencypher.org/) project provides an open language specification, technical compatibility kit, and reference implementation of the parser, planner, and runtime for Cypher. It is backed by several companies in the database industry and allows implementors of databases and clients to freely benefit from, use, and contribute to the development of the openCypher language.

Throughout the book, I provide examples and exercises to help you learn this language. If you would like to read more about Cypher, I would recommend the Neo4j’s guide (https://neo4j.com/developer/cypher/) as a good reference, and it’s chock full of examples.

B.4 Plugins installation

One of the remarkable things about Neo4j is how easily one can extend it. Neo4j allows developers to customize it in many ways, and you can enrich the Cypher language with new procedures and functions you can call when querying the graph. You can customize its security with authentication and

authorization plugins. Moreover, you can enable new surfaces to be created in the HTTP API via server extensions.

You’ll find a lot of pre-existing plugins to download, configure, and use. The most relevant are developed by Neo4j themselves, supported by the entire community because they are also open source. For the purposes of the book and the examples presented, we consider two of them:

The Awesome Procedures on Cypher library (APOC)—The APOC library is a standard utility library containing common procedures and functions. With over 450 procedures, it is believed to be the largest and most-widely used extension library for Neo4j. It provides functionality for utilities, conversions, graph updates, and more. This library is well-supported and is extremely easy to run as a separate function or to include in Cypher queries. This allows developers across platforms and industries to use a standard library for common procedures and to only write their own functionality for business logic and specific needs. The Graph Data Science library (GDS)—This library of procedures exploits the predictive power of relationships and network structures in existing data to answer previously intractable questions, increasing prediction accuracy. It is the analytics engine of Neo4j, which makes it possible to address complex questions about system dynamics and group behavior. Data scientists benefit from a customized, flexible data structure for global computations and a repository of powerful, robust algorithms to quickly compute results over tens of billions of nodes.

The next two subsections describe how to download, install, and configure these libraries. I recommend following the directions in these sections before reading chapter 4, where we start using Cypher queries.

B.4.1 APOC installation

Installing plugins in Neo4j is extremely simple. Let’s start with the APOC library (https://github.com/neo4j-contrib/neo4j-apoc-procedures/releases).

If you installed the server version, download the plugin from the related

GitHub release page (take the *-all.jar to get the full library and select the version that matches your version of Neo4j 4.1.x, 4.2.x, and so forth). Copy it to the plugins directory inside your NEO4J_HOME folder. At this point, edit the configuration file conf/neo4j.conf, adjusting or adding the following lines to that file:

dbms.security.procedures.unrestricted=apoc.* dbms.security.procedures.allowlist=apoc.*

Restart Neo4j and open the browser. Run the procedure in the following listing to check if everything is in place.

Listing B.2 Checking if APOC is correctly installed

CALL dbms.procedures() YIELD name WHERE name STARTS WITH "apoc" RETURN name

You should see a list of APOC procedures. If you are using the desktop version, it will be even simpler. After creating the database (see section B.2.2 up to the step 6), click the database name to open a sidebar on the right (figure B.10). Select the Plugins tab and then click the down-pointing arrow next to APOC to expand the box. Click the Install and Restart button to install the APOC library.

Figure B.10 APOC installation from the Neo4j Desktop

For further details and explanation, see the official APOC installation guide. You’ll find this at https://neo4j.com/labs/apoc/4.2/installation/.

B.4.2 GDS installation

You can follow a similar procedure to install the GDS Library. If you

installed the server version, download the plugin from the related GitHub release page (https://github.com/neo4j/graph-data-science/releases). Take the *-standalone.jar and copy it to the plugins directory inside your NEO4J_HOME folder. Now edit the configuration file conf/neo4j.conf, adjusting or adding the following lines:

dbms.security.procedures.unrestricted=apoc.*,gds.* dbms.security.procedures.allowlist=apoc.*,gds.*

Restart Neo4j and open the browser. Run the following procedure to check if everything is in place.

Listing B.3 Checking if GDS is correctly installed

RETURN gds.version()

You should be able to see the version of the GDS you downloaded. If you use the desktop version, it’s even simpler. After creating the database (see section B.2.2 up to step 6), click the database name to open a sidebar on the right (figure B.11). Select the Plugins tab and then click the down-pointing arrow next to Graph Data Science Library to expand the box. Click the Install and Restart button to install the GDS library.

Figure B.11 GDS installation from the Neo4j Desktop

After these steps, you are ready to have fun with Neo4j. Make sure to run all the examples and the exercises in the book.

B.5 Cleaning

Sometimes, you may need to completely clean up your database. You can do this using the functions available in the APOC library that you just installed into your database. Listings B.4 and B.5 provides the code.

Listing B.4 Deleting everything

CALL apoc.periodic.iterate('MATCH (n) RETURN n',

'DETACH DELETE n', {batchSize:1000})

Listing B.5 Dropping all constraints

CALL apoc.schema.assert({}, {})

B.6 References

1. Robinson, Ian, Jim Webber, and Emil Eifrem. 2015. Graph Databases.

2nd ed. Sebastopol, CA: O'Reilly. 2. Vukotic, Aleksa, Dominic Fox, Jonas Partner, Nicki Watt, and Tareq

Abedrabbo. 2014. Neo4j in Action. Shelter Island, NY: Manning. 3. Woodie, and Alex. 2016. Neo4j Pushes Graph DB Limits Past a

Quadrillion Nodes. April 26. https://www.datanami.com/2016/04/26/neo4j-pushes-graph-db-limits- past-quadrillion-nodes/.

[1] DB-Engines scoring considers multiple factors, spanning from the number of mentions on websites to the frequency of technical questions on Stack Overflow and DBA Stack exchange and from the number of jobs offered to its relevance in social network and so on.

[2] If you haven’t read it yet, see chapter 2, section 2.3.5, for details about the label property graph.

[3] For a discussion on graph representation, refer to appendix A.

## Appendix_C._Build_knowledge_graphs_from_structured_sources

This appendix covers

Knowledge acquisition from structured and semi-structured data sources Data reconciliation, entity merging and data cleaning Post processing and knowledge graph analytics

Earlier in this book (Chapter 4), we imported and explored three already existing knowledge graphs built by other people, organizations, or universities - including Hetionet, DisGeNET, and other biomedical knowledge graphs. It was a great experience to see immediately the value they can bring and how they can be useful to answer concrete questions. Hopefully we convinced you enough since now the hardest part is going to come. It is time to try the entire process by yourself. The reason why this task is so critical is because rarely you’ll be able to find an existing knowledge graph that fulfills all your needs, and it is ready to query and build your applications on top of it. In the real world, you must build it from scratch, you need to get your hands dirty and crunch different data sources, fight to perform reconciliation among different naming conventions and identifiers, clean your graph after you polluted with millions of relationships of which you care only of 10%, when you are lucky. Welcome to the real world of working with knowledge graphs (the hard way).

This chapter teaches you how to build your own knowledge graph from structured data sources. Some of them has some small text inside that we must normalize to merge entities, so we consider them semi-structured.

Even for this chapter we continue the biomedical use case. We found a use case that is simple enough to fit in a chapter but challenging enough to cover many of the relevant aspects that building process requires you to go through. The topic we decided to take is the detection of miRNA diseases associations, introduced in the Chapter 4 and that is discussed in more details here. The

next image shows the core of the knowledge graph we are going to build.

Figure C.1 An example of relationships between a disease, the celiac disease, and the miRNA. Many of these connections are known, but the vast majority must be discovered by clinicians and researchers. Our work in this chapter aims at helping this process.

We were even not sure to release the final version of the database, to force you to try the full process. At the end we decided to provide it anyway, to be sure that you can move ahead.

Now three things could have been happened: we scared you and then you already closed the book, or you get back and played with the exercises of the previous chapter, or we intrigued you so much that you can’t wait to start challenging yourself with the amazing task of knowledge graph building.

Are you still there? Cool! Let’s start then.

For this project we will use the same CRISP-DM Model adapted to Knowledge Graphs we have been used so far. We copied it once more in the figure C.2 below.

Figure C.2 CRISP-DM Model adapted to Knolwedge Graphs

According to this mental model, we start the chapter by introducing the key

concepts of the domain, then we introduce the business goals and the data we will use for achieving them. At that point we will be ready to start building our knowledge graph from scratch.

C.1 Micro RNA-disease association – Warm-up

As mentioned in chapter 4, micro-RNA-disease association is a very relevant use case for knowledge graphs in the biomedical space. In this section we introduce quickly the biological description of what micro-RNAs are and how they are connected to diseases, immediately after we describe the business goals and the data we have at our disposal to achieve those goals.

C.1.1 Key concepts

The micro-RNA (from now on referred as miRNA) is a relatively new discovered type of non-encoding RNA – i.e. RNA which is not going to be translated into a protein[1]. These very small molecules (containing from 19 to 22 nucleotides) interfere with complementary messenger RNAs (which are supposed to be translated into a protein) causing what is called "Gene silencing": the regulation or interference of the gene expression. MiRNAs accomplish silencing through a combination of translational repression and mRNA destabilization [2]

The following figure C.3 shows how the normal encoding works.

Figure C.3 Information encoded into DNA portions get transcribed into messenger RNA (mRNA) and then translated into amino acid chains that make up the synthesized protein.

While the figure C.4 describes how miRNAs affect it.

Figure C.4 MiRNA sequences are too small to translate into protein but big enough to target a specific mRNA. When this appends the mRNA cannot flow through the ribosome and the protein synthesis cannot take place.

Existing studies have shown that miRNAs are involved in many important biological processes [3, 4], like cell differentiation [5], proliferation [6], signal transduction [7], viral infection [8], and so on. Therefore, emerging evidence also implicates miRNAs in the pathogenesis of human complex diseases such as cancer and metabolic disorders [9, 10, 11, 12, 13, 14, 15]. This last aspect is where this example is going to focus. For example, researchers found that mir-433 miRNA is involved in gastric carcinoma by regulating the expression of GRB2, which is a known tumor-associated protein [16].

C.1.2 Business Understanding

Suppose that we want to predict connection between micro-RNAs (miRNA) and diseases. This would be a very useful feature for a very relevant task. The potential number of combinations is huge and in vitro experiments to validate assumptions are expensive. A prediction capability would reduce the spectrum of possible miRNA-disease relationships, to be validated experimentally, to the most probable. In such way, it will reduce the cost and the time of these discoveries.

For many miRNAs, the target gene is unknown, but we know the biological regulatory functions in which they are involved. Many thousands of miRNAs have been found so far and probably their number and impact is more important than was previously suspected [17].

In this context, due to the large number of discovered miRNA and the quantity of pathologies they could be involved in, predicting the correlations between miRNA and disease is something that can help researchers to narrow down their investigations to the most likely ones [18].

The scenario considered is related to the development of an intelligent advisor system, mentioned in chapter 2, that is an automated system providing support to researchers, physicians, biologists, and similar to identify the most relevant experiments to identify the miRNAs involved in certain diseases.

This business goal also determines the type of analysis to be performed on

the knowledge graph and the type of “advices” to be provided to the end users. Nevertheless, the focus of this chapter remains on the construction phase, but the understanding of the required analysis helps to refine the graph model itself, so it is important to be consider in the early stages of the process.

Of course, regardless the specific scenario used as concrete example of the step-by-step guide presented here, the approaches and the techniques can be immediately, almost without adjustments, adapted to any scenario. The goal here is to show how to construct a knowledge graph starting from existing structured data sources.

C.1.3 Data understanding

Even though the research on miRNA is relatively new, the amount of information available for this very specific field of research is huge and easy to find. At the time of writing, a platform called Tool4miRs[1] [19], claims to offer “all the tools you need to computationally analyze your miRNAs”. It contains a manually curated set of methods (above 170) for the broadly defined miRNA analysis and many different databases (in many cases resulting from the use of those tools for the analysis).

miRNAs operate in the same way in all living organisms, but, as for all our examples, we focus only on human processes. Moreover, we selected only the data sources that are relevant for our purposes. Each of the database used is presented just before we use it. A link to the database is provided, so you can check if there is a new version available. If this field is of interest for you and you would like to extend your knowledge graph, we highly recommend using Tool4miRs as your entry point.

C.2 miRNA knowledge graph building

The knowledge graph we are going to build must capture the complexity of the connection between miRNAs and pathologies. This should make it possible to design a machine learning model that exploits the data abundance and can learn how to predict missing links i.e. connections yet to be discovered. While we don't cover the prediction algorithms here, it's

important to understand the opportunities that this type of knowledge graph construction will enable.

One may think to start by ingesting every available dataset about anything that is somehow related to miRNAs and diseases and then let the machine learning model to deal with that. This "greedy" approach assumes that having more data resolves to capture better the complexity of the matter, which in turn should give the machine learning model more chances to figure out the true underling rules that govern the relationship between miRNAs and disease.

While this is true in theory, throwing everything into the knowledge graph is not a wise choice in the practice for many reasons:

Every dataset comes with its signal to noise ratio and, even in good datasets, the noise can easily overwhelm the benefits of the signal if the dataset is only slightly related to the link prediction. New datasets need to be reconciled to the existing knowledge graph before any graph machine learning algorithm can take the full advantaged of the new information. Sometimes the reconciliation process is not straightforward, and the resulting relationships could be affected by errors, this can again amplify the noise associated to the new dataset.

We would rather start by selecting a proper number of datasets related to the miRNA to serve the pathology link prediction task. This approach will result in a knowledge graph with a relatively low degree of complexity which is easier to reason about. The link prediction results coming from this knowledge graph can be used as baseline, so it is possible to quantify how new data sources can improve the model.

C.2.1 Importing miRNA-Disease known connection

Let's start in the most obvious way by identifying datasets containing known connections between miRNAs and diseases. The data sources selected for this first ingestion round are Human miRNA Disease Database (HMDD)[2][20, 21, 22], Database of Differentially Expressed miRNAs in Human Cancers

(dbDEMC)[3][23] and miR2Disease[4][24]. These datasets, described later in slightly more details, come from different sources and are the result of different research efforts therefore they encode the relations we are looking for in different ways and contain different side information.

For this reason, each dataset will be processed and imported in a slightly different way, and they will converge in the single graph. The following image shows an example of how the merge will happen in practice with two toys datasets.

Figure C.5 Datasets A is more balanced and includes reference publications, Datasets B is focused on fewer diseases but includes more miRNA. After the ingestion the information coming from different sources is unified in a single source of truth.

Considering the two sources, the relevant information contained and the goal we have, the following schema could suite our needs at this point.

Figure C.6 First iteration target Schema. It contains the type of relationships – REGULATES or RELATED_TO – we would like to predict.

Here we are using two types of relationships, one for each of the two datasets we will be importing. This choice allows us to preserve the nuances of meaning that the two datasets give to the relationship. Additionally, it enables us to quickly identify which of the two sources a particular relationship comes from when observing the result of a query. Later on, when we build our model, we can easily merge these two types of relationships if necessary.

We will begin by importing the HMDD. It is a curated experiment-supported evidence for human miRNA and disease association. This dataset will provide a solid foundation for our knowledge graph being both manual curated and capturing exactly the miRNA to disease link we want to predict. Listing C.1 shows the implementation of the HMDD importer class that handles the data ingestion process.

Listing C.1 Import HMDD dataset

class HMDDImporter(BaseImporter): #A

def __init__(self, argv): super().__init__(command=__file__, argv=argv) self._database = "hmdd2.0"

def get_rows(self, HMDD_file): #B with open(HMDD_file, 'r+', encoding="latin-1") as in_file reader = csv.reader(in_file, delimiter='\t') #D header = next(reader) #E for row in reader: yield dict(zip(header, row)) #F

def import_HMDD(self, HMDD_file): #G query = """ UNWIND $batch as item #H WITH trim(toLower(item.disease)) as disease, toLower(item.mir) as mir,item #I MERGE (d:Disease {name: disease}) #J MERGE (m:MiRNA {name:mir}) #K SET m:MiRNA_HMDD MERGE (m)-[r:RELATED_TO]->(d) #L SET r.description = item.description, r.pmid=item.pmid, r.category = item.category MERGE (ref:Reference {pubmed_id:item.pmid})

MERGE (m)-[:HAS_REFERENCE]->(ref) #M """ size = self.get_csv_size(HMDD_file, encoding="latin-1"). self.batch_store(query, self.get_rows(HMDD_file), size=size, strategy="aggregate") #O

def set_constraints(self): with self._driver.session(database=self._database) as ses query = """ #P CREATE CONSTRAINT ON (a:Disease) ASSERT a.name IS UNI CREATE CONSTRAINT ON (a:MiRNA) ASSERT a.name IS UNIQU CREATE CONSTRAINT ON (a:Reference) ASSERT a.pubmed_id IS UNIQUE; CREATE CONSTRAINT ON (a:Target) ASSERT a.name IS UNIQ for q in query.split(";"): #Q try: session.run(q) except Neo4jClientError as e: #R # ignore if we already have the rule in place if (e.code != "Neo.ClientError.Schema.EquivalentSchemaRuleAlreadyExi raise e [...] if __name__ == '__main__':

importing = HMDDImporter(argv=sys.argv[1:]) importing.set_constraints() importing.import_HMDD(HMDD_file) importing.close()

Listing C.2 contains an example of a generated dictionary for a HMDD record:

Listing C.2 HMDD record sample

{'category': 'genetics_GWAS',

'mir': 'hsa-mir-502', 'disease': 'Carcinoma, Renal Cell, Clear-Cell', 'pmid': 27346408, 'description': "Polymorphism at the miR-502 binding site in the

This record tells us the fact that the miRNA called hsa-mir-502 is connected to the kidney cancer called Clear cell Renal cell carcinoma and that this connection is supported by experiments described by the scientific publication identified by the PubMed Id: 27346408. We capture this

information by creating the relevant miRNA and Disease nodes and connecting them through the RELATED_TO relationship.

Figure C.7 Record as a graph portion stored in the database.

After this import, we collected 1207 distinct miRNAs, 18732 distinct connections between these miRNAs and 849 distinct diseases, which represents a solid foundation for our link prediction task.

The second dataset we import is the dbDEMC. It is an integrated database containing differentially expressed miRNA focused on human cancer. It contains a collection of 403 miRNA expression datasets obtained by both microarray platform and miRNA sequencing. Listing C.3 shows the implementation of the dbDEMC importer that processes this specialized cancer-related miRNA data

Listing C.3 Import dbDEMC dataset

class DBDEMCImporter(BaseImporter):

[...]

@staticmethod def get_rows(miRNA_file): #A with open(miRNA_file, 'r+') as in_file: reader = csv.reader(in_file, delimiter='\t') header = next(reader) for row in reader: if len(row) < 2: #B continue record = dict(zip(header, row))

if record["Species"] != "Homo sapiens": #C continue

if len(record["CancerSubtype"]) > 1: #D disease = record["CancerSubtype"].lower() else: disease = record["CancerType"].lower()

disease = (disease.replace(",", "") .replace("/", " ") .replace("-", " "))

name = record["miRNA_ID"].lower().strip(). #E

yield { #F "name": name, "disease": disease, "experiment": record["ExperimentID"], "regulated": record["Status"] }

def import_dbDEMC(self, miRDB_file): #G exact_match_query = """ UNWIND $batch as item #H MERGE (m:MiRNA {name: item.name}) #I SET m:MiRNA_dbDEMC WITH m,item MERGE (n:Disease {name: item.disease}) #J SET n:DiseaseDbDEMC, n.name_in_db_demc = item.dis MERGE (m)-[r:REGULATES {regulated: item.regulated SET r.source = 'dbDEMC', r.experiment = item.expe """ size = self.get_csv_size(miRDB_file) #L self.batch_store(exact_match_query, self.get_rows(miRDB_file), size=size) #

Listing C.4 contains an example of a record generated for this dataset:

Listing C.4 dbDEMC record sample

{'name': 'hsa-miR-155',

'disease': 'glioblastoma', 'experiment': 'EXP00065', 'regulated': 'UP'}

This specific record is telling us that the experiment EXP00065 is showing that the miR-155 miRNA levels are elevated in Glioblastomas tumors compared with healthy adult brain tissue (this miRNA is up-regulated in this tumor cells).

In another experiment we learn, for example, that the miR-193b miRNA was significantly down-regulated in Melanomas tissues compared with nevi tissues, in other words the levels of this miRNA are lower on diseased tissues.

By using the MERGE clause – which doesn’t create new nodes if they already exist –, we are improving the richness of our knowledge base. Let's keep the miR-155 miRNA example. We know from the HDDB dataset (our first import) that mir-155 is considerably over-expressed in active Multiple Sclerosis lesions versus controls. The same dataset also tells us that increased miR-155 levels are registered in Ischemic Cardiomyopathies (a disease affecting the heart muscle caused by a narrowing of the coronary arteries which supply blood to the heart).

At the end of the second ingestion (from dbDEMC), we will see that the same node representing this specific miRNA is connected to both tumor and non- tumor brain diseases (Glioblastomas and Multiple Sclerosis respectively) and it is also connected to brain and hearth diseases (Glioblastomas and Ischemic Cardiomyopathies respectively).

Figure C.8 Merging datasets allows information fusion. We are using two types of relationships for the two databases: RELATED_TO for the HMDD dataset, REGULATES for dbDEMC.

Now suppose that all these three pathologies are connected somehow to some excessive inflammation promoted by elevated levels of miR-155, suppose also another new pathology is also correlated to the same inflammation process. In this case the link prediction model has all the necessary information to infer a not (yet) documented relationship between the miR-155 miRNA and this new pathology.

We can enrich our knowledge graph further with a third dataset, the miR2Disease database. miR2Disease is a manually curated database that provides information about miRNA deregulation in various human diseases. It also provides a submission page that allows researcher to submit new miRNA-disease relationships so we can expect it to grow over time. Listing C.5 demonstrates the implementation of the miR2Disease importer that integrates this resource into our knowledge graph.

Listing C.5 Import miR2Disease dataset

class Mir2DiseaseImporter(BaseImporter):

[...] def get_rows(self, miRDB_file): #A with open(miRDB_file, 'r+') as in_file: reader = csv.reader(in_file, delimiter='\t') for row in reader: if len(row) < 2: #B continue yield { #C "name": row[0].lower(), "disease": row[1].lower(), "regulated": row[2] }

def import_miR2Disease(self, miR2Disease_file): #D query = """ UNWIND $batch as item #E MERGE (d:Disease {name: item.disease}) #F MERGE (m:MiRNA {name:item.name}) #G MERGE (m)-[r:REGULATES]->(d) #H SET r.regulation = item.regulated """ size = self.get_csv_size(miR2Disease_file) #I self.batch_store(query, self.get_rows(miR2Disease_file), size=size, strategy="aggregate"). #J

Once these three datasets are imported, our knowledge graph consists of 4874 distinct miRNAs, 118806 distinct connections between these miRNAs and 1144 distinct diseases.

We can see how the unique miRNAs are distributed amongst the three datasets. Listing C.6 shows the Cypher query that computes this distribution, allowing us to analyze the overlap and unique contributions of each data source.

Listing C.6 Compute the ingested miRNAs distribution.

MATCH (n:MiRNA) #A WITH

DISTINCT LABELS(n) AS labels, #B COUNT(*) as count RETURN

[l in labels where "MiRNA"<> l ] AS labels, #C Count ORDER by count DESC

The result of the query will look like:

[MiRNA_dbDEMC] 2550 [MiRNA_HMDD, MiRNA_dbDEMC] 583 [MiRNA_HMDD, MiRNA_dbDEMC, MiRNA_miR2Disease] 328 [MiRNA_HMDD] 280 [MiRNA_dbDEMC, MiRNA_miR2Disease] 84 [MiRNA_miR2Disease] 32 [MiRNA_HMDD, MiRNA_miR2Disease] 15

A better representation of the results above is proposed in the figure C.9 which shows them as a Venn diagram.

Figure C.9 Venn diagram representing the unique miRNA distribution. It clearly shows the overlapping miRNAs among 2 or three of the datasets.

We can see that there is a fairly balanced distribution between shared and not shared miRNAs. This is relevant since, for shared miRNAs our knowledge graph, and the next machine learning tasks, will benefit from knowledge coming from multiple datasets while the other are representing the unique contribution of every dataset we ingested so far.

This figure also makes it clear how the cancer research is interested in exploring as much unique miRNAs as possible since the dbDEMC dataset is contributing with more than two thousand five hundred miRNAs which are not present in other datasets.

C.2.2 Importing disease ontology

If we look closely to the imported data, we can notice that different data sources can use different terms to refer to the same disease. This is a very common problem since often different datasets, even when related to the same topic, may use different standards in defining objects. In the databases we’ve seen in the previous chapters this was a clear problem mitigated by the researchers creating the knowledge graphs. This happens to be very frequent in the case of biological or medical dataset.

One detrimental effect of this misalignment is that, for example, two miRNAs would be connected to two apparently different diseases, while they are just referring the same disease with different names.

The following is one clear example of the typical problem here:

Figure C.10 miRNAs apparently referring to different diseases.

In this case each dataset referred to Burkitt Lymphoma with a different spelling, relying on different disease naming conventions and thus resulting in three different disease nodes.

As consequence of this the knowledge graph cannot be considered correct. As you know, the knowledge graph is a representation of real-world entities, if one concept is represented by different entities, the graph ceases to be a reliable source of truth. Furthermore, when we will perform our link

prediction task, the model would be misled to believe that the cited miRNAs are disconnected, preventing it to learn properly. In general, when this type of misalignment happens, every representation we build on top of it will be proportionally deteriorated.

Luckily there are many ontologies available that we can use to "normalize" the nomenclature of disease coming from different datasets. We will use here the Unified Medical Language System (UMLS[5]) [25] ontology. The UMLS integrates and distributes key terminology, classification and coding standards, and associated resources to promote creation of more effective and interoperable biomedical information systems and services, including electronic health records.

To make our life easier we will use scispaCy[6], a Python package containing specifically spaCy models for processing biomedical, scientific, or clinical text.

ScispaCy can perform automatic named entity recognition of UMLS entities, returning Canonical name, Concept Id and Type Id for every identified entity in the disease name property. We can exploit scispaCy to automatically infer the canonical name for every disease node we ingested, generating new NormalizedDisease nodes that we will connect equivalent Diseases nodes. The decision to add a new node to our schema – instead of just merging the three nodes in a single node – has been taken since we would like to keep the original structure. This is useful for many reasons. First, it will be much easier to check the results and, potentially, correct them. Secondly, resetting everything and re-running, if necessary, it will be way more straightforward.

Figure C.11 Update target schema containing normalized Disease.

Listing C.7 demonstrates the implementation of the disease normalization process, which leverages NLP techniques to standardize disease entities across our different data sources.

Listing C.7 Normalize Diseases

class Reconciliator(BaseImporter): #A

def __init__(self, argv): super().__init__(command=__file__, argv=argv) self._database = "hmdd2.0" self.resolver = DiseaseResolver()

def get_normalized_diseases(self): #B with self._driver.session(database=self._database) as ses diseases_data = session.run(""" MATCH (d:Disease) RETURN id(d) as id, d.name as name""").data() #C

diseases_text = [d["name"] for d in diseases_data]. #D disease_ids = [d["id"] for d in diseases_data] #E

diseases_text = [ " ".join(i for i in reversed(d.split(","))).strip() for d in diseases_text] #F

# parse disease texts using scispacy model diseases_items = [self.resolver.nlp(disease) for disease in diseases_text] #G

disease_normalized = [ self.resolver.normalize(item) for item in diseases_items] #H

diseases = [{ "source_id": disease_id, "name": disease_name, "umnls_id": disease_UMNLS_ID} for disease_id, (disease_name, disease_UMNLS_ID) in zip(disease_ids, disease_normalized)] #I

return diseases

def import_normalized_diseases(self): #J

query = """ UNWIND $batch as item #K MATCH (d:Disease) WHERE id(d) = item.source_id #L MERGE (nd:NormalizedDisease {name:item.name}) #M SET nd.umnls_id = item.umnls_id MERGE (d)-[:REPRESENTS]->(nd) #N """ diseases = self.get_normalized_diseases() self.batch_store(query, iter(diseases), size=len(diseases), strategy="aggregate"

Listing C.8 contains the resolution logic is encapsulated within the DiseaseResolver:

Listing C.8 Diseases Resolver Class

class DiseaseResolver:

full = ["Finding", "Organ or Tissue Function", "Tissue"] #A banned = ["Human", "Body Part, Organ, or Organ Component", "Qualitative Concept", "Temporal Concept", "Functional Concept", "Body Space or Junction", "Spatial Concept"] #B

def __init__(self): self.nlp = nlp = spacy.load("en_core_sci_sm") #C config = { "resolve_abbreviations": True, "linker_name": "umls"} nlp.add_pipe("scispacy_linker", config=config) #D linker = nlp.get_pipe("scispacy_linker") #E self.type_tree = linker.kb.semantic_type_tree. #F self.cui_to_entity = linker.kb.cui_to_entity #G

def canonical(self, entity): #H """get canonical name from entity""" entities = entity._.kb_ents if len(entities) == 0: return # select the first entity return self.cui_to_entity[entities[0][0]].canonical_name

def types(self, entity): #I """return semantic types for the entity""" entities = entity._.kb_ents if len(entities) == 0:

return [] return [self.type_tree.get_canonical_name(t) for t in self.cui_to_entity[entities[0][0]].types

@staticmethod def matchesAll(entity): #J """return trie if the entity covers the whole content""" return entity.start == 0 and entity.end == len(entity.doc

def containsOnly(self, entity, targets): #K """return true if the entity types are within the target types""" intersection = set(self.types(entity)).intersection(targe return (intersection == set(self.types(entity)))

def validEntity(self, entity): #L """ exploits the entity types to detect if an entity is correctly identified as disease """ if self.containsOnly(entity, self.banned): #M return False if self.containsOnly(entity, self.full): return self.matchesAll(entity) #N return True #O

def normalize(self, item): """"main entrypoint: convert item into a normalized disea return ( normalized_name, UMNLS_ID | None ) """ # single entity found if len(item.ents) == 1: return self.normalize_entity(item) #P # multiple entityies found falling back to default if len(item.ents) > 1: return self.normalize_default(item) #Q # otherwise return self.normalize_default(item) #R

def normalize_entity(self, item): #S """ normalize item when there is only one detected entity entity = item.ents[0] if self.validEntity(entity): #T return self.canonical(entity), entity._.kb_ents[0][0] return self.normalize_default(item) #U

def normalize_default(self, item): #V """When no other better options are available return capitalized version of disease"""

item = str(item) item = " ".join(i.capitalize() for i in item.split()) return item.strip(), None #W

After this normalization step, we can see that the three different Burkitt Lymphoma nodes with different spelling are now connected by a single NormalizedDisease node.

Figure C.12 miRNAs now connected to the through the NormalizedDisease node “Burkitt Lymphoma”.

We have effectively reduced the graph fragmentation by adding an extra node connecting some previously disconnected component. We can evaluate the graph connectivity – with and without the normalization nodes – using the weakly connected component (WCC) algorithm that we used before for other knowledge graphs. With WCC we can detect sets of connected nodes that form a single connected component, or, in other words, we can identify how many disconnected subgraphs do exist in our graph, labelling each node after the disconnected subgraph it belongs.

The intuition is that through our normalization we may have connected previously disconnected graphs. We can measure this by running WCC before and after the normalization process and compare the results.

Before running WCC, as any other GDS algorithm, we have to create a named in-memory representation of the graph we are going to analyze. In this case we will create two representations, one with the NormalizedDisease nodes and relative relationships and one without. Listing C.9 shows the Cypher queries needed to project these two graph representations into memory, setting up our environment for comparative analysis.

Listing C.9 in-memory graphs creation

CALL gds.graph.project( #A "not-normalized", #B ["MiRNA","Disease"], #C ["REGULATES","RELATED_TO"]); #D

CALL gds.graph.project( "normalized", #E ["MiRNA","Disease","NormalizedDisease"], #F ["REGULATES","RELATED_TO","REPRESENTS"]) #G

We can run now the WCC algorithm once over both in-memory representations and then compare the results. Listing C.10 demonstrates how to execute the Weakly Connected Components algorithm on the non- normalized graph, providing insights into the connectivity structure before our disease normalization process.

Listing C.10 WCC run over the not normalized graph representation

CALL gds.wcc.stream('not-normalized') #A YIELD nodeId,componentId #B RETURN componentId AS subgraph, count(nodeId) AS componentSize

The results of the WCC algorithm on the non-normalized graph provide insights into the initial connectivity structure of our knowledge graph. Table 5.1 shows the distribution of components before normalization, revealing the baseline connectivity patterns that we'll compare against after our disease entity standardization.

Table C.1 WCC Components distribution before normalization

Subgraph ComponentSize 0 5010 1166 3 1838 2

After examining the connectivity of the non-normalized graph, we can perform the same analysis on our normalized representation to assess the impact of disease standardization. Listing C.11 shows the execution of the WCC algorithm on the normalized graph, allowing us to measure how entity normalization affects the overall connectivity structure.

Listing C.11 WCC run over the normalized graph representation

CALL gds.wcc.stream('normalized') #A YIELD nodeId,componentId #B RETURN componentId AS Subgraph, count(nodeId) AS ComponentSize

The results of the WCC algorithm on the normalized graph reveal changes in the connectivity structure following our disease entity standardization. Table C.2 presents the component distribution after normalization, enabling a direct comparison with the pre-normalization state to quantify the improvements in graph cohesion.

Table C.2 WCC Components distribution after normalization

Subgraph ComponentSize 0 6033

1166 4 1838 3

As we can see there are no significative changes if we compare the graph structures before and after the normalization since we have just a single big, connected component containing almost every node.

While having a well-connected graph with negligible fragmentation, is good news for most graph applications, here we have to resort to other techniques to quantify the impact of the normalization step.

The GDS library contains several community detection algorithms that can serve the purpose of statistically evaluating the structural changes we get after the normalization process. Nevertheless, the differences between the results we get before and after the normalization are more difficult to interpret when we use other community detection algorithms compared to WCC.

Let’s consider the subgraph in Figure C.13

Figure C.13 Chain of miRNAs connected through Disease nodes

Here hsa-mir-199a* is connected to hsa-mir-182 through hepatocellular carcinoma (hcc) and, at the same time, hsa-mir-182 is connected to hsa-mir- 4728 through carcinoma, thyroid, papillary.

We can say that the distance between hsa-mir-199a* and hsa-mir-182 is equal to one since the shortest path connecting the two miRNAs contains only one Disease node.

At the same time, the distance between hsa-mir-199a* and hsa-mir-4728 is equal to two since in this case the shortest path passes through no less than two Disease nodes.

However, we know now that Burkitt's lymphoma and lymphoma, Burkitt are actually the same disease thus the distance between hsa-mir-199a* and hsa- mir-4728 should be, in fact, one. Figure C.14 illustrates the same chain of miRNAs after disease normalization, showing how the previously disparate disease nodes are now connected through the shared NormalizedDisease

node, effectively reducing the path lengths between related miRNAs.

Figure C.14 The same chain of miRNAs in Figure C.13 after the connection through NormalizedDisease nodes.

In general, we expect the distances between miRNAs to become shorter after the disease normalization. We can effectively measure these distances leveraging the All Pairs Shortest Path (APSP) algorithm, available in alpha tier at the time of writing, within the Path finding algorithms of Neo4j GDS library.

We will create a first in-memory graph representation where two miRNAs nodes will be connected only if there exist at least one Disease node connected to both of them. This projection will represent the state of the

graph before the normalization step.

We will also create a second in-memory graph representation where two miRNAs’ nodes are connected if there exists a chain (Disease)-[]- (NormalizedDisease)-[]-(Disease) connecting them. This projection will represent the state of the graph after the normalization step. Listing C.12 shows the Cypher query that creates this in-memory graph connecting miRNAs through their shared Disease nodes.

Listing C.12 In-memory miRNA to miRNA connection through Disease

call gds.graph.project.cypher( #A

"DiseaseDistance", #B "MATCH (n:MiRNA) return id(n) as id", #C "MATCH (a:MiRNA)-[:REGULATES|RELATED_TO]-> (:Disease)<-[:REGULATES|RELATED_TO]-(b:MiRNA) #D WHERE id(a)<id(b) #E RETURN distinct #F id(a) as source, id(b) as target") #G

Listing C.13 will generate an in-memory graph which leverages the NormalizedDisease nodes created to normalize the diseases.

Listing C.13 In-memory miRNA to miRNA connection through NormalizedDisease

call gds.graph.project.cypher( #A

"NormalizedDiseaseDistance", #B "MATCH (n:MiRNA) return id(n) as id", #C "MATCH p1=(a:MiRNA)-[:REGULATES|RELATED_TO]->()-[:REPRESENTS] MATCH p2=(d)<-[:REPRESENTS]-()<-[:REGULATES|RELATED_TO]-(b:M WHERE id(a)<id(b) #E RETURN distinct #F id(a) as source, id(b) as target") #G

These queries may require some time to execute, in general, calls to gds.graph.project are faster compared to gds.graph.project.cypher, since the firsts leverage information about the graph nodes and edges that are already present and stored in the database.

On the other hand, cypher projection are more flexible and definitely more

useful for exploratory and debug purposes because one can potentially project one graph to another using complex computation similar to what we just did.

If we were working in a production environment where we need to frequently build this in-memory representations, it may be beneficial to materialize the miRNA-to-miRNA links by storing the relevant relationships into the database. Once the projections are ready we can run the APSP algorithms and compare the results.

Listing C.14 computes the distance before normalization.

Listing C.14 APSP run on pre normalization graph.

CALL gds.allShortestPaths.stream('DiseaseDistance',{}) #A YIELD distance RETURN distinct distance, count(distance) AS Count #B

Listing C.15 computes the distance after normalization.

Listing C.15 APSP run on post normalization graph.

CALL gds.allShortestPaths.stream('NormalizedDiseaseDistance',{}) YIELD distance RETURN distinct distance, count(distance) AS Count #B

The results of the previous listings are condensed in the next table.

Table C.3 Distance distributions

Distance Paths count before Path count after Variation 1 5911305 6179329 +4.5% 2 1244305 1010851 -18.7% 3 35795 25888 -27.6% 4 870 612 -29.6% 5 46 22 -52.1% 6 1 0 -100%

We can see that the count of the miRNAs pairs at distance 1 is increased by

about 5% which means that the distance for these pairs is actually shortened. The decreasing we observe at the other distances also indicates that there is a shifting toward shorter distances.

This is an important analysis and a great achievement. As we will see later in part three, many of the embeddings’ techniques are message passing based, which means that in the process they leverage the relationships to pass messages that will be used for computing the new embeddings at each iteration. Proper shorter connections among relevant nodes will directly relate to a higher quality of the final embeddings.

C.2.3 Using Large Language Models for entity normalization

While our approach using scispaCy successfully normalized disease entities like 'Burkitt Lymphoma', 'lymphoma, Burkitt', and 'Burkitt's lymphoma', Large Language Models (LLMs) could provide an alternative solution in situations where specialized biomedical NLP tools are unavailable or unsuitable. LLMs benefit from their exposure to vast corpora of biomedical literature during pre-training, giving them an inherent understanding of terminology variations.

LLMs could potentially address entity reconciliation challenges in several ways:

Handling terminology variations: LLMs can recognize semantic equivalence between terms even when they don't follow predictable transformation patterns. For example, recognizing that "Gastric adenocarcinoma" and "Stomach cancer" refer to the same entity despite using entirely different words. Domain-agnostic application: The entity reconciliation approach demonstrated here isn't limited to biomedical domains. LLMs could provide similar normalization capabilities in various domains where specialized tools like scispaCy aren't available, such as legal documents, financial reports, or technical specifications. Zero-shot capabilities: Unlike our current approach that required specific ontologies like UMLS, LLMs might perform reasonably well at entity normalization without requiring external knowledge bases,

particularly for common entities.

Despite these potential benefits, key limitations should be considered. The probabilistic nature of LLM outputs may lead to inconsistent entity mappings across different runs, potentially compromising the reproducibility of the knowledge graph construction process. Moreover, deploying large language models for entity reconciliation at scale would require substantial computational resources compared to more lightweight approaches like scispaCy.

C.2.4 Importing miRNA information

So far, we ingested datasets containing known miRNA to Disease associations and we improved the quality of the diseases relationships through disease normalization and merge.

Both those steps go in the direction of obtaining rich and high-quality relationships to provide a solid foundation for the link prediction task. As we will see later in the book, many of the graph-based machine learning algorithms use a message passing paradigm that are driven by the topology of the graph data. This means that information coming from regions of the graph with rich and high-quality connections, is easier to exploit when it comes to learn a model compared to information that live in a poorly or badly connected region.

We can take some further step in the enrichment of the knowledge graph by ingesting other datasets that provide information about miRNA and connections among them.

Figure C.15 Updated version of the target schema.

As we can see from the updated schema in Figure C.15, some of the new relationships we are going to store connects miRNAs to each other, giving

direct information about miRNA similarities and relations. Others, like miRNA to Target and miRNA to Reference, are giving indirect connection information. If two miRNAs bind to the same Target messenger RNA, for example, they are similar in the sense that they are regulating – or silencing – the same gene expression. Similarly, two or more miRNAs cited in the same publication give us some clue about the fact that they somehow related at least from the authors' perspective.

Figure C.16 presents this idea with an example dataset.

Figure C.16 Example of direct similarity and similarity induced by shared nodes. The model will learn which similarity is most relevant for the link prediction task.

Let's start by importing the miRBase[7] dataset[26, 27, 28, 29], a searchable database of about two hundred published miRNA sequences and annotations. For each miRNA it reports a list of relevant publications mentioning it as well as a list of connected miRNAs. Listing C.16 shows the implementation of the miRBase importer that handles the extraction and integration of this miRNA reference database into our knowledge graph.

Listing C.16 Import miRBase dataset

from Bio import SeqIO

class BioImporter(BaseImporter):

[...] @staticmethod def get_rows(miRNA_dat): #A for record in SeqIO.parse(miRNA_dat, "embl"): #B if not record.name.startswith("hsa"): #C continue if len(record.name) < 2: continue yield { "name": record.name.lower(), "description": record.description, "seq": str(record.seq), "comment": record.annotations.get('comment', ''), "references": [ {"authors": r.authors, "title": r.title, "pubmed_id": r.pubmed_id, "journal": r.journ for r in (record.annotations .get('references', []))], #D "features": [ {"type": r.type, "accession": (r.qualifiers .get('accession', [""])[0]), "name": (r.qualifiers .get('product', [""])[0] .lower())} for r in record.features if r.type == "miRNA" }

def import_miRNA_dat(self, miRDB_file): query = """ UNWIND $batch as item #F MATCH (m:MiRNA {name: item.name}) #G SET

m:MiRNA_miRBase, m.description = item.description, m.seq = item.seq, m.comment = item.comment #H WITH m,item FOREACH (feature in item.features | #I MERGE (f:MiRNA {name: feature.name}) #J MERGE (m)-[:HAS_FEATURE]->(f) #K ) WITH m,item UNWIND item.references as reference #L MERGE (r:Reference {pubmed_id: reference.pubmed_i ON CREATE SET r.authors = reference.authors, r.title = reference.title, r.journal = reference.journal MERGE (m)-[:HAS_REFERENCE]->(r) #N """ size = self.get_embl_size(miRDB_file) #O self.batch_store(query, self.get_rows(miRDB_file), size=s

Listing C.17 shows an example of the records provided by miRbase. Without providing too much biological background we can say that it contains a lot of relevant information that allows us to connect miRNAs among themselves and to reference articles.

Listing C.17 miRBase record sample.

name: hsa-let-7a-1 description: Homo sapiens let-7a-1 stem-loop comment:

let-7a-3p cloned in [6] has a 1 nt 3' extension (U), which is incompatible with the genome sequence. seq:

UGGGAUGAGGUAGUAGGUUGUAUAGUUUUAGGGUCACACCCACCACUGGGAGAUAACU AUACAAUCUACUGUCUUUCCUA features:

- accession: MIMAT0000062 name: hsa-let-7a-5p type: miRNA - accession: MIMAT0004481 name: hsa-let-7a-3p type: miRNA references:

- authors: Lagos-Quintana M, Rauhut R, Lendeckel W, Tuschl T journal: Science. 294:853-858(2001). pubmed_id: 11679670 title:

Identification of novel genes coding for small expressed RNA - authors: Suh MR, Lee Y, Kim JY, Kim SK, Moon SH, Lee JY, Cha KY, Chung HM, Yoon HS, Moon SY, Kim VN, Kim KS journal: Dev Biol. 270:488-498(2004). pubmed_id: 15183728 title: Human embryonic stem cells express a unique set of miRNAs [...]

The next dataset to be ingested is miRDB[8][30, 31], an online database for miRNA target prediction and functional annotations. As discussed before, miRNAs function mainly by downregulating the expression of their gene targets. Thus, accurate prediction of miRNA targets is critical for characterization of miRNA functions. The targets in the dataset are predicted using a bioinformatics tool, MirTarget[9][32], which was developed by analyzing miRNA-target interactions from high-throughput sequencing experiments. miRDB includes 3.5 million predicted targets regulated by 7000 miRNAs in five species, but we will focus on the miRNAs we imported already in our graph (so related to human). Listing C.18 shows a miRBase record sample including the confidence score, which helps evaluate the strength of the miRNA-target association.

Listing C.18 miRBase record sample including confidence score

{'name': 'hsa-mir-96-5p', 'target': 'NM_012214', 'value': 90.3926}

The result of the import will look like what is shown in Figure C.17.

Figure C.17 Same record from miRDB as it is stored in the graph.

As a last step we will import a relatively small dataset containing the miRNA pairwise functional similarity. Similarity score is obtained using a bioinformatics tool, MISIM[10][33] which computes miRNA functional similarity by comparing the semantic value of the disease associated to the two miRNAs. Listing C.19 shows a MISIM record sample, also including the confidence score.

Listing C.19 MISIM record sample

{"sourceName": "hsa-mir-376a",

"destinationName": "hsa-mir-449a", "value": 0.9101 }

The result will look like the small graph depicted in Figure C.18.

Figure C.18 Same record as it is stored in the graph.

This last import completes our ingestion process. Before moving ahead would be useful to run some simple exercises to check a bit the content of the database, familiarize with it and see how the different components participate in knowledge graph. The questions in the exercises are very useful to get your head around the database size. This will affect the time required to run certain algorithms and the final quality of the generated model. So, it is usual a good practice to run them before a deeper analysis.

Exercises

How many nodes of each type exist in the database? How many miRNAs, how many diseases? What is the disease connected to the most of miRNAs? What is the median value? Vice versa, what is the miRNAs with more connections to different diseases? What is the median value?

C.3 Exploring and analyzing the miRNA Knowledge Graph

As we have done for the biomedical knowledge graphs built by other

organizations or people, it is time to go through the graph we built to extract some information and validate the quality of the knowledge contained, before moving to more complex tasks. In order to do that, we can run some query that will allow us to observe similarities between nodes of the same type as well as nodes of different types.

This kind of similarities represents some of the implicit relationships that will be exploited by machine learning algorithms to perform their task. As we will see, during the training phase many embeddings’ algorithms learn to identify which of these implicit relationships are more useful to obtain the desired results.

As promised, in case you didn’t create the database by yourself Listing C.20 shows you how to import using our own database[11]. It can be used also to verify your database if you run through the full process.

Listing C.20 Importing the micro RNAs database from a backup

# Add the following line to the neo4j.conf file # dbms.databases.seed_from_uri_providers=URLConnectionSeedProvide # then run the following command

CREATE DATABASE `hmdd2.0` OPTIONS { existingData: "use", seedUri

For the moment let's try to evaluate, for example, how much miRNAs can be similar each other, based on the number of Target messenger RNA they have in common. We will consider two miRNAs similar if they share many Target nodes. Figure C.19 illustrates this concept, showing how MiRNA2 is more similar to MiRNA3 than it is to MiRNA1, with the thickness of arrow lines representing the strength of target connections.

Figure C.19 MiRNA2 is more similar to MiRNA3 as it is to MiRNA1. The thicker the arrow lines, the stronger targets connections

To compute this type of similarity we will use the Node Similarity function from GDS in its weighted version so miRNAs connected to Targets with a high score value will be considered more compared to others weakier connected. Before this algorithm, we have to create a named in-memory representation of the graph we are going to analyze. Listing C.21 demonstrates how to create these in-memory graph projection required for our analysis.

Listing C.21 In-memory Graph creation

CALL gds.graph.project("MiRNA_Target_similarity",

["Target","MiRNA"], #A

{HAS_TARGET:{properties:["value"]}}) #B

Once the in-memory database has been created we can run the node similarity computation. Listing C.22 demonstrates how to calculate similarity between nodes using the weighted version of the algorithm, allowing us to prioritize connections with higher score values.

Listing C.22 Similarity computation

CALL gds.nodeSimilarity.stream(

"MiRNA_Target_similarity", {relationshipWeightProperty: 'value'}) #A YIELD node1,node2, similarity WITH gds.util.asNode(node1) AS source,

gds.util.asNode(node2) AS target, similarity RETURN source.name AS source, target.name AS target, similarity ORDER BY similarity DESC, source , target

The results of the node similarity computation are reported in the Table C.4.

Table C.4 Results of the similarity query in Listing C.22.

Source Target Similarity hsa-let-7a-5p hsa-let-7c-5p 1.0 hsa-let-7a-5p hsa-let-7e-5p 1.0 hsa-mir-107 hsa-mir-103a-3p 1.0 hsa-mir-570-5p hsa-mir-548ai 1.0

Even on the topmost similar miRNA we obtained in this way, we can find some obvious and some less obvious examples. For example, searching for the miRNAs in line 3, hsa-mir-107 and hsa-mir-103s-3p, over internet you can easily find tons of articles speaking about these two miRNAs connected to, among the others, to osteoarthritis[12] or cystic fibrosis[13].

Furthermore, we can push ourselves a bit further. As we know, miRNAs regulate the gene expression by interfering to specific messenger RNA, when the regulation is abnormal, we may incur in pathologies. It is reasonable to wonder how a Target messenger RNA can be considered "similar", or affine, to a Disease based on how many miRNAs the two has in common.

This kind of analysis besides being interesting on its own, it also shows how it is possible to compare different entities (Diseases and Targets) using information coming from different datasets.

Again, we will use the Node Similarity function in GDS but this time it's filtered version, since we are interested in similarity relations between Targets a Diseases and not between Targets and Targets or between MiRNA and Disease for example. Listing C.23 shows the Cypher query to create the in-memory graph representation that will serve as the basis for our similarity analysis.

Listing C.23 in-memory graph creation

CALL gds.graph.project("Disease_Target_similarity",

["Target","MiRNA","Disease"], {HAS_TARGET:{orientation:"UNDIRECTED"}, #A RELATED_TO:{orientation:"UNDIRECTED"}, SIMILAR_TO:{orientation:"UNDIRECTED"}})

Once the graph has been created in memory, we can run the similarity computation, as shown in Listing C.24, and evaluate the results.

Listing C.24 Similarity computation

CALL gds.alpha.nodeSimilarity.filtered.stream( #A

"Disease_Target_similarity", {sourceNodeFilter:"Disease",targetNodeFilter:"Target"}) #B yield node1,node2, similarity WITH gds.util.asNode(node1) AS source,

gds.util.asNode(node2) AS target, similarity MATCH (source)-[]-(m:MiRNA)-[:HAS_TARGET]-(target) #C WITH source, target, similarity, count(m) as miRNAs WHERE miRNAs > 10 #D RETURN source.name AS source, target.name AS target, similarity, ORDER BY similarity DESCENDING, source , target

The result of the similarity query is reported in the table C.5.

Table C.5 Results of the similarity query in Listing C.24.

Source Target Similarity miRNAs

meningioma NM_203347 0.047619048 11 meningioma NM_001031745 0.045454545 12 prostate neoplasms

NM_012316 0.030769231 16

prostate neoplasms

NM_001260491 0.030373832 13

We can examine the first resulting row in detail using the query in Listing C.25:

Listing C.25 Meningioma – NM_203347 similarity in detail

MATCH p=(source)-[]-(m:MiRNA)-[:HAS_TARGET]-(target)-[:HAS_TARGET WHERE source.name = "meningioma"

AND target.name = "NM_203347" RETURN p

The result of the query looks like the graph depicted in Figure C.20.

Figure C.20 The topmost result from the similarity computation as it appears in the graph.

In this example we can see that almost all the miRNA associated to the target messenger RNA NM_203347 are related to the Meningioma pathology. Again, this type of findings does not have to be significant by a medical standpoint, but they definitely represent informational material exploitable by a machine learning algorithm.

The last analysis we run on this brand-new knowledge graph is the same we used for Hetionet, where we leveraged DWPC to find relevant paths from a disease to a GO Process. As a reminder, DWPC helps us to avoid biasing our analysis by nodes that are part of many paths.

Even in this case, we will use the celiac disease as our reference disease because we know it well and we can evaluate the results. In the exercise we recommend testing the query with other diseases. Listing C.26 demonstrates how to query our knowledge graph to identify relevant targets potentially associated with celiac disease.

Listing C.26 Search for relevant Target connected to the Celiac Disease

MATCH path = (d:Disease)<-[:REGULATES|RELATED_TO]-(m)-[:HAS_TARGE WHERE d.name = "celiac disease" WITH [

size([(d)<-[:REGULATES|RELATED_TO]-() | d]), size([()<-[:REGULATES|RELATED_TO]-(m) | m]), size([(m)-[:HAS_TARGET]->() | m]), size([()-[:HAS_TARGET]->(t) | t]) ] AS degrees, path, d, t WITH d.name as disease_name, t.name as target_name, count(path) a

size([(t)-[:HAS_TARGET]-() | t]) AS n_miRNA WHERE n_miRNA >= 5 and PC >= 2 RETURN disease_name, target_name, PC, DWPC, n_miRNA ORDER BY DWPC desc LIMIT 10

The result of the query is in the summarized in Table C.26.

Table C.6 Results of the query in Listing C.26.

Disease Name Target Name PC DWPC # miRNA celiac disease NM_080601 2 0.00417 25 celiac disease NM_001224 2 0.00322 111 celiac disease NM_032982 2 0.00318 114 celiac disease NM_152617 2 0.00295 136 celiac disease NM_032983 2 0.00278 160 celiac disease NM_198926 3 0.00241 158

celiac disease NM_019099 3 0.00234 169 celiac disease NM_005235 2 0.00219 286 celiac disease NM_052845 2 0.00210 138 celiac disease NM_001142551 2 0.00209 138

As in the case of Hetionet, these results are definitely interesting since they are in line with the scientific evidence connected to celiac disease. For example, the first target in the list is NM_080601. It is the “homo sapiens protein tyrosine phosphatase non-receptor type 11 (PTPN11), transcript variant 2, mRNA”[14]. Very recently has been discovered the role of protein tyrosine phosphatases in regulating the immune system and the implication this has for chronic intestinal inflammation [34]. Continuing the list, the elements number 2, 3, and 5 are, respectively, NM_001224, NM_032982, and NM_032983. All these are different variant of the ”homo sapiens caspase 2, apoptosis-related cysteine peptidase”, CASP2.

Several studies, including a single-cell RNA-seq survey of gluten-specific T cells [35], provide the knowledge base for finding unique targets for the removal of gluten-specific T cells as a curative therapeutic option for celiac disease. In these studies, researchers found marked upregulation of several apoptosis-related genes, such as FAS, TRAIL and CASP2 in tetramer- positive cells, possibly due to in vivo activation by gluten antigens. These findings encourage the use of activation induced cell death for the removal of gluten-specific T cells.

For the other targets, emerged from the simple query above we didn’t find any easy to access article connecting celiac disease to related genes variants expressed by those targets. Nevertheless, considering how active the research is around this disease, and how recent are the studies mentioned above, could happen that soon some correlation will be easy to find.

In order to complete our analysis, let’s run the query showing how miRNA are connected to the first target in the list. Listing C.27 demonstrates how to search for all paths connecting the celiac disease with the target NM:080601.

Listing C.27 Searching for all the paths between the celiac disease and the NM:080601

MATCH path = (d:Disease)<-[:REGULATES|RELATED_TO]-(m)-[:HAS_TARGE

WHERE d.name = 'celiac disease'

AND t.name = 'NM_080601' RETURN path

The result is shown in Figure C.21.

Figure C.21 Paths between the celiac disease and the target NM_080601 that is the first in the results.

Once more we demonstrate that combining multiple sources in a single holistic knowledge graph allows us to analyze information in different ways from several perspectives. The last examples specifically also showed how metrics like DWPC have a broad application in several contexts.

Exercise

As possible exercise, if you are interested in the domain, run some of the previous queries changing the target disease and evaluate the results.

C.4 Summary

Combining structured and unstructured data sources is necessary to create holistic knowledge graphs that include data coming from many relevant data sources. Acquiring knowledge from multiple distributed data sources it is not simple because of difference in name and code conventions. The construction process requires to have a clear objective in mind and a good understanding of the domain. Using reference ontologies or dataset it is possible to mitigate code and naming differences among datasets and normalizing entries. Simple text analysis techniques can be used to merge entities using slightly different names. Once constructed the knowledge graph can be explored using DWPC, similarity, and other simple but powerful analysis techniques. This is mandatory to validate the import and evaluate the knowledge quality before jumping in more complex machine learning algorithms.

C.5 Reference

1. Lee, Rosalind C., Rhonda L. Feinbaum, and Victor Ambros. "The C.

elegans heterochronic gene lin-4 encodes small RNAs with antisense complementarity to lin-14." cell 75.5 (1993): 843-854. 2. Jonas, Stefanie, and Elisa Izaurralde. "Towards a molecular

understanding of microRNA-mediated gene silencing." Nature reviews genetics 16.7 (2015): 421-433.

3. Chen, Xing, et al. "RBMMMDA: predicting multiple types of disease-

microRNA associations." Scientific reports 5.1 (2015): 13877. 4. Bartel, David P. "MicroRNAs: target recognition and regulatory

functions." cell 136.2 (2009): 215-233. 5. Miska, Eric A. "How microRNAs control cell division, differentiation

and death." Current opinion in genetics & development 15.5 (2005): 563-568. 6. Cheng, Angie M., et al. "Antisense inhibition of human miRNAs and

indications for an involvement of miRNA in cell growth and apoptosis." Nucleic acids research 33.4 (2005): 1290-1297. 7. Cui, Qinghua, et al. "Principles of microRNA regulation of a human

cellular signaling network." Molecular systems biology 2.1 (2006): 46. 8. Chen, Xing, Ming-Xi Liu, and Gui-Ying Yan. "RWRMDA: predicting

novel human microRNA–disease associations." Molecular BioSystems 8.10 (2012): 2792-2798. 9. Chen, Xing, and Gui-Ying Yan. "Semi-supervised learning for potential

human microRNA-disease associations inference." Scientific reports 4.1 (2014): 5501. 10. Li, Jingjing, et al. "Evidence for positive selection on a number of

microRNA regulatory interactions during recent human evolution." PLoS genetics 8.3 (2012): e1002578. 11. Chen, Kevin, and Nikolaus Rajewsky. "Natural selection on human

microRNA binding sites inferred from SNP data." Nature genetics 38.12 (2006): 1452-1456. 12. Saunders, Matthew A., Han Liang, and Wen-Hsiung Li. "Human

polymorphism at microRNAs and microRNA target sites." Proceedings of the National Academy of Sciences 104.9 (2007): 3300-3305. 13. Sethupathy, Praveen, and Francis S. Collins. "MicroRNA target site

polymorphisms and human disease." Trends in genetics 24.10 (2008): 489-497. 14. Deng, Su-Ping, Lin Zhu, and De-Shuang Huang. "Predicting hub genes

associated with cervical cancer through gene co-expression networks." IEEE/ACM Transactions on Computational Biology and Bioinformatics 13.1 (2015): 27-35. 15. Deng, Su-Ping, Lin Zhu, and De-Shuang Huang. "Mining the bladder

cancer-associated genes by an integrated strategy for the construction and analysis of differential co-expression networks." BMC genomics.

Vol. 16. No. 3. BioMed Central, 2015. 16. Eulalio, Ana, Eric Huntzinger, and Elisa Izaurralde. "Getting to the root

of miRNA-mediated gene silencing." Cell 132.1 (2008): 9-14. 17. Ambros, Victor. "The functions of animal microRNAs." Nature

431.7006 (2004): 350-355. 18. Chen, Xing, et al. "MicroRNAs and complex diseases: from

experimental results to computational models." Briefings in bioinformatics 20.2 (2019): 515-539.5.1.3 19. Lukasik, Anna, Maciej Wójcikowski, and Piotr Zielenkiewicz.

"Tools4miRs–one place to gather all the tools for miRNA analysis." Bioinformatics 32.17 (2016): 2722-2724. 20. Lu, Ming, et al. "An analysis of human microRNA and disease

associations." PloS one 3.10 (2008): e3420. 21. Li, Yang, et al. "HMDD v2. 0: a database for experimentally supported

human microRNA and disease associations." Nucleic acids research 42.D1 (2014): D1070-D1074. 22. Huang, Zhou, et al. "HMDD v3. 0: a database for experimentally

supported human microRNA–disease associations." Nucleic acids research 47.D1 (2019): D1013-D1017. 23. Xu, Feng, et al. "dbDEMC 3.0: functional exploration of differentially

expressed miRNAs in cancers of human and model organisms." Genomics, Proteomics & Bioinformatics 20.3 (2022): 446-454. 24. Jiang, Qinghua, et al. "miR2Disease: a manually curated database for

microRNA deregulation in human disease." Nucleic acids research 37.suppl_1 (2009): D98-D104.5.2.2 25. Bodenreider, Olivier. "The unified medical language system (UMLS):

integrating biomedical terminology." Nucleic acids research 32.suppl_1 (2004): D267-D270. 26. Kozomara, Ana, Maria Birgaoanu, and Sam Griffiths-Jones. "miRBase:

from microRNA sequences to function." Nucleic acids research 47.D1 (2019): D155-D162. 27. Kozomara, Ana, and Sam Griffiths-Jones. "miRBase: annotating high

confidence microRNAs using deep sequencing data." Nucleic acids research 42.D1 (2014): D68-D73. 28. Kozomara, Ana, and Sam Griffiths-Jones. "miRBase: integrating

microRNA annotation and deep-sequencing data." Nucleic acids research 39.suppl_1 (2010): D152-D157.

29. Griffiths-Jones, Sam, et al. "miRBase: microRNA sequences, targets and

gene nomenclature." Nucleic acids research 34.suppl_1 (2006): D140- D144. 30. Chen, Yuhao, and Xiaowei Wang. "miRDB: an online database for

prediction of functional microRNA targets." Nucleic acids research 48.D1 (2020): D127-D131. 31. Liu, Weijun, and Xiaowei Wang. "Prediction of functional microRNA

targets by integrative modeling of microRNA binding and target expression data." Genome biology 20 (2019): 1-10. 32. Kern, Fabian, et al. "miRTargetLink 2.0—interactive miRNA target

gene and target pathway networks." Nucleic acids research 49.W1 (2021): W409-W416. 33. Li, Jianwei, et al. "MISIM v2. 0: a web server for inferring microRNA

functional similarity based on microRNA-disease associations." Nucleic acids research 47.W1 (2019): W536-W541. 34. Spalinger, Marianne R., et al. "Role of protein tyrosine phosphatases in

regulating the immune system: implications for chronic intestinal inflammation." _Inflammatory bowel diseases_ 21.3 (2015): 645-655. 35. Yao, Ying, et al. "Differential expression profile of gluten-specific T

cells identified by single-cell RNA-seq." _Plos one_ 16.10 (2021): e0258029.

[1] https://tools4mirs.org/ (you could have some issues with the expired SSL certificates, you know researchers don’t care about these details)

[2] The Human microRNA Disease Database http://www.cuilab.cn/hmdd

[3] A Database of Differentially Expressed miRNAs in Human Cancers https://www.biosino.org/dbDEMC/index

[4] A manually curated database that aims at providing a comprehensive resource of miRNA deregulation in various human diseases. http://www.mir2disease.org

[5] The unified medical language system https://www.nlm.nih.gov/research/umls/index.html

[6] scispaCy a Python package containing spaCy models for processing

biomedical, scientific or clinical text. https://allenai.github.io/scispacy/

[7] A searchable database of published miRNA sequences and annotation https://www.mirbase.org/

[8] An online database for miRNA target prediction and functional annotations https://mirdb.org/

[9] Interactive network visualizations tool about microRNA-mRNA interactions https://ccb-compute.cs.uni-saarland.de/mirtargetlink2

[10] miRNA Funcional Similarity http://www.lirmed.com/misim/

[11] https://www.dropbox.com/s/lnlwz9snwt7vmfe/miRNA-5.4.0- 2023.backup?dl=1

[12] https://pesquisa.bvsalud.org/portal/resource/pt/wpr-910210

[13] https://link.springer.com/content/pdf/10.1038/s41598-019-51890-7.pdf

[14] https://www.ncbi.nlm.nih.gov/nuccore/NM_080601